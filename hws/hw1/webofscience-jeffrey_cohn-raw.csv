PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
J	Ertugrul, IO; Ahn, YA; Bilalpur, M; Messinger, DS; Speltz, ML; Cohn, JF				Ertugrul, Itir Onal; Ahn, Yeojin Amy; Bilalpur, Maneesh; Messinger, Daniel S.; Speltz, Matthew L.; Cohn, Jeffrey			Infant AFAR: Automated facial action recognition in infants	BEHAVIOR RESEARCH METHODS			English	Article; Early Access						Automatic facial action unit detection; Facial action coding system; Infant behavior; Cross domain generalizability; Deep learning	CRANIOFACIAL MICROSOMIA; EXPRESSIONS; FACE; MODELS	Automated detection of facial action units in infants is challenging. Infant faces have different proportions, less texture, fewer wrinkles and furrows, and unique facial actions relative to adults. For these and related reasons, action unit (AU) detectors that are trained on adult faces may generalize poorly to infant faces. To train and test AU detectors for infant faces, we trained convolutional neural networks (CNN) in adult video databases and fine-tuned these networks in two large, manually annotated, infant video databases that differ in context, head pose, illumination, video resolution, and infant age. AUs were those central to expression of positive and negative emotion. AU detectors trained in infants greatly outperformed ones trained previously in adults. Training AU detectors across infant databases afforded greater robustness to between-database differences than did training database specific AU detectors and outperformed previous state-of-the-art in infant AU detection. The resulting AU detection system, which we refer to as Infant AFAR (Automated Facial Action Recognition), is available to the research community for further testing and applications in infant emotion, social interaction, and related topics.	[Ertugrul, Itir Onal] Univ Utrecht, Utrecht, Netherlands; [Ahn, Yeojin Amy; Messinger, Daniel S.] Univ Miami, Miami, FL USA; [Bilalpur, Maneesh; Cohn, Jeffrey] Univ Pittsburgh, Pittsburgh, PA USA; [Speltz, Matthew L.] Univ Washington, Seattle, WA 98195 USA		Ertugrul, IO (corresponding author), Univ Utrecht, Utrecht, Netherlands.	i.onalertugrul@uu.nl			US National Institutes of Health [DE026513, DE022438, MH096951, R01-GM105004-01]; National Science Foundation [1052736]; Center for Clinical and Translational Research at Seattle Children'dss Research Institute [UL1 TR000423]	US National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Science Foundation(National Science Foundation (NSF)); Center for Clinical and Translational Research at Seattle Children'dss Research Institute	The work was supported in part by US National Institutes of Health grants DE026513, DE022438, MH096951, R01-GM105004-01, National Science Foundation grant 1052736, and the Center for Clinical and Translational Research at Seattle Children'dss Research Institute grant UL1 TR000423.	Adamson LB, 2003, INFANCY, V4, P451, DOI 10.1207/S15327078IN0404_01; Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019; Bansal S., 2019, P 2019 C N AM CHAPT, V1, P5868; Barrett LF, 2019, PSYCHOL SCI PUBL INT, V20, P1, DOI 10.1177/1529100619832930; Beebe B, 2020, COMPANION PUBLICATON OF THE 2020 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION (ICMI '20 COMPANION), P365, DOI 10.1145/3395035.3425357; Beebe B, 2013, ATTACH HUM DEV, V15, P583, DOI 10.1080/14616734.2013.841050; BRENNAN RL, 1981, EDUC PSYCHOL MEAS, V41, P687, DOI 10.1177/001316448104100307; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; CAMPBELL SB, 1995, DEV PSYCHOL, V31, P349, DOI 10.1037/0012-1649.31.3.349; CAMRAS LA, 1992, COGNITION EMOTION, V6, P269, DOI 10.1080/02699939208411072; Chen JX, 2013, PATTERN RECOGN LETT, V34, P1964, DOI 10.1016/j.patrec.2013.02.002; Chen M, 2021, MULTIVAR BEHAV RES, V56, P739, DOI 10.1080/00273171.2020.1762065; Chu W. S., 2017, FG, P2532; Cohn J. F., 1991, DEV PSYCHOPATHOL, V3, P367, DOI [DOI 10.1017/S0954579400007574, 10.1017/S0954579400007574]; Cohn J.F., 2007, HDB EMOTION ELICITAT, P203, DOI [10.1007/978-3-540-72348-6_1, DOI 10.1007/978-3-540-72348-6_1]; Cohn JF, 2010, BEHAV RES METHODS, V42, P1079, DOI 10.3758/BRM.42.4.1079; Cohn JF., 2005, NEW HDB METHODS NONV, P9; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Cowen AS, 2021, NATURE, V589, P251, DOI 10.1038/s41586-020-3037-7; Deng J., 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848; Eibl-Eibesfeldt I, 1970, ETHOLOGY BIOL BEHAV, P530; Ekman P, 2002, RES NEXUS; Ertugrul Itir Onal, 2020, IEEE Trans Biom Behav Identity Sci, V2, P158, DOI [10.1109/TBIOM.2020.2977225, 10.1109/tbiom.2020.2977225]; Ertugrul Itir Onal, 2019, Proc Int Conf Autom Face Gesture Recognit, V2019, DOI 10.1109/FG.2019.8756623; Forestell Catherine A, 2017, Curr Nutr Rep, V6, P141, DOI 10.1007/s13668-017-0205-y; Girard Jeffrey M., 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163106; Girard JM, 2017, IEEE INT CONF AUTOMA, P581, DOI 10.1109/FG.2017.144; Goldsmith H. H., 1999, LAB TEMPERAMENT ASSE; Hammal Z, 2018, CLEFT PALATE-CRAN J, V55, P711, DOI 10.1177/1055665617753481; Hammal Z, 2017, INT CONF AFFECT, P216, DOI 10.1109/ACII.2017.8273603; Hammal Z, 2015, IEEE T AFFECT COMPUT, V6, P361, DOI 10.1109/TAFFC.2015.2422702; Heike CL, 2016, BIRTH DEFECTS RES A, V106, P915, DOI 10.1002/bdra.23560; HINTON GE, 1992, SCI AM, V267, P145, DOI 10.1038/scientificamerican0992-144; Hsu C.-W., 2008, PRACTICAL GUIDE SUPP; Jeni L. A., 2013, ACII; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Jiang BH, 2014, IEEE T CYBERNETICS, V44, P161, DOI 10.1109/TCYB.2013.2249063; Kohut SA, 2012, PAIN, V153, P2458, DOI 10.1016/j.pain.2012.09.005; LeCun Y., 2015, NATURE, V521, P436, DOI DOI 10.1038/NATURE14539; Luquetti DV, 2019, CLEFT PALATE-CRAN J, V56, P877, DOI 10.1177/1055665618821014; Mahoor Mohammad H., 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P74, DOI 10.1109/CVPR.2009.5204259; MATIAS R, 1993, DEV PSYCHOL, V29, P524, DOI 10.1037/0012-1649.29.3.524; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Mattson WI, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0080161; Mesman J, 2009, DEV REV, V29, P120, DOI 10.1016/j.dr.2009.02.001; Messinger DS, 2012, EMOTION, V12, P430, DOI 10.1037/a0026498; Messinger DS, 2009, INFANCY, V14, P285, DOI 10.1080/15250000902839963; Niinuma K., 2019, BMVC; Onal Ertugrul I., 2019, CROSS DOMAIN DETECTI; Ertugrul IO, 2019, FRONT COMP SCI-SWITZ, V1, DOI 10.3389/fcomp.2019.00011; Oster H., 2006, UNPUBLISHED MONOGRAP; Rosenstein D., 1988, CHILD DEV; Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34; Simonyan K., 2014, ARXIV; Speltz ML, 2018, J PEDIATR-US, V198, P226, DOI 10.1016/j.jpeds.2018.02.076; Valstar M., 2004, 2004 IEEE INT C SYST, V1; Yang L, 2019, INT CONF AFFECT; Zaker N, 2014, IEEE IMAGE PROC, P1357, DOI 10.1109/ICIP.2014.7025271; Zhao K., 2016, CVPR; Zhou ZW, 2017, PROC CVPR IEEE, P4761, DOI 10.1109/CVPR.2017.506	60	0	0	3	3	SPRINGER	NEW YORK	ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES	1554-351X	1554-3528		BEHAV RES METHODS	Behav. Res. Methods												10.3758/s13428-022-01863-y	http://dx.doi.org/10.3758/s13428-022-01863-y		MAY 2022	12	Psychology, Mathematical; Psychology, Experimental	Social Science Citation Index (SSCI)	Psychology	1C4JY	35538295	hybrid			2022-10-03	WOS:000793088700002
J	Cohn, JF; Motta, M; Parrish, RM				Cohn, Jeffrey; Motta, Mario; Parrish, Robert M.			Quantum Filter Diagonalization with Compressed Double-Factorized Hamiltonians	PRX QUANTUM			English	Article							APPROXIMATE INTEGRALS; DECOMPOSITION; ALGORITHM	We demonstrate a method that merges the quantum filter diagonalization (QFD) approach for hybrid quantum-classical solution of the time-independent electronic Schrodinger equation with a low-rank double factorization (DF) approach for the representation of the electronic Hamiltonian. In particular, we explore the use of a novel sparse "compressed" double factorization (C-DF) truncation of the Hamiltonian within the time-propagation elements of QFD, while retaining a similarly compressed but numerically converged double-factorized representation of the Hamiltonian for the operator expectation values needed in the QFD quantum matrix elements. The new C-DF method is found to provide substantial additional compression at any given accuracy metric over the traditional "explicit" double factorization approach. Together with significant circuit reduction optimizations and number-preserving postselection and echosequencing error mitigation strategies, the method is found to provide accurate predictions for low-lying eigenspectra in a number of representative molecular systems, while requiring reasonably short circuit depths and modest measurement costs. The method is demonstrated by experiments on noise-free simulators, simulations including models of decoherence and shot-noise, and real quantum hardware.	[Cohn, Jeffrey; Motta, Mario] IBM Res Almaden, IBM Quantum, San Jose, CA 95120 USA; [, Robert M.] QC Ware Corp, Palo Alto, CA 94301 USA		Cohn, JF (corresponding author), IBM Res Almaden, IBM Quantum, San Jose, CA 95120 USA.	jeffrey.cohn@ibm.com; mario.motta@ibm.com; rob.parrish@qcware.com	Motta, Mario/AHD-9813-2022		U.S. Department of Energy, Office of Science, Basic Energy Sciences, Chemical Sciences, Geosciences and Biosciences Division	U.S. Department of Energy, Office of Science, Basic Energy Sciences, Chemical Sciences, Geosciences and Biosciences Division(United States Department of Energy (DOE))	The QC Ware effort in this work was supported by the U.S. Department of Energy, Office of Science, Basic Energy Sciences, Chemical Sciences, Geosciences and Biosciences Division. We thank an anonymous reviewer for a suggestion to more thoroughly explore the error characteristics of X-DF versus C-DF in the context of deviations in chemically important observables such as total adiabatic state energies.	Aharonov D., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P427; Aleksandrowicz G., 2019, ZENODO; Aquilante F, 2007, J CHEM PHYS, V126, DOI 10.1063/1.2736701; Aquilante F, 2009, J CHEM PHYS, V130, DOI 10.1063/1.3116784; Beebe NHF, 1977, INT J QUANTUM CHEM, V12, P683, DOI 10.1002/qua.560120408; Berry DW, 2019, QUANTUM-AUSTRIA, V3, DOI 10.22331/q-2019-12-02-208; Bonet-Monroig X, 2018, PHYS REV A, V98, DOI 10.1103/PhysRevA.98.062339; Bravyi SB, 2002, ANN PHYS-NEW YORK, V298, P210, DOI 10.1006/aphy.2002.6254; Cross AW, 2019, PHYS REV A, V100, DOI 10.1103/PhysRevA.100.032328; DUNLAP BI, 1977, INT J QUANTUM CHEM, P81; DUNLAP BI, 1979, J CHEM PHYS, V71, P3396, DOI 10.1063/1.438728; FEYEREISEN M, 1993, CHEM PHYS LETT, V208, P359, DOI 10.1016/0009-2614(93)87156-W; Georgescu IM, 2014, REV MOD PHYS, V86, P153, DOI 10.1103/RevModPhys.86.153; Gokhale P., 2020, IEEE T QUANTUM ENG, V1, P1; Google Al Quantum Collaborators, 2020, SCIENCE, V369, P1084, DOI 10.1126/science.abb9811; HEADGORDON M, 1988, J PHYS CHEM-US, V92, P3063, DOI 10.1021/j100322a012; Hohenstein EG, 2012, J CHEM PHYS, V137, DOI 10.1063/1.4732310; Huggins WJ, 2021, NPJ QUANTUM INFORM, V7, DOI 10.1038/s41534-020-00341-7; Huggins WJ, 2020, NEW J PHYS, V22, DOI 10.1088/1367-2630/ab867b; Kempe J, 2006, SIAM J COMPUT, V35, P1070, DOI 10.1137/S0097539704445226; Kendall RA, 1997, THEOR CHEM ACC, V97, P158, DOI 10.1007/s002140050249; Kivlichan ID, 2018, PHYS REV LETT, V120, DOI 10.1103/PhysRevLett.120.110501; Klymko K., 2021, ARXIV210308563; Koch H, 2003, J CHEM PHYS, V118, P9481, DOI 10.1063/1.1578621; KOMORNICKI A, 1993, J CHEM PHYS, V98, P1398, DOI 10.1063/1.465054; Lee J., 2020, ARXIV201103494; Matsuzawa Y, 2020, J CHEM THEORY COMPUT, V16, P944, DOI 10.1021/acs.jctc.9b00963; McArdle S, 2019, PHYS REV LETT, V122, DOI 10.1103/PhysRevLett.122.180501; McClean JR, 2017, PHYS REV A, V95, DOI 10.1103/PhysRevA.95.042308; Motta M., 2018, ARXIV180802625; Motta M, 2020, NAT PHYS, V16, P205, DOI 10.1038/s41567-019-0704-4; Motta M, 2019, J CHEM THEORY COMPUT, V15, P3510, DOI 10.1021/acs.jctc.8b00996; Nakanishi KM, 2019, PHYS REV RES, V1, DOI 10.1103/PhysRevResearch.1.033062; NEUHAUSER D, 1994, J CHEM PHYS, V100, P5076, DOI 10.1063/1.467224; NEUHAUSER D, 1990, J CHEM PHYS, V93, P2611, DOI 10.1063/1.458900; Ollitrault PJ, 2020, PHYS REV RES, V2, DOI 10.1103/PhysRevResearch.2.043140; Olsen, 2013, MOL ELECT STRUCTURE; Parrish R. M., 2019, ARXIV190908925; Parrish RM, 2019, PHYS REV LETT, V122, DOI 10.1103/PhysRevLett.122.230401; Parrish RM, 2013, PHYS REV LETT, V111, DOI 10.1103/PhysRevLett.111.132505; Parrish RM, 2012, J CHEM PHYS, V137, DOI 10.1063/1.4768233; Peng B, 2017, J CHEM THEORY COMPUT, V13, P4179, DOI 10.1021/acs.jctc.7b00605; Peruzzo A, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5213; Poulin D, 2015, QUANTUM INF COMPUT, V15, P361; RECK M, 1994, PHYS REV LETT, V73, P58, DOI 10.1103/PhysRevLett.73.58; RENDELL AP, 1994, J CHEM PHYS, V101, P400, DOI 10.1063/1.468148; ROEGGEN I, 1986, CHEM PHYS LETT, V132, P154, DOI 10.1016/0009-2614(86)80099-9; Stair NH, 2020, J CHEM THEORY COMPUT, V16, P2236, DOI 10.1021/acs.jctc.9b01125; Tran MC, 2021, PRX QUANTUM, V2, DOI 10.1103/PRXQuantum.2.010323; VAHTRAS O, 1993, CHEM PHYS LETT, V213, P514, DOI 10.1016/0009-2614(93)89151-7; Verteletskyi V, 2020, J CHEM PHYS, V152, DOI 10.1063/1.5141458; von Burg V., 2020, ARXIV200714460; Wecker D, 2015, PHYS REV A, V92, DOI 10.1103/PhysRevA.92.062318; Weigend F, 2002, PHYS CHEM CHEM PHYS, V4, P4285, DOI 10.1039/b204199p; WHITTEN JL, 1973, J CHEM PHYS, V58, P4496, DOI 10.1063/1.1679012; WILCOX RM, 1967, J MATH PHYS, V8, P962, DOI 10.1063/1.1705306	56	2	2	2	2	AMER PHYSICAL SOC	COLLEGE PK	ONE PHYSICS ELLIPSE, COLLEGE PK, MD 20740-3844 USA		2691-3399		PRX QUANTUM	PRX Quantum	DEC 15	2021	2	4							040352	10.1103/PRXQuantum.2.040352	http://dx.doi.org/10.1103/PRXQuantum.2.040352			19	Quantum Science & Technology; Physics, Applied; Physics, Multidisciplinary	Science Citation Index Expanded (SCI-EXPANDED)	Physics	YI9XP		gold, Green Submitted			2022-10-03	WOS:000744194200001Parrish
J	Provenza, NR; Sheth, SA; Dastin-van Rijn, EM; Mathura, RK; Ding, YH; Vogt, GS; Avendano-Ortega, M; Ramakrishnan, N; Peled, N; Gelin, LFF; Xing, D; Jeni, LA; Ertugrul, IO; Barrios-Anderson, A; Matteson, E; Wiese, AD; Xu, JQ; Viswanathan, A; Harrison, MT; Bijanki, KR; Storch, EA; Cohn, JF; Goodman, WK; Borton, DA				Provenza, Nicole R.; Sheth, Sameer A.; Dastin-van Rijn, Evan M.; Mathura, Raissa K.; Ding, Yaohan; Vogt, Gregory S.; Avendano-Ortega, Michelle; Ramakrishnan, Nithya; Peled, Noam; Gelin, Luiz Fernando Fracassi; Xing, David; Jeni, Laszlo A.; Ertugrul, Itir Onal; Barrios-Anderson, Adriel; Matteson, Evan R.; Wiese, Andrew D.; Xu, Junqian; Viswanathan, Ashwin; Harrison, Matthew T.; Bijanki, Kelly R.; Storch, Eric A.; Cohn, Jeffrey; Goodman, Wayne K.; Borton, David A.			Long-term ecological assessment of intracranial electrophysiology synchronized to behavioral markers in obsessive-compulsive disorder	NATURE MEDICINE			English	Article							DEEP BRAIN-STIMULATION; MULTISOURCE INTERFERENCE TASK; LOCAL-FIELD POTENTIALS; DBS; NUCLEUS; INFANT; OPTIMIZATION; REGISTRATION; EXPRESSION; SEVERITY	Detection of neural signatures related to pathological behavioral states could enable adaptive deep brain stimulation (DBS), a potential strategy for improving efficacy of DBS for neurological and psychiatric disorders. This approach requires identifying neural biomarkers of relevant behavioral states, a task best performed in ecologically valid environments. Here, in human participants with obsessive-compulsive disorder (OCD) implanted with recording-capable DBS devices, we synchronized chronic ventral striatum local field potentials with relevant, disease-specific behaviors. We captured over 1,000 h of local field potentials in the clinic and at home during unstructured activity, as well as during DBS and exposure therapy. The wide range of symptom severity over which the data were captured allowed us to identify candidate neural biomarkers of OCD symptom intensity. This work demonstrates the feasibility and utility of capturing chronic intracranial electrophysiology during daily symptom fluctuations to enable neural biomarker identification, a prerequisite for future development of adaptive DBS for OCD and other psychiatric disorders. The identification of candidate neural biomarkers of obsessive-compulsive disorder symptom intensity in ecologically valid environments.	[Provenza, Nicole R.; Dastin-van Rijn, Evan M.; Xing, David; Matteson, Evan R.; Borton, David A.] Brown Univ, Sch Engn, Providence, RI 02912 USA; [Provenza, Nicole R.] Charles Stark Draper Lab, Cambridge, MA USA; [Sheth, Sameer A.; Mathura, Raissa K.; Viswanathan, Ashwin; Bijanki, Kelly R.] Baylor Coll Med, Dept Neurosurg, Houston, TX 77030 USA; [Ding, Yaohan] Univ Pittsburgh, Intelligent Syst Program, Pittsburgh, PA USA; [Vogt, Gregory S.; Avendano-Ortega, Michelle; Ramakrishnan, Nithya; Wiese, Andrew D.; Xu, Junqian; Bijanki, Kelly R.; Storch, Eric A.; Goodman, Wayne K.] Baylor Coll Med, Menninger Dept Psychiat & Behav Sci, Houston, TX 77030 USA; [Peled, Noam] MGH HST Martinos Ctr Biomed Imaging, Charlestown, MA USA; [Peled, Noam] Harvard Med Sch, Cambridge, MA USA; [Gelin, Luiz Fernando Fracassi] Brown Univ, Ctr Computat & Visualizat, Providence, RI 02912 USA; [Jeni, Laszlo A.] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA; [Ertugrul, Itir Onal] Tilburg Univ, Dept Cognit Sci & Artificial Intelligence, Tilburg, Netherlands; [Barrios-Anderson, Adriel] Brown Univ, Warren Alpert Med Sch, Providence, RI 02912 USA; [Wiese, Andrew D.] Univ Missouri, Dept Psychol, Kansas City, MO 64110 USA; [Xu, Junqian] Baylor Coll Med, Dept Radiol, Houston, TX 77030 USA; [Harrison, Matthew T.] Brown Univ, Div Appl Math, Providence, RI 02912 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Borton, David A.] Brown Univ, Carney Inst Brain Sci, Providence, RI 02912 USA; [Borton, David A.] Dept Vet Affairs, Rehabil R&D Serv, Ctr Neurorestorat & Neurotechnol, Providence, RI 02908 USA		Borton, DA (corresponding author), Brown Univ, Sch Engn, Providence, RI 02912 USA.; Borton, DA (corresponding author), Brown Univ, Carney Inst Brain Sci, Providence, RI 02912 USA.; Borton, DA (corresponding author), Dept Vet Affairs, Rehabil R&D Serv, Ctr Neurorestorat & Neurotechnol, Providence, RI 02908 USA.	david_borton@brown.edu	Xu, Junqian/D-2247-2009	Xu, Junqian/0000-0001-8438-2066; Dastin-van Rijn, Evan/0000-0002-1428-0723; Vogt, Gregory/0000-0001-8259-5885; Peled, Nir/0000-0003-3714-4377	National Institutes of Health (NIH) NINDS BRAIN Initiative [UH3NS100549, UH3NS103549]; Charles Stark Draper Laboratory Fellowship; McNair Foundation; Texas Higher Education Coordinating Board [NIH 1RF1MH121371, U54HD083092]; Karen T. Romer Undergraduate Teaching and Research Award at Brown University; NIH [MH096951, K01MH116364, R21NS104953, 3R25MH10107605S2, 1S10OD025181]	National Institutes of Health (NIH) NINDS BRAIN Initiative; Charles Stark Draper Laboratory Fellowship; McNair Foundation; Texas Higher Education Coordinating Board; Karen T. Romer Undergraduate Teaching and Research Award at Brown University; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	The authors thank the participants and their families for their involvement in the research program. The authors also thank K. Lane for artistic contribution in the creation of Fig. 1. This work relied heavily on the community expertise and resources made available by the Open Mind Consortium (https://openmind-consortium.github.io/).Summit RC + S devices were donated by Medtronic as part of the BRAIN Initiative Public-Private Partnership Program. We thank J. Murphy for expertise and contributions in designing and machining equipment used in this study. Part of this research was conducted with the help of research staff at the Center for Computation and Visualization, Brown University (senior research software engineers B. Roarr and M. McGrath). The research was supported by the National Institutes of Health (NIH) NINDS BRAIN Initiative via contracts UH3NS100549 (to S.A.S., J.F.C., D.A.B., E.A.S. and W.K.G.) and UH3NS103549 (to S.A.S.), the Charles Stark Draper Laboratory Fellowship (to N.R.P.), the McNair Foundation (to S.A.S.), the Texas Higher Education Coordinating Board NIH 1RF1MH121371 and U54HD083092 (to E.A.S.), NIH MH096951 (to J.F.C.), K01MH116364 and R21NS104953 (to K.B.), 3R25MH10107605S2 (to A.B.-A.), award 1S10OD025181 (to J. Sanes at Brown University for computational resources) and the Karen T. Romer Undergraduate Teaching and Research Award at Brown University (E.M.D.-v.R. under the guidance of D.A.B.).	Alonso P, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0133591; Angst J, 2004, EUR ARCH PSY CLIN N, V254, P156, DOI 10.1007/s00406-004-0459-4; Baker JK, 2010, INT J BEHAV DEV, V34, P88, DOI 10.1177/0165025409350365; Basu I, CLOSED LOOP ENHANCEM, DOI [10.1101/2020.04.24.059964(2020, DOI 10.1101/2020.04.24.059964(2020]; Bergey GK, 2015, NEUROLOGY, V84, P810, DOI 10.1212/WNL.0000000000001280; Bijanki KR, 2019, J CLIN INVEST, V129, P1152, DOI 10.1172/JCI120110; Bouthour W, 2019, NAT REV NEUROL, V15, P343, DOI 10.1038/s41582-019-0166-4; Bush G, 2003, MOL PSYCHIATR, V8, P60, DOI 10.1038/sj.mp.4001217; Bush G, 2006, NAT PROTOC, V1, P308, DOI 10.1038/nprot.2006.48; Cohn J. F, 2016, P IEEE C COMP VIS PA, P87; Cohn JF, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P40, DOI 10.1145/3242969.3243023; Cowen AS, 2021, NATURE, V589, P251, DOI 10.1038/s41586-020-3037-7; Cuthbert BN, 2013, BMC MED, V11, DOI 10.1186/1741-7015-11-126; Denys D, 2020, AM J PSYCHIAT, V177, P265, DOI 10.1176/appi.ajp.2019.19060656; Dibeklioglu H, 2018, IEEE J BIOMED HEALTH, V22, P525, DOI 10.1109/JBHI.2017.2676878; Ding YH, 2020, COMPANION PUBLICATON OF THE 2020 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION (ICMI '20 COMPANION), P354, DOI 10.1145/3395035.3425354; Eijsker N, 2020, ENEURO, V7, DOI 10.1523/ENEUR0.0105-20.2020; Ekman P, 2002, RES NEXUS; Ekman P., 2005, WHAT FACE REVEALS BA, V2nd ed., DOI DOI 10.1093/ACPROF:OSO/9780195179644.001.0001; Ertugrul Itir Onal, 2019, Proc Int Conf Autom Face Gesture Recognit, V2019, DOI 10.1109/FG.2019.8756623; Felsenstein O., 2019, MULTIMODAL NEUROIMAG; Figee M, 2021, NAT MED, V27, P196, DOI 10.1038/s41591-021-01243-7; Figee M, 2013, NAT NEUROSCI, V16, P386, DOI 10.1038/nn.3344; Figee M, 2011, BIOL PSYCHIAT, V69, P867, DOI 10.1016/j.biopsych.2010.12.003; Fischl B, 2012, NEUROIMAGE, V62, P774, DOI 10.1016/j.neuroimage.2012.01.021; Frank AC, 2021, BRAIN STIMUL, V14, P1002, DOI 10.1016/j.brs.2021.06.009; Fullana MA, 2020, NEUROSCI BIOBEHAV R, V118, P504, DOI 10.1016/j.neubiorev.2020.08.008; Gibson WS, 2017, CEREB CORTEX, V27, P2183, DOI 10.1093/cercor/bhw074; Gillan CM, 2016, ELIFE, V5, DOI 10.7554/eLife.11305; Gilron R, 2021, NAT BIOTECHNOL, V39, P1078, DOI 10.1038/s41587-021-00897-5; Goodman WK, 2021, AM J PSYCHIAT, V178, P17, DOI 10.1176/appi.ajp.2020.20111601; Goodman WK, 2010, BIOL PSYCHIAT, V67, P535, DOI 10.1016/j.biopsych.2009.11.028; GOODMAN WK, 1989, ARCH GEN PSYCHIAT, V46, P1006; Graat I, 2017, INT REV PSYCHIATR, V29, P178, DOI 10.1080/09540261.2017.1282439; Grover S, 2021, NAT MED, V27, P232, DOI 10.1038/s41591-020-01173-w; Haber SN, 2021, BIOL PSYCHIAT, V90, P667, DOI 10.1016/j.biopsych.2020.06.031; Haeffel GJ, 2010, AM J PSYCHOL, V123, P181, DOI 10.5406/amerjpsyc.123.2.0181; Haines N, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0211735; Hammal Z., 2015, FRONT ICT, V2, P21, DOI [10.3389/fict.2015.00021, DOI 10.3389/FICT.2015.00021]; Hammal Z, 2019, PRS-GLOB OPEN, V7, DOI 10.1097/GOX.0000000000002081; Hammal Z, 2015, IEEE T AFFECT COMPUT, V6, P361, DOI 10.1109/TAFFC.2015.2422702; Hammal Z, 2014, IEEE T AFFECT COMPUT, V5, P155, DOI 10.1109/TAFFC.2014.2326408; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Jenkinson M, 2002, NEUROIMAGE, V17, P825, DOI 10.1006/nimg.2002.1132; Jenkinson M, 2001, MED IMAGE ANAL, V5, P143, DOI 10.1016/S1361-8415(01)00036-6; Joshi A, 2011, NEUROINFORMATICS, V9, P69, DOI 10.1007/s12021-010-9092-8; Kacem A, 2018, IEEE INT CONF AUTOMA, P739, DOI 10.1109/FG.2018.00116; Kremen V, 2018, IEEE J TRANSL ENG HE, V6, DOI 10.1109/JTEHM.2018.2869398; Le Yang, 2019, 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII). Proceedings, P538, DOI 10.1109/ACII.2019.8925514; Lega BC, 2011, NEUROREPORT, V22, P795, DOI 10.1097/WNR.0b013e32834b2975; Li NF, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16734-3; Liebrand LC, 2019, BRAIN STIMUL, V12, P353, DOI 10.1016/j.brs.2018.11.014; Little S, 2016, J NEUROL NEUROSUR PS, V87, P717, DOI 10.1136/jnnp-2015-310972; Mataix-Cols D, 2002, AM J PSYCHIAT, V159, P263, DOI 10.1176/appi.ajp.159.2.263; Mercier MR, 2017, NEUROIMAGE, V147, P219, DOI 10.1016/j.neuroimage.2016.08.037; Messinger DS, 2009, INFANCY, V14, P285, DOI 10.1080/15250000902839963; Miller KJ, 2019, J NEUROPHYSIOL, V121, P2336, DOI 10.1152/jn.00096.2019; Neumann WJ, 2014, MOL PSYCHIATR, V19, P1186, DOI 10.1038/mp.2014.2; Niinuma Koichiro, 2019, BMVC, V2019; Nota JA, 2014, J COGN PSYCHOTHER, V28, P134, DOI 10.1891/0889-8391.28.2.134; Okun MS, 2010, EXPERT REV NEUROTHER, V10, P1847, DOI 10.1586/ERN.10.156; Olsen ST, 2020, FRONT HUM NEUROSCI, V14, DOI 10.3389/fnhum.2020.569973; Ertugrul IO, 2019, FRONT COMP SCI-SWITZ, V1, DOI 10.3389/fcomp.2019.00011; Ooms P, 2014, J NEUROL NEUROSUR PS, V85, P153, DOI 10.1136/jnnp-2012-302550; Opri E, 2020, SCI TRANSL MED, V12, DOI 10.1126/scitranslmed.aay7680; Pallanti S, 2004, J CLIN PSYCHIAT, V65, P6; Peled N., 2017, GITHUB REPOSITORY; Powers R, 2021, SCI TRANSL MED, V13, DOI 10.1126/scitranslmed.abd7865; Prkachin KM, 2008, PAIN, V139, P267, DOI 10.1016/j.pain.2008.04.010; Provenza NR, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/ab2c58; Provenza NR, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00152; Provenza Nicole R, BRAZ J PSYCHIAT, DOI [10.1590/1516-4446-2020-1675(2021, DOI 10.1590/1516-4446-2020-1675(2021]; Scangos KW, 2021, NAT MED, V27, P229, DOI 10.1038/s41591-020-01175-8; Schwabe K, 2021, J NEURAL TRANSM, V128, P215, DOI 10.1007/s00702-020-02297-6; Sellers KK, 2021, FRONT HUM NEUROSCI, V15, DOI 10.3389/fnhum.2021.714256; Sildatke E, 2021, FRONT HUM NEUROSCI, V14, DOI 10.3389/fnhum.2020.627564; Smith EH, 2019, NAT NEUROSCI, V22, P1883, DOI 10.1038/s41593-019-0494-0; Smith EE, 2020, NEUROIMAGE, V220, DOI 10.1016/j.neuroimage.2020.117138; Stanslaski S, 2018, IEEE T BIOMED CIRC S, V12, P1230, DOI 10.1109/TBCAS.2018.2880148; Stenner MP, 2015, J NEUROPHYSIOL, V114, P29, DOI 10.1152/jn.00988.2014; Storch EA, 2015, COMPR PSYCHIAT, V63, P30, DOI 10.1016/j.comppsych.2015.08.007; Storch EA, 2010, J ANXIETY DISORD, V24, P650, DOI 10.1016/j.janxdis.2010.04.010; Sun FT, 2014, EXPERT REV MED DEVIC, V11, P563, DOI 10.1586/17434440.2014.947274; Swann NC, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aabc9b; Topalovic U, 2020, NEURON, V108, P322, DOI 10.1016/j.neuron.2020.08.021; Tyagi H, 2019, BIOL PSYCHIAT, V85, P726, DOI 10.1016/j.biopsych.2019.01.017; Ung HM, 2017, J NEURAL ENG, V14, DOI 10.1088/1741-2552/aa7f40; Voon V, 2017, BRAIN, V140, P442, DOI 10.1093/brain/aww309; Widge AS, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-09557-4; Widge AS, 2017, EXP NEUROL, V287, P461, DOI 10.1016/j.expneurol.2016.07.021; Wingeier B, 2006, EXP NEUROL, V197, P244, DOI 10.1016/j.expneurol.2005.09.016; Wu H, 2018, P NATL ACAD SCI USA, V115, P192, DOI 10.1073/pnas.1712214114; Zhang Z, 2016, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2016.374	93	5	5	2	6	NATURE PORTFOLIO	BERLIN	HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY	1078-8956	1546-170X		NAT MED	Nat. Med.	DEC	2021	27	12					2154	+		10.1038/s41591-021-01550-z	http://dx.doi.org/10.1038/s41591-021-01550-z		DEC 2021	29	Biochemistry & Molecular Biology; Cell Biology; Medicine, Research & Experimental	Science Citation Index Expanded (SCI-EXPANDED)	Biochemistry & Molecular Biology; Cell Biology; Research & Experimental Medicine	XP3FG	34887577	Green Accepted			2022-10-03	WOS:000728466100005
J	Allawala, A; Bijanki, KR; Goodman, W; Cohn, JF; Viswanathan, A; Yoshor, D; Borton, DA; Pouratian, N; Sheth, SA				Allawala, Anusha B.; Bijanki, Kelly R.; Goodman, Wayne K.; Cohn, Jeffrey; Viswanathan, Ashwin; Yoshor, Daniel; Borton, David A.; Pouratian, Nader; Sheth, Sameer A.			In Reply: A Novel Framework for Network-Targeted Neuropsychiatric Deep Brain Stimulation	NEUROSURGERY			English	Letter									[Allawala, Anusha B.; Borton, David A.] Brown Univ, Sch Engn, Providence, RI 02912 USA; [Bijanki, Kelly R.; Viswanathan, Ashwin; Sheth, Sameer A.] Baylor Coll Med, Dept Neurosurg, Houston, TX 77030 USA; [Goodman, Wayne K.] Baylor Coll Med, Menninger Dept Psychiat & Behav Sci, Houston, TX 77030 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA USA; [Yoshor, Daniel] Univ Penn, Dept Neurosurg, Philadelphia, PA 19104 USA; [Pouratian, Nader] Univ Texas Southwestern, Dept Neurol Surg, Dallas, TX USA		Allawala, A (corresponding author), Brown Univ, Sch Engn, Providence, RI 02912 USA.				National Institutes of Health [1UH3NS103549-01]	National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This work was supported by the National Institutes of Health (Grant#1UH3NS103549-01).	Allawala A, 2021, NEUROSURGERY, V89, pE116, DOI 10.1093/neuros/nyab112; Brown NJ, 2021, NEUROSURGERY, V89, pE281, DOI 10.1093/neuros/nyab284	2	0	0	0	1	OXFORD UNIV PRESS INC	CARY	JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA	0148-396X	1524-4040		NEUROSURGERY	Neurosurgery	NOV	2021	89	5					E283	E283		10.1093/neuros/nyab308	http://dx.doi.org/10.1093/neuros/nyab308		AUG 2021	1	Clinical Neurology; Surgery	Science Citation Index Expanded (SCI-EXPANDED)	Neurosciences & Neurology; Surgery	XL8HZ	34383050	Bronze, Green Published			2022-10-03	WOS:000728383600012
J	Allawala, A; Bijanki, KR; Goodman, W; Cohn, JF; Viswanathan, A; Yoshor, D; Borton, DA; Pouratian, N; Sheth, SA				Allawala, Anusha B.; Bijanki, Kelly R.; Goodman, Wayne K.; Cohn, Jeffrey; Viswanathan, Ashwin; Yoshor, Daniel; Borton, David A.; Pouratian, Nader; Sheth, Sameer A.			A Novel Framework for Network-Targeted Neuropsychiatric Deep Brain Stimulation	NEUROSURGERY			English	Article						deep brain stimulation; neuromodulation; depression; stereoelectroencephalography; neuropsychiatry	LARGE-SCALE; CLINICAL-TRIAL; DEPRESSION; DYSFUNCTION; PSYCHIATRY; BIOMARKERS; OUTCOMES; VOLUME	Deep brain stimulation (DBS) has emerged as a promising therapy for neuropsychiatric illnesses, including depression and obsessive-compulsive disorder, but has shown inconsistent results in prior clinical trials. We propose a shift away from the empirical paradigm for developing new DBS applications, traditionally based on testing brain targets with conventional stimulation paradigms. Instead, we propose a multimodal approach centered on an individualized intracranial investigation adapted from the epilepsy monitoring experience, which integrates comprehensive behavioral assessment, such as the Research Domain Criteria proposed by the National Institutes of Mental Health. In this paradigm-shifting approach, we combine readouts obtained from neurophysiology, behavioral assessments, and self-report during broad exploration of stimulation parameters and behavioral tasks to inform the selection of ideal DBS parameters. Such an approach not only provides a foundational understanding of dysfunctional circuits underlying symptom domains in neuropsychiatric conditions but also aims to identify generalizable principles that can ultimately enable individualization and optimization of therapy without intracranial monitoring.	[Allawala, Anusha B.; Borton, David A.] Brown Univ, Sch Engn, Providence, RI 02912 USA; [Bijanki, Kelly R.; Viswanathan, Ashwin; Yoshor, Daniel; Sheth, Sameer A.] Baylor Coll Med, Dept Neurosurg, 7200 Cambridge St,A9-121 BCM650, Houston, TX 77030 USA; [Goodman, Wayne K.] Baylor Coll Med, Menninger Dept Psychiat & Behav Sci, Houston, TX 77030 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Yoshor, Daniel] Univ Penn, Dept Neurosurg, Philadelphia, PA 19104 USA; [Borton, David A.] Brown Univ, Carney Inst Brain Sci, Providence, RI 02912 USA; [Borton, David A.] Providence VA Med Ctr Neurorestorat & Neurotechno, Dept Vet Affairs, Providence, RI USA; [Pouratian, Nader] UT Southwestern Med Ctr, Dept Neurol Surg, Dallas, TX USA		Sheth, SA (corresponding author), Baylor Coll Med, Dept Neurosurg, 7200 Cambridge St,A9-121 BCM650, Houston, TX 77030 USA.	sameer.sheth@bcm.edu		Pouratian, Nader/0000-0002-0426-3241	National Institutes of Health [UH3 NS103549]; McNair Foundation; Dana Foundation; National Science Foundation Graduate Research Fellowship Program	National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); McNair Foundation; Dana Foundation; National Science Foundation Graduate Research Fellowship Program(National Science Foundation (NSF))	This work was supported by the National Institutes of Health award UH3 NS103549 (S.S., N.P., W.G., D.B., J.C., K.B.), the McNair Foundation (S.S.), the Dana Foundation (S.S.), and the National Science Foundation Graduate Research Fellowship Program (A.A.).	Alomar S, 2016, NEUROSURG CLIN N AM, V27, P83, DOI 10.1016/j.nec.2015.08.003; Aman JE, 2020, NEUROBIOL DIS, V139, DOI 10.1016/j.nbd.2020.104819; BANCAUD J, 1970, ELECTROEN CLIN NEURO, V28, P85; Bari AA, 2018, J NEUROL NEUROSUR PS, V89, P886, DOI 10.1136/jnnp-2017-317082; Boccard SGJ, 2017, WORLD NEUROSURG, V106, P625, DOI 10.1016/j.wneu.2017.06.173; Broadway JM, 2012, NEUROPSYCHOPHARMACOL, V37, P1764, DOI 10.1038/npp.2012.23; Butson CR, 2007, NEUROIMAGE, V34, P661, DOI 10.1016/j.neuroimage.2006.09.034; Chang EF, 2015, NEURON, V86, P68, DOI 10.1016/j.neuron.2015.03.037; Chaturvedi A, 2012, BRAIN STIMUL, V5, P369, DOI 10.1016/j.brs.2011.05.002; Chauvel P, 1996, MULTIMETHODOLOGICAL, P80; de Hemptinne C, 2015, NAT NEUROSCI, V18, P779, DOI 10.1038/nn.3997; Dougherty DD, 2015, BIOL PSYCHIAT, V78, P240, DOI 10.1016/j.biopsych.2014.11.023; Drysdale AT, 2017, NAT MED, V23, P28, DOI 10.1038/nm.4246; Gaynes BN, 2009, PSYCHIAT SERV, V60, P1439, DOI 10.1176/ps.2009.60.11.1439; Gupte AA, 2011, STEREOT FUNCT NEUROS, V89, P131, DOI 10.1159/000324906; Hendriks S, 2019, JAMA NEUROL, V76, P1506, DOI 10.1001/jamaneurol.2019.3523; Holtzheimer PE, 2017, LANCET PSYCHIAT, V4, P839, DOI 10.1016/S2215-0366(17)30371-1; Hwang JW, 2015, TRANSL PSYCHIAT, V5, DOI 10.1038/tp.2015.174; Ince NF, 2010, NEUROSURGERY, V67, P390, DOI 10.1227/01.NEU.0000372091.64824.63; Insel T, 2010, AM J PSYCHIAT, V167, P748, DOI 10.1176/appi.ajp.2010.09091379; Kaiser RH, 2015, JAMA PSYCHIAT, V72, P603, DOI 10.1001/jamapsychiatry.2015.0071; Keifer OP, 2014, NEUROSURG CLIN N AM, V25, P671, DOI 10.1016/j.nec.2014.07.009; Lempka SF, 2017, ANN NEUROL, V81, P653, DOI 10.1002/ana.24927; Little S, 2016, J NEUROL NEUROSUR PS, V87, P1388, DOI 10.1136/jnnp-2016-313518; Madler B, 2012, AM J NEURORADIOL, V33, P1072, DOI 10.3174/ajnr.A2906; McGovern RA, 2019, EPILEPSIA, V60, P571, DOI 10.1111/epi.14668; McLoughlin G, 2014, AM J MED GENET B, V165, P111, DOI 10.1002/ajmg.b.32208; Morrell MJ, 2011, NEUROLOGY, V77, P1295, DOI 10.1212/WNL.0b013e3182302056; Murray CJL, 2013, JAMA-J AM MED ASSOC, V310, P591, DOI 10.1001/jama.2013.13805; Ng TH, 2019, TRANSL PSYCHIAT, V9, DOI 10.1038/s41398-019-0644-x; Nuttin B, 2014, J NEUROL NEUROSUR PS, V85, P1003, DOI 10.1136/jnnp-2013-306580; Ostrem JL, 2007, MOVEMENT DISORD, V22, P1885, DOI 10.1002/mds.21580; Penfield W, 1939, NEW ENGL J MED, V221, P209, DOI 10.1056/NEJM193908102210601; Pujara M, 2014, NEUROSCIENTIST, V20, P82, DOI 10.1177/1073858413499407; Rabins P, 2009, ARCH GEN PSYCHIAT, V66, P931, DOI 10.1001/archgenpsychiatry.2009.113; Reese R, 2011, MOVEMENT DISORD, V26, P691, DOI 10.1002/mds.23549; Rush AJ, 2006, AM J PSYCHIAT, V163, P1905, DOI 10.1176/appi.ajp.163.11.1905; Sanger TD, 2018, BRAIN SCI, V8, DOI 10.3390/brainsci8070135; Swann NC, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aabc9b; Tandon N, 2019, JAMA NEUROL, V76, P672, DOI 10.1001/jamaneurol.2019.0098; Tenke CE, 2011, BIOL PSYCHIAT, V70, P388, DOI 10.1016/j.biopsych.2011.02.016; Wade Elizabeth C, 2016, Biol Psychiatry Cogn Neurosci Neuroimaging, V1, P411, DOI 10.1016/j.bpsc.2016.06.002; Webb CA, 2016, NEUROPSYCHOPHARMACOL, V41, P454, DOI 10.1038/npp.2015.165; Widge AS, 2019, AM J PSYCHIAT, V176, P44, DOI 10.1176/appi.ajp.2018.17121358; Widge AS, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00175; Williams LM, 2016, LANCET PSYCHIAT, V3, P472, DOI 10.1016/S2215-0366(15)00579-9; World Health Organization (WHO), 2017, DEPR OTH COMM MENT D; Youngerman BE, 2016, J NEUROSURG, V125, P461, DOI 10.3171/2015.7.JNS142599	48	12	12	1	4	OXFORD UNIV PRESS INC	CARY	JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA	0148-396X	1524-4040		NEUROSURGERY	Neurosurgery	AUG	2021	89	2					E116	E121		10.1093/neuros/nyab112	http://dx.doi.org/10.1093/neuros/nyab112		APR 2021	6	Clinical Neurology; Surgery	Science Citation Index Expanded (SCI-EXPANDED)	Neurosciences & Neurology; Surgery	UA5OK	33913499	Green Published			2022-10-03	WOS:000685210900005
J	Niinuma, K; Ertugrul, IO; Cohn, JF; Jeni, LA				Niinuma, Koichiro; Ertugrul, Itir Onal; Cohn, Jeffrey; Jeni, Laszlo A.			Systematic Evaluation of Design Choices for Deep Facial Action Coding Across Pose	FRONTIERS IN COMPUTER SCIENCE			English	Article						action unit; facial expression coding; design choice in deep learning; AU intensity estimation; AU occurrence detection; cross-pose evaluation; cross-domain evaluation	3D	The performance of automated facial expression coding is improving steadily. Advances in deep learning techniques have been key to this success. While the advantage of modern deep learning techniques is clear, the contribution of critical design choices remains largely unknown, especially for facial action unit occurrence and intensity across pose. Using the The Facial Expression Recognition and Analysis 2017 (FERA 2017) database, which provides a common protocol to evaluate robustness to pose variation, we systematically evaluated design choices in pre-training, feature alignment, model size selection, and optimizer details. Informed by the findings, we developed an architecture that exceeds state-of-the-art on FERA 2017. The architecture achieved a 3.5% increase in F-1 score for occurrence detection and a 5.8% increase in Intraclass Correlation (ICC) for intensity estimation. To evaluate the generalizability of the architecture to unseen poses and new dataset domains, we performed experiments across pose in FERA 2017 and across domains in Denver Intensity of Spontaneous Facial Action (DISFA) and the UNBC Pain Archive.	[Niinuma, Koichiro] Fujitsu Labs Amer, Pittsburgh, PA USA; [Ertugrul, Itir Onal] Tilburg Univ, Dept Cognit Sci & Artificial Intelligence, Tilburg, Netherlands; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Jeni, Laszlo A.] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA		Niinuma, K (corresponding author), Fujitsu Labs Amer, Pittsburgh, PA USA.	kniinuma@fujitsu.com			Fujitsu Laboratories of America; NIH [NS100549, MH096951]; NSF [CNS-1629716]	Fujitsu Laboratories of America; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	This research was supported in part by Fujitsu Laboratories of America, NIH awards NS100549 and MH096951, and NSF award CNS-1629716.	Corneanu CA, 2016, IEEE T PATTERN ANAL, V38, P1548, DOI 10.1109/TPAMI.2016.2515606; Amirian M, 2017, IEEE INT CONF AUTOMA, P854, DOI 10.1109/FG.2017.109; Baltrusaitis T., 2015, 2015 11 IEEE INT C W, V6, P1, DOI DOI 10.1109/FG.2015.7284869; Batista JC, 2017, IEEE INT CONF AUTOMA, P866, DOI 10.1109/FG.2017.111; Cavallo F, 2018, J BIONIC ENG, V15, P185, DOI 10.1007/s42235-018-0015-y; Chu WS, 2019, IMAGE VISION COMPUT, V81, P1, DOI 10.1016/j.imavis.2018.10.002; Cohn J. F., 2015, OXFORD HDB AFFECTIVE, DOI DOI 10.1093/OXFORDHB/9780199942237.001.0001; Cohn JF, 2019, COMPUT VIS PATT REC, P407, DOI 10.1016/B978-0-12-814601-9.00026-2; Ekman P, 2002, RES NEXUS; Ertugrul Itir Onal, 2020, IEEE Trans Biom Behav Identity Sci, V2, P158, DOI [10.1109/TBIOM.2020.2977225, 10.1109/tbiom.2020.2977225]; Ertugrul IO, 2018, IEEE COMPUT SOC CONF, P2211, DOI 10.1109/CVPRW.2018.00287; Ghosh S, 2015, INT CONF AFFECT, P609, DOI 10.1109/ACII.2015.7344632; GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478; He J, 2017, IEEE INT CONF AUTOMA, P848, DOI 10.1109/FG.2017.108; Jeni LA, 2012, IMAGE VISION COMPUT, V30, P785, DOI 10.1016/j.imavis.2012.02.003; King DE, 2009, J MACH LEARN RES, V10, P1755; Kumano S, 2009, INT J COMPUT VISION, V83, P178, DOI 10.1007/s11263-008-0185-x; Li S., 2018, ARXIV180408348, DOI [10.1109/TAFFC.2020.2981446, DOI 10.1109/TAFFC.2020.2981446]; Li W, 2018, IEEE T PATTERN ANAL, V40, P2583, DOI 10.1109/TPAMI.2018.2791608; Li XR, 2017, IEEE INT CONF AUTOMA, P860, DOI 10.1109/FG.2017.110; Lucey P, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P57, DOI 10.1109/FG.2011.5771462; Martinez Brais, 2019, IEEE Transactions on Affective Computing, V10, P325, DOI 10.1109/TAFFC.2017.2731763; Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4; McColl D, 2016, J INTELL ROBOT SYST, V82, P101, DOI 10.1007/s10846-015-0259-2; Niinuma K., 2019, BRIT MACH VIS C BMVC; Parkhi O.M., 2015, P BR MACH VIS, V1, P6, DOI 10.5244/C.29.41; Rudovic O, 2013, IEEE T PATTERN ANAL, V35, P1357, DOI 10.1109/TPAMI.2012.233; Simonyan K., 2014, INT C LEARN REPR, P1; Taheri S, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P306, DOI 10.1109/FG.2011.5771415; Tang CG, 2017, IEEE INT CONF AUTOMA, P878, DOI 10.1109/FG.2017.113; Toser Z, 2016, LECT NOTES COMPUT SC, V9915, P359, DOI 10.1007/978-3-319-49409-8_29; Valstar MF, 2017, IEEE INT CONF AUTOMA, P839, DOI 10.1109/FG.2017.107; Vedaldi A., 2014, INT C LEARNING REPRE; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhang Z, 2016, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2016.374; Zhi RC, 2020, VISUAL COMPUT, V36, P1067, DOI 10.1007/s00371-019-01707-5; Zhou YQ, 2017, IEEE INT CONF AUTOMA, P872, DOI 10.1109/FG.2017.112	38	1	1	0	2	FRONTIERS MEDIA SA	LAUSANNE	AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND		2624-9898		FRONT COMP SCI-SWITZ	Front. Comput. Sci.-Switz	APR 29	2021	3								636094	10.3389/fcomp.2021.636094	http://dx.doi.org/10.3389/fcomp.2021.636094			14	Computer Science, Interdisciplinary Applications	Emerging Sources Citation Index (ESCI)	Computer Science	RX4TF		gold			2022-10-03	WOS:000647217100001
C	Darzi, A; Provenza, NR; Jeni, LA; Borton, DA; Sheth, SA; Goodman, WK; Cohn, JF		Struc, V; Ivanovska, M		Darzi, Ali; Provenza, Nicole R.; Jeni, Laszlo A.; Borton, David A.; Sheth, Sameer A.; Goodman, Wayne K.; Cohn, Jeffrey			Facial Action Units and Head Dynamics in Longitudinal Interviews Reveal OCD and Depression severity and DBS Energy	2021 16TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG 2021)	IEEE International Conference on Automatic Face and Gesture Recognition and Workshops		English	Proceedings Paper	16th IEEE International Conference on Automatic Face and Gesture Recognition (FG)	DEC 15-18, 2021	TIH iHub Drishti, ELECTR NETWORK	IEEE, IEEE Photon Soc, IEEE Biometr Council, Google, NVIDIA, CCS Comp, Mukh Technologies, IEEE Comp Soc	TIH iHub Drishti		METAANALYSIS	Neuromodulation therapy, specifically Deep Brain Stimulation (DBS) of the ventral capsule/ventral striatum (VC/VS), is promising treatment for severe and intractable obsessive-compulsive disorder (OCD). To assess treatment response to DBS, reliable biomarkers are needed. We explored the hypothesis that facial action units and head dynamics in an interview context reveal severity of OCD, related depression, and DBS energy in participants undergoing DBS treatment. Participants were 5 patients (3 females, 2 males) with implanted DBS to VC/VS. They were recorded during brief open-ended interviews by a clinician at pre- and post-surgery baselines and then at 3-month intervals following activation of the DBS electrodes. Facial action units and head dynamics were assessed using AFAR (Automatic Facial Affect Recognition). OCD severity was assessed using clinical interview (YBOCS-II) and depression symptoms were assessed using participant self-report (BDI). After testing for multicollinearity and dropping highly-correlated features, a linear mixed-effects model using chi-square feature selection predicted 61% of the variation in YBOCS-II; 59% of the variation in BDI; and 37% of the variation in delivered energy by DBS to VC/VS. These findings suggest that automatically detected facial action units and head dynamics are potential biomarkers of OCD, depression se verity, and DBS energy.	[Darzi, Ali; Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Provenza, Nicole R.; Borton, David A.] Brown Univ, Sch Engn, Providence, RI 02912 USA; [Jeni, Laszlo A.] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA; [Sheth, Sameer A.] Baylor Coll Med, Dept Neurosurg, Houston, TX 77030 USA; [Goodman, Wayne K.] Baylor Coll Med, Dept Psychiat & Behav Sci, Houston, TX 77030 USA		Darzi, A (corresponding author), Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA.				NIH NINDS BRAIN Initiative [UH3NS100549]; NIH [MH096951]	NIH NINDS BRAIN Initiative; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	The research was supported in part by the NIH NINDS BRAIN Initiative via award UH3NS100549 and NIH award MH096951. DBS devices were donated by Medtronic as part of the BRAIN Initiative Public-Private Partnership Program (PPP).	Baltrusaitis T, 2016, IEEE WINT CONF APPL; CACIOPPO JT, 1988, J PERS SOC PSYCHOL, V54, P592, DOI 10.1037/0022-3514.54.4.592; Campbell K, 2019, AUTISM, V23, P619, DOI 10.1177/1362361318766247; Cohn J. F, 2018, P 2018 INT C MULT IN, P40; Cummins N, 2015, SPEECH COMMUN, V71, P10, DOI 10.1016/j.specom.2015.03.004; Dibeklioglu H, 2018, IEEE J BIOMED HEALTH, V22, P525, DOI 10.1109/JBHI.2017.2676878; Ding YH, 2020, COMPANION PUBLICATON OF THE 2020 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION (ICMI '20 COMPANION), P354, DOI 10.1145/3395035.3425354; Ekman P, 2002, NETWORK RES INFORM; Ertugrul Itir Onal, 2019, 2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019), DOI 10.1109/FG.2019.8756623; Ertugrul I. O, 2020, IEEE T BIOMETRICS BE; Feusner JD, 2007, ARCH GEN PSYCHIAT, V64, P1417, DOI 10.1001/archpsyc.64.12.1417; Fitzmaurice GM, 2008, CIRCULATION, V118, P2005, DOI 10.1161/CIRCULATIONAHA.107.714618; Gavrilescu M, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19173693; Girard JM, 2014, IMAGE VISION COMPUT, V32, P641, DOI 10.1016/j.imavis.2013.12.007; Goodman WK, 2020, AM J PSYCHIAT, V177, P200, DOI 10.1176/appi.ajp.2020.20010037; Hammal Z, 2015, FRONTIERS ICT, V2, P12; Harati S, 2020, IEEE J BIOMED HEALTH, V24, P815, DOI 10.1109/JBHI.2019.2930604; Iguyon I, 2003, INTRO VARIABLE FEATU; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Kacem A, 2018, IEEE INT CONF AUTOMA, P739, DOI 10.1109/FG.2018.00116; Li BJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02116; Martin KB, 2018, MOL AUTISM, V9, DOI 10.1186/s13229-018-0198-4; McAuley MD, 2020, BRAIN STIMUL, V13, P1414, DOI 10.1016/j.brs.2020.07.020; McCall Micaela V, 2020, Neurol Psychiatry Brain Res, V37, P33, DOI 10.1016/j.npbr.2020.05.002; Niinuma K, 2013, 2013 IEEE SIXTH INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS (BTAS); Ost LG, 2015, CLIN PSYCHOL REV, V40, P156, DOI 10.1016/j.cpr.2015.06.003; Provenza NR, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00152; Rasmussen SA, 2018, BIOL PSYCHIAT, V84, P355, DOI 10.1016/j.biopsych.2017.11.034; Romanelli RJ, 2014, DEPRESS ANXIETY, V31, P641, DOI 10.1002/da.22232; Storch EA, 2010, PSYCHOL ASSESSMENT, V22, P223, DOI 10.1037/a0018492; Tsung-Hsien Yang, 2016, 2016 International Conference on Orange Technologies (ICOT), P5, DOI 10.1109/ICOT.2016.8278966; Widge AS, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-09557-4; Zhang Z, 2016, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2016.374	33	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2326-5396		978-1-6654-3176-7	IEEE INT CONF AUTOMA			2021														6	Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering; Imaging Science & Photographic Technology	BS9ZW					2022-10-03	WOS:000784811600093
C	Niinuma, K; Ertugrul, IO; Cohn, JF; Jeni, LA			IEEE	Niinuma, Koichiro; Ertugrul, Itir Onal; Cohn, Jeffrey; Jeni, Laszlo A.			Synthetic Expressions are Better Than Real for Learning to Detect Facial Actions	2021 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV 2021)	IEEE Winter Conference on Applications of Computer Vision		English	Proceedings Paper	IEEE Winter Conference on Applications of Computer Vision (WACV)	JAN 05-09, 2021	ELECTR NETWORK	IEEE, IEEE Comp Soc, Adobe, Amazon, iRobot, Kitware, Verisk				Critical obstacles in training classifiers to detect facial actions are the limited sizes of annotated video databases and the relatively low frequencies of occurrence of many actions. To address these problems, we propose an approach that makes use of facial expression generation. Our approach reconstructs the 3D shape of the face from each video frame, aligns the 3D mesh to a canonical view, and then trains a GAN-based network to synthesize novel images with facial action units of interest. To evaluate this approach, a deep neural network was trained on two separate datasets: One network was trained on video of synthesized facial expressions generated from FERA17; the other network was trained on unaltered video from the same database. Both networks used the same train and validation partitions and were tested on the test partition of actual video from FERA17. The network trained on synthesized facial expressions outperformed the one trained on actual facial expressions and surpassed current state-of-the-art approaches.	[Niinuma, Koichiro] Fujitsu Labs Amer, Pittsburgh, PA 15213 USA; [Ertugrul, Itir Onal] Tilburg Univ, Tilburg, Netherlands; [Cohn, Jeffrey] Univ Pittsburgh, Pittsburgh, PA USA; [Jeni, Laszlo A.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA		Niinuma, K (corresponding author), Fujitsu Labs Amer, Pittsburgh, PA 15213 USA.	kniinuma@fujitsu.com; i.onal@uvt.nl; jeffcohn@pitt.edu; laszlojeni@cmu.edu			Fujitsu Laboratories; NIH [NS100549, MH096951]; NSF [CNS-1629716]	Fujitsu Laboratories; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	This research was supported in part by Fujitsu Laboratories, NIH awards NS100549 and MH096951, and NSF award CNS-1629716.	Abbasnejad I, 2017, IEEE INT CONF COMP V, P1609, DOI 10.1109/ICCVW.2017.189; Ambadar Z, 2005, PSYCHOL SCI, V16, P403, DOI 10.1111/j.0956-7976.2005.01548.x; Amirian M, 2017, IEEE INT CONF AUTOMA, P854, DOI 10.1109/FG.2017.109; Batista JC, 2017, IEEE INT CONF AUTOMA, P866, DOI 10.1109/FG.2017.111; Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Chu WS, 2019, IMAGE VISION COMPUT, V81, P1, DOI 10.1016/j.imavis.2018.10.002; Cohn JF., 2005, NEW HDB METHODS NONV, P9; Ding H, 2018, AAAI CONF ARTIF INTE, P6781; Ekman P, 2002, FACIAL ACTION CODING, V1; Ertugrul Itir Onal, 2020, IEEE T BIOMETRICS BE; Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33; Geng Z., 2019, P IEEE C COMP VIS PA, P9821; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Hensel M, 2017, ADV NEUR IN, V30; Jeni LA, 2016, LECT NOTES COMPUT SC, V9914, P511, DOI 10.1007/978-3-319-48881-3_35; Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241; King DE, 2009, J MACH LEARN RES, V10, P1755; Kollias D., 2019, ARXIV; Kollias D, 2020, INT J COMPUT VISION, V128, P1455, DOI 10.1007/s11263-020-01304-3; Li W, 2017, IEEE INT CONF AUTOMA, P103, DOI 10.1109/FG.2017.136; Li Y, 2019, PROC CVPR IEEE, P10916, DOI 10.1109/CVPR.2019.01118; Liu ZL, 2020, LECT NOTES COMPUT SC, V11962, P514, DOI 10.1007/978-3-030-37734-2_42; Lucey P, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P57, DOI 10.1109/FG.2011.5771462; Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4; McKeown G, 2015, EMOT REV, V7, P30, DOI 10.1177/1754073914544475; Niinuma Koichiro, 2019, BMVC, V2019; Niu XS, 2019, ADV NEUR IN, V32; Peng GZ, 2018, PROC CVPR IEEE, P2188, DOI 10.1109/CVPR.2018.00233; Pumarola A., 2019, INT J COMPUTER VISIO; Qiao F., 2018, ARXIV180201822; Ruiz A, 2015, IEEE I CONF COMP VIS, P3703, DOI 10.1109/ICCV.2015.422; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Song LX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P627, DOI 10.1145/3240508.3240612; Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962; Valstar MF, 2017, IEEE INT CONF AUTOMA, P839, DOI 10.1109/FG.2017.107; Wu BY, 2015, PATTERN RECOGN, V48, P2279, DOI 10.1016/j.patcog.2015.01.022; Xinyue Zhu, 2018, Advances in Knowledge Discovery and Data Mining. 22nd Pacific-Asia Conference, PAKDD 2018. Proceedings: LNAI 10939, P349, DOI 10.1007/978-3-319-93040-4_28; Zeng JB, 2015, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2015.413; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhang Y, 2018, PROC CVPR IEEE, P5108, DOI 10.1109/CVPR.2018.00536; Zhang Y, 2018, PROC CVPR IEEE, P2314, DOI 10.1109/CVPR.2018.00246; Zhang Z, 2016, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2016.374; Zhang Zheng, 2018, IDENTITY BASED ADVER, P226; Zhao KL, 2018, PROC CVPR IEEE, P2090, DOI 10.1109/CVPR.2018.00223; Zhou YQ, 2017, IEEE INT CONF AUTOMA, P872, DOI 10.1109/FG.2017.112	46	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	2472-6737		978-0-7381-4266-1	IEEE WINT CONF APPL			2021							1247	1256		10.1109/WACV48630.2021.00129	http://dx.doi.org/10.1109/WACV48630.2021.00129			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering; Imaging Science & Photographic Technology	BS1KX		Green Submitted			2022-10-03	WOS:000692171000124
C	Vail, AK; Girard, J; Bylsma, L; Cohn, JF; Fournier, J; Swartz, H; Morency, LP		Struc, V; Ivanovska, M		Vail, Alexandria K.; Girard, Jeffrey M.; Bylsma, Lauren; Cohn, Jeffrey; Fournier, Jay; Swartz, Holly; Morency, Louis-Philippe			Goals, Tasks, and Bonds: Toward the Computational Assessment of Therapist Versus Client Perception of Working Alliance	2021 16TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG 2021)	IEEE International Conference on Automatic Face and Gesture Recognition and Workshops		English	Proceedings Paper	16th IEEE International Conference on Automatic Face and Gesture Recognition (FG)	DEC 15-18, 2021	TIH iHub Drishti, ELECTR NETWORK	IEEE, IEEE Photon Soc, IEEE Biometr Council, Google, NVIDIA, CCS Comp, Mukh Technologies, IEEE Comp Soc	TIH iHub Drishti		RETROSPECTIVE RECALL; THERAPEUTIC ALLIANCE; RAPPORT; PSYCHOTHERAPY	Early client dropout is one of the most significant challenges facing psychotherapy: recent studies suggest that at least one in five clients will leave treatment prematurely. Clients may terminate therapy for various reasons, but one of the most common causes is the lack of a strong working alliance. The concept of working alliance captures the collaborative relationship between a client and their therapist when working toward the progress and recovery of the client seeking treatment. Unfortunately, clients are often unwilling to directly express dissatisfaction in care until they have already decided to terminate therapy. On the other side, therapists may miss subtle signs of client discontent during treatment before it is too late. In this work, we demonstrate that nonverbal behavior analysis may aid in bridging this gap. The present study focuses primarily on the head gestures of both the client and therapist, contextualized within conversational turn-taking actions between the pair during psychotherapy sessions. We identify multiple behavior patterns suggestive of an individual's perspective on the working alliance; interestingly, these patterns also differ between the client and the therapist. These patterns inform the development of predictive models for self-reported ratings of working alliance, which demonstrate significant predictive power for both client and therapist ratings. Future applications of such models may stimulate preemptive intervention to strengthen a weak working alliance, whether explicitly attempting to repair the existing alliance or establishing a more suitable client-therapist pairing, to ensure that clients encounter fewer barriers to receiving the treatment they need.	[Vail, Alexandria K.] Carnegie Mellon Univ, Human Comp Interact Inst, Pittsburgh, PA 15213 USA; [Girard, Jeffrey M.] Univ Kansas, Dept Psychol, Lawrence, KS 66045 USA; [Bylsma, Lauren; Swartz, Holly] Univ Pittsburgh, Dept Psychiat, Pittsburgh, PA USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Fournier, Jay] Ohio State Univ, Dept Psychiat, Columbus, OH 43210 USA; [Morency, Louis-Philippe] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA		Vail, AK (corresponding author), Carnegie Mellon Univ, Human Comp Interact Inst, Pittsburgh, PA 15213 USA.			Girard, Jeffrey M./0000-0002-7359-3746	Center for Machine Learning and Health at Carnegie Mellon University, National Science Foundation [1722822, 1750439]; National Institutes of Health [R01MH125740, R01MH096951, UL1TR001857, U01MH116925]	Center for Machine Learning and Health at Carnegie Mellon University, National Science Foundation; National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This material is based upon work partially supported by The Center for Machine Learning and Health at Carnegie Mellon University, National Science Foundation awards #1722822 and #1750439, and National Institutes of Health awards R01MH125740, R01MH096951, UL1TR001857, and U01MH116925. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors, and no official endorsement should be inferred.	Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019; Bickmore T., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P396; Bordin Edward S., 1979, PSYCHOTHERAPY THEORY, V16, P252, DOI [DOI 10.1037/H0085885, 10.1037/h0085885]; Breiman L., 2001, MACH LEARN, V5, P5, DOI [10.1023/A:1010933404324, DOI 10.1023/A:1010933404324]; Capretto T., 2021, ARXIV201210754STAT; Cassell J, 2004, J APPL DEV PSYCHOL, V25, P75, DOI 10.1016/j.appdev.2003.11.003; Cassell J, 2003, USER MODEL USER-ADAP, V13, P89, DOI 10.1023/A:1024026532471; Cassell J, 1999, APPL ARTIF INTELL, V13, P519, DOI 10.1080/088395199117360; Cohn Jeffrey F., 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPR.2009.5204260; Drucker H, 1997, ADV NEUR IN, V9, P155; Eyben F, 2013, INT CONF ACOUST SPEE, P483, DOI 10.1109/ICASSP.2013.6637694; FRANK AF, 1990, ARCH GEN PSYCHIAT, V47, P228; Gaston L., 1991, PSYCHOTHER RES, V1, P104, DOI DOI 10.1080/10503309112331335531; Gelman A., 2015, BAYESIAN DATA ANAL; Gratch J, 2007, LECT NOTES ARTIF INT, V4722, P125; Hamaker EL, 2020, PSYCHOL METHODS, V25, P365, DOI 10.1037/met0000239; HAMILTON M, 1960, J NEUROL NEUROSUR PS, V23, P56, DOI 10.1136/jnnp.23.1.56; Hatcher RL, 2006, PSYCHOTHER RES, V16, P12, DOI 10.1080/10503300500352500; Hill C., 1992, PSYCHOTHER RES, V2, P143, DOI [10.1080/10503309212331332914, DOI 10.1080/10503309212331332914]; Hill CE, 1996, J COUNS PSYCHOL, V43, P207, DOI 10.1037/0022-0167.43.2.207; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Horvath A.O., 1986, PSYCHOTHERAPEUTIC PR, P529; HORVATH AO, 1991, J COUNS PSYCHOL, V38, P139, DOI 10.1037/0022-0167.38.2.139; Jongejan B., 2016, P 4 EUR 7 NORD S MUL, P8; Kapoor A., 2001, P 2001 WORKSH PERC U, P1; KOKOTOVIC AM, 1990, J COUNS PSYCHOL, V37, P16, DOI 10.1037/0022-0167.37.1.16; Kruschke J., 2015, DOING BAYESIAN DATA; Lambert M. J., 2013, BERGIN GAR FIELDS HD, V6th; LUBORSKY L, 1985, ARCH GEN PSYCHIAT, V42, P602; Makowski D, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02767; Martin DJ, 2000, J CONSULT CLIN PSYCH, V68, P438, DOI 10.1037//0022-006X.68.3.438; Meijer E, 2008, HDB MULTILEVEL ANAL; Neal R. M., 1993, CRGTR931 U TOR DEP C; NOWICKI S, 1994, J NONVERBAL BEHAV, V18, P9, DOI 10.1007/BF02169077; Provost EM, 2015, IEEE T AFFECT COMPUT, V6, P395, DOI 10.1109/TAFFC.2015.2407898; RENNIE DL, 1994, J COUNS PSYCHOL, V41, P427, DOI 10.1037/0022-0167.41.4.427; RHODES RH, 1994, J COUNS PSYCHOL, V41, P473, DOI 10.1037/0022-0167.41.4.473; Riek LD, 2010, J MULTIMODAL USER IN, V3, P99, DOI 10.1007/s12193-009-0028-2; Ringeval F., 2017, P 7 ANN WORKSH AUD V, P3, DOI DOI 10.1145/3133944.3133953; Salvatier J, 2016, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.55; SAUNDERS S. M., 1989, PSYCHOL ASSESSMENT J, V1, P323, DOI DOI 10.1037/1040-3590.1.4.323; Tickle-Degnen L., 1990, PSYCHOL INQ, V1, P285, DOI [DOI 10.1207/S15327965PLI0104_1, 10.1207/s15327965pli0104_1]; TSUI P, 1985, AM J ORTHOPSYCHIAT, V55, P561, DOI 10.1111/j.1939-0025.1985.tb02706.x; Wei H., 2013, P 14 INT WORKSH IM A, P1; Zadeh A, 2017, IEEE INT CONF COMP V, P2519, DOI 10.1109/ICCVW.2017.296; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	46	0	0	1	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2326-5396		978-1-6654-3176-7	IEEE INT CONF AUTOMA			2021														8	Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering; Imaging Science & Photographic Technology	BS9ZW	35937037				2022-10-03	WOS:000784811600087
J	Chen, M; Chow, SM; Hammal, Z; Messinger, DS; Cohn, JF				Chen, Meng; Chow, Sy-Miin; Hammal, Zakia; Messinger, Daniel S.; Cohn, Jeffrey			A Person- and Time-Varying Vector Autoregressive Model to Capture Interactive Infant-Mother Head Movement Dynamics	MULTIVARIATE BEHAVIORAL RESEARCH			English	Article						Time-varying parameters; vector autoregressive models; state-space models; generalized additive models; parent-infant interactions; head movements; still-face paradigm	STILL-FACE; PARAMETERS; REGRESSION; ATTACHMENT; LIKELIHOOD; BEHAVIOR; SIGNALS	Head movement is an important but often overlooked component of emotion and social interaction. Examination of regularity and differences in head movements of infant-mother dyads over time and across dyads can shed light on whether and how mothers and infants alter their dynamics over the course of an interaction to adapt to each others. One way to study these emergent differences in dynamics is to allow parameters that govern the patterns of interactions to change over time, and according to person- and dyad-specific characteristics. Using two estimation approaches to implement variations of a vector-autoregressive model with time-varying coefficients, we investigated the dynamics of automatically-tracked head movements in mothers and infants during the Face-Face/Still-Face Procedure (SFP) with 24 infant-mother dyads. The first approach requires specification of a confirmatory model for the time-varying parameters as part of a state-space model, whereas the second approach handles the time-varying parameters in a semi-parametric ("mostly" model-free) fashion within a generalized additive modeling framework. Results suggested that infant-mother head movement dynamics varied in time both within and across episodes of the SFP, and varied based on infants' subsequently-assessed attachment security. Code for implementing the time-varying vector-autoregressive model using two R packages,dynrandmgcv, is provided.	[Chen, Meng; Chow, Sy-Miin] Penn State Univ, Human Dev & Family Studies, 420 Biobehav Hlth Bldg,Univ Pk, State Coll, PA 16802 USA; [Hammal, Zakia; Cohn, Jeffrey] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Messinger, Daniel S.] Univ Miami, Dept Psychol, Coral Gables, FL 33124 USA; [Messinger, Daniel S.] Univ Miami, Dept Pediat, Coral Gables, FL 33124 USA; [Messinger, Daniel S.] Univ Miami, Dept Mus Engn, Coral Gables, FL 33124 USA; [Messinger, Daniel S.] Univ Miami, Dept Elect, Coral Gables, FL 33124 USA; [Messinger, Daniel S.] Univ Miami, Dept Comp Engn, Coral Gables, FL 33124 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychiat, Pittsburgh, PA 15260 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Intelligent Syst, Pittsburgh, PA 15260 USA		Chen, M (corresponding author), Penn State Univ, Human Dev & Family Studies, 420 Biobehav Hlth Bldg,Univ Pk, State Coll, PA 16802 USA.	mengc2013@gmail.com	Chen, Meng/CAF-2307-2022	Chen, Meng/0000-0002-1311-5101; Chen, Meng/0000-0002-1490-5463				Ainsworth M. D. S., 1978, PATTERNS ATTACHMENT; Akaike H, 1973, P 2 INT S INFORM THE, P261; Anderson B. D. O., 1979, OPTIMAL FILTERING; ANSLEY CF, 1985, ANN STAT, V13, P1286, DOI 10.1214/aos/1176349739; Asparouhov T, 2018, STRUCT EQU MODELING, V25, P359, DOI 10.1080/10705511.2017.1406803; Bar-Shalom Y., 2001, ESTIMATION APPL TRAC, DOI [10.1002/0471221279, DOI 10.1002/0471221279]; Beebe B, 2016, DEV PSYCHOL, V52, P556, DOI 10.1037/a0040085; Beebe B, 2010, ATTACH HUM DEV, V12, P3, DOI 10.1080/14616730903338985; Bertenthal B. I., 2007, DATA ANAL TECHNIQUES, P1; Bosma H. A., 2001, IDENTITY EMOTION DEV, DOI [10.1017/CBO9780511598425, DOI 10.1017/CBO9780511598425]; Bringmann LF, 2018, MULTIVAR BEHAV RES, V53, P293, DOI 10.1080/00273171.2018.1439722; Bringmann LF, 2017, PSYCHOL METHODS, V22, P409, DOI 10.1037/met0000085; Calkins SD, 2011, NATL SYMP FAM ISS, P49, DOI 10.1007/978-1-4419-7361-0_3; Cao JG, 2012, J COMPUT GRAPH STAT, V21, P42, DOI 10.1198/jcgs.2011.10021; Carpenter B, 2017, J STAT SOFTW, V76, P1, DOI 10.18637/jss.v076.i01; Chow SM, 2008, STAT NEERL, V62, P131, DOI 10.1111/j.1467-9574.2007.00379.x; Chow SM, 2007, MULTIVAR BEHAV RES, V42, P283, DOI 10.1080/00273170701360423; Chow SM, 2019, MULTIVAR BEHAV RES, V54, P690, DOI 10.1080/00273171.2019.1566050; Chow SM, 2018, PSYCHOMETRIKA, V83, P476, DOI 10.1007/s11336-018-9605-1; Chow SM, 2011, MULTIVAR BEHAV RES, V46, P303, DOI 10.1080/00273171.2011.563697; Chow SM, 2010, STRUCT EQU MODELING, V17, P303, DOI 10.1080/10705511003661553; Chow SM, 2010, EMOTION, V10, P101, DOI 10.1037/a0017824; Chow SM, 2009, BRIT J MATH STAT PSY, V62, P683, DOI 10.1348/000711008X384080; COHN JF, 1988, DEV PSYCHOL, V24, P386, DOI 10.1037/0012-1649.24.3.386; Cox M., 2013, 10 IEEE INT C AUT FA; De Haan-Rietdijk S, 2016, PSYCHOMETRIKA, V81, P217, DOI 10.1007/s11336-014-9417-x; Del Negro M., 2008, STAFF REPORTS; Driver CC, 2018, PSYCHOL METHODS, V23, P774, DOI 10.1037/met0000168; DUNCAN S, 1972, J PERS SOC PSYCHOL, V23, P283, DOI 10.1037/h0033031; Dziak JJ, 2019, STAT SURV, V13, P150, DOI 10.1214/19-SS126; EKMAN P, 1974, J PERS SOC PSYCHOL, V29, P288, DOI 10.1037/h0036006; Feldman R, 2003, INFANT MENT HEALTH J, V24, P1, DOI 10.1002/imhj.10041; Gates K. M., ANAL INTRAINDIVIDUAL; Girard JM, 2015, PATTERN RECOGN LETT, V66, P13, DOI 10.1016/j.patrec.2014.10.004; Haley DW, 2003, CHILD DEV, V74, P1534, DOI 10.1111/1467-8624.00621; Hamaker EL, 2018, MULTIVAR BEHAV RES, V53, P820, DOI 10.1080/00273171.2018.1446819; Hamaker EL, 2009, PSYCHOMETRIKA, V74, P727, DOI 10.1007/s11336-009-9113-4; Hammal Z., 2015, FRONT ICT, V2, P21, DOI [10.3389/fict.2015.00021, DOI 10.3389/FICT.2015.00021]; Hammal Z, 2015, IEEE T AFFECT COMPUT, V6, P361, DOI 10.1109/TAFFC.2015.2422702; Hammal Z, 2014, IEEE T AFFECT COMPUT, V5, P155, DOI 10.1109/TAFFC.2014.2326408; Harvey A. C., 2001, FORECASTING STRUCTUR; HASTIE T, 1993, J ROY STAT SOC B MET, V55, P757; Hurez I, 2018, CAS 2018 PROCEEDINGS: 2018 INTERNATIONAL SEMICONDUCTOR CONFERENCE, P205, DOI 10.1109/SMICND.2018.8539764; Jaffe J, 2001, MONOGR SOC RES CHILD, V66, P1, DOI 10.1111/1540-5834.00137; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Johnson S.G., 2014, NLOPT NONLINEAR OPTI; Jokinen K., 2010, P 2010 WORKSH EYE GA, P118, DOI 10.1145/2002333.2002352; Kalman RE., 1960, J BASIC ENG-T ASME, V82, P35, DOI 10.1115/1.3662552; Kleinsmith A, 2013, IEEE T AFFECT COMPUT, V4, P15, DOI 10.1109/T-AFFC.2012.16; KOPP CB, 1982, DEV PSYCHOL, V18, P199, DOI 10.1037/0012-1649.18.2.199; KRAFT D, 1994, ACM T MATH SOFTWARE, V20, P262, DOI 10.1145/192115.192124; Kraft D, 1988, SOFTWARE PACKAGE SEQ; Kuppens P, 2010, PSYCHOL SCI, V21, P984, DOI 10.1177/0956797610372634; Lewis M. D., 2001, IDENTITY EMOTION DEV, P177, DOI DOI 10.1017/CBO9780511598425; Li R, 2015, TVEM TIME VARYING EF; Li R., 2014, TVEM TIME VARYING EF; Liang H, 2010, ANN APPL STAT, V4, P460, DOI 10.1214/09-AOAS290; Magnusson D., 1996, DEVELOPMENTAL SCI, P7, DOI DOI 10.1017/CBO9780511571114.003; Marra G, 2012, SCAND J STAT, V39, P53, DOI 10.1111/j.1467-9469.2011.00760.x; McKeown GJ, 2014, PSYCHOL METHODS, V19, P155, DOI 10.1037/a0034282; MICHEL GF, 1992, INFANT BEHAV DEV, V15, P347, DOI 10.1016/0163-6383(92)80004-E; Minagawa-Kawai Y, 2009, CEREB CORTEX, V19, P284, DOI 10.1093/cercor/bhn081; MOLENAAR PCM, 1992, PSYCHOMETRIKA, V57, P333, DOI 10.1007/BF02295422; MOLENAAR PCM, 1987, MULTIVAR BEHAV RES, V22, P329, DOI 10.1207/s15327906mbr2203_5; Molenaar PCM., 1994, ADV PSYCHOPHYSIOLOGY, P229; Molenaar PCM, 2019, PREV SCI, V20, P442, DOI 10.1007/s11121-018-0919-0; Molenaar PCM, 2009, DEV PSYCHOL, V45, P260, DOI 10.1037/a0014170; Newell A, 1990, UNIFIED THEORIES COG; Nowak A., 1994, DYNAMICAL SYSTEMS SO, P17; Ou L, 2019, R J, V11, P91, DOI 10.32614/rj-2019-012; Prado R, 2001, J ROY STAT SOC C-APP, V50, P95, DOI 10.1111/1467-9876.00222; R Core Team, 2018, R LANG ENV STAT COMP; Rajan JJ, 1996, IEEE T SIGNAL PROCES, V44, P2498, DOI 10.1109/78.539034; RICHTERS JE, 1988, CHILD DEV, V59, P512, DOI 10.1111/j.1467-8624.1988.tb01485.x; ROTHBART M K, 1992, P7; Rubin D. B., 2002, STAT ANAL MISSING DA, DOI 10.1002/9781119013563; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; SCHWEPPE FC, 1965, IEEE T INFORM THEORY, V11, P61, DOI 10.1109/TIT.1965.1053737; Scott Kelso J. A., 1995, DYNAMIC PATTERNS SEL, P1; Shiyko M, 2014, NICOTINE TOB RES, V16, pS144, DOI 10.1093/ntr/ntt109; Tarvainen MP, 2006, PHYSIOL MEAS, V27, P225, DOI 10.1088/0967-3334/27/3/002; Tarvainen MP, 2004, IEEE T BIO-MED ENG, V51, P516, DOI 10.1109/TBME.2003.821029; TODA S, 1993, DEV PSYCHOL, V29, P532, DOI 10.1037/0012-1649.29.3.532; TRONICK E, 1978, J AM ACAD CHILD PSY, V17, P1, DOI 10.1016/S0002-7138(09)62273-1; TURVEY MT, 1990, AM PSYCHOL, V45, P938, DOI 10.1037/0003-066X.45.8.938; van Geert P. L. C., 2018, INT J BEHAV DEV B, V1, P5; Vasilenko SA, 2014, NICOTINE TOB RES, V16, pS135, DOI 10.1093/ntr/ntt185; WAHBA G, 1978, J ROY STAT SOC B MET, V40, P364; Wallbott HG, 1998, EUR J SOC PSYCHOL, V28, P879, DOI 10.1002/(SICI)1099-0992(1998110)28:6<879::AID-EJSP901>3.0.CO;2-W; Wang Qian, 2014, J Diabetes Sci Technol, V8, P331; Weiss A. A., 1985, J TIME SER ANAL, V6, P181, DOI DOI 10.1111/j.1467-9892.1985.tb00408.x; Wood S. N., 2019, PACKAGE MGCV COMPUTE; Wood SN, 2013, BIOMETRIKA, V100, P221, DOI 10.1093/biomet/ass048; Wood SN, 2011, J R STAT SOC B, V73, P3, DOI 10.1111/j.1467-9868.2010.00749.x; Wood SN, 2003, J ROY STAT SOC B, V65, P95, DOI 10.1111/1467-9868.00374; Wu C. O., 2018, NONPARAMETRIC MODELS, DOI [10.1201/b20631, DOI 10.1201/B20631]; Yee TW, 2015, SPRINGER SER STAT, pCP2, DOI 10.1007/978-1-4939-2818-7; Zhu HH, 2007, J COMPUT GRAPH STAT, V16, P813, DOI 10.1198/106186007X255991	98	6	7	2	8	ROUTLEDGE JOURNALS, TAYLOR & FRANCIS LTD	ABINGDON	2-4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND	0027-3171	1532-7906		MULTIVAR BEHAV RES	Multivariate Behav. Res.	OCT 29	2021	56	5					739	767		10.1080/00273171.2020.1762065	http://dx.doi.org/10.1080/00273171.2020.1762065		JUN 2020	29	Mathematics, Interdisciplinary Applications; Social Sciences, Mathematical Methods; Psychology, Experimental; Statistics & Probability	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Mathematics; Mathematical Methods In Social Sciences; Psychology	WN9OF	32530313	Green Accepted			2022-10-03	WOS:000545469800001
J	Vanderver, A; Bernard, G; Helman, G; Sherbini, O; Boeck, R; Cohn, JF; Collins, A; Demarest, S; Dobbins, K; Emrick, L; Fraser, J; Masser-Frye, D; Hayward, J; Karmarkar, S; Keller, S; Mirrop, S; Mitchell, W; Pathak, S; Sherr, E; van Haren, K; Waters, E; Wilson, JL; Zhorne, L; Schiffmann, R; van der Knaap, MS; Pizzino, A; Dubbs, H; Shults, J; Simons, C; Taft, RJ				Vanderver, Adeline; Bernard, Genevieve; Helman, Guy; Sherbini, Omar; Boeck, Ryan; Cohn, Jeffrey; Collins, Abigail; Demarest, Scott; Dobbins, Katherine; Emrick, Lisa; Fraser, Jamie; Masser-Frye, Diane; Hayward, Jean; Karmarkar, Swati; Keller, Stephanie; Mirrop, Samuel; Mitchell, Wendy; Pathak, Sheel; Sherr, Elliott; van Haren, Keith; Waters, Erica; Wilson, Jenny L.; Zhorne, Leah; Schiffmann, Raphael; van der Knaap, Marjo S.; Pizzino, Amy; Dubbs, Holly; Shults, Justine; Simons, Cas; Taft, Ryan J.		LeukoSEQ Workgrp	Randomized Clinical Trial of First-Line Genome Sequencing in Pediatric White Matter Disorders	ANNALS OF NEUROLOGY			English	Article							LEUKODYSTROPHIES; CLASSIFICATION; DIAGNOSIS; VERSATILE; VARIANTS; SYSTEM	Objective Genome sequencing (GS) is promising for unsolved leukodystrophies, but its efficacy has not been prospectively studied. Methods A prospective time-delayed crossover design trial of GS to assess the efficacy of GS as a first-line diagnostic tool for genetic white matter disorders took place between December 1, 2015 and September 27, 2017. Patients were randomized to receive GS immediately with concurrent standard of care (SoC) testing, or to receive SoC testing for 4 months followed by GS. Results Thirty-four individuals were assessed at interim review. The genetic origin of 2 patient's leukoencephalopathy was resolved before randomization. Nine patients were stratified to the immediate intervention group and 23 patients to the delayed-GS arm. The efficacy of GS was significant relative to SoC in the immediate (5/9 [56%] vs 0/9 [0%]; Wild-Seber, p < 0.005) and delayed (control) arms (14/23 [61%] vs 5/23 [22%]; Wild-Seber, p < 0.005). The time to diagnosis was significantly shorter in the immediate-GS group (log-rank test, p = 0.04). The overall diagnostic efficacy of combined GS and SoC approaches was 26 of 34 (76.5%, 95% confidence interval = 58.8-89.3%) in <4 months, greater than historical norms of <50% over 5 years. Owing to loss of clinical equipoise, the trial design was altered to a single-arm observational study. Interpretation In this study, first-line GS provided earlier and greater diagnostic efficacy in white matter disorders. We provide an evidence-based diagnostic testing algorithm to enable appropriate clinical GS utilization in this population. ANN NEUROL 2020	[Vanderver, Adeline; Sherbini, Omar; Pizzino, Amy; Dubbs, Holly] Childrens Hosp Philadelphia, Div Neurol, 3415 Civ Ctr Blvd, Philadelphia, PA 19104 USA; [Vanderver, Adeline] Univ Penn, Perelman Sch Med, Philadelphia, PA 19104 USA; [Bernard, Genevieve] McGill Univ, Dept Neurol & Neurosurg, Montreal, PQ, Canada; [Bernard, Genevieve] McGill Univ, Dept Pediat, Montreal, PQ, Canada; [Bernard, Genevieve] McGill Univ, Dept Human Genet, Montreal, PQ, Canada; [Bernard, Genevieve] McGill Univ Hlth Ctr, Montreal Childrens Hosp, Dept Med Genet, Montreal, PQ, Canada; [Bernard, Genevieve] McGill Univ Hlth Ctr, Child Hlth & Human Dev Program, Res Inst, Montreal, PQ, Canada; [Helman, Guy; Simons, Cas] Univ Queensland, Inst Mol Biosci, Brisbane, Qld, Australia; [Helman, Guy; Simons, Cas] Royal Childrens Hosp, Murdoch Childrens Res Inst, Melbourne, Vic, Australia; [Boeck, Ryan] Child Neurol Consultants Austin, Austin, TX USA; [Boeck, Ryan] Univ Texas Austin, Dell Med Sch, Austin, TX 78712 USA; [Cohn, Jeffrey] Broadlands Family Practice Ashburn, Family Med, Ashburn, VA USA; [Collins, Abigail; Demarest, Scott] Univ Colorado, Sch Med, Dept Neurol, Anschutz Med Campus, Aurora, CO USA; [Dobbins, Katherine] Walter Reed Natl Mil Med Ctr, Bethesda, MD USA; [Emrick, Lisa] Baylor Coll Med, Dept Mol & Human Genet, Houston, TX 77030 USA; [Fraser, Jamie] Childrens Natl Hlth Syst, Div Genet & Metab, Rare Dis Inst, Washington, DC USA; [Fraser, Jamie] George Washington Univ, Washington, DC USA; [Masser-Frye, Diane] Rady Childrens Hosp, San Diego, CA USA; [Hayward, Jean] Kaiser Oakland, Dept Pediat, Oakland, CA USA; [Karmarkar, Swati] Le Bonheur Childrens Hosp, Dept Neurol, Memphis, TN USA; [Karmarkar, Swati] Univ Tennessee, Dept Pediat, Hlth Sci Ctr, Memphis, TN USA; [Keller, Stephanie] Emory Univ, Dept Pediat, Div Neurol, Atlanta, GA 30322 USA; [Mirrop, Samuel] Childrens Hosp Los Angeles, Div Neurol, Los Angeles, CA 90027 USA; [Mirrop, Samuel] Univ Southern Calif, Keck Sch Med, Los Angeles, CA 90007 USA; [Mitchell, Wendy] Pediat Associates Austin, Austin, TX USA; [Pathak, Sheel] Washington Univ Clin Associates, Clin Neurol, St Louis, MO USA; [Pathak, Sheel] Washington Univ, Sch Med, Dept Neurol, St Louis, MO 63110 USA; [Sherr, Elliott] Univ Calif San Francisco, Sch Med, Dept Neurol, San Francisco, CA USA; [van Haren, Keith] Stanford Univ, Med Ctr, Dept Neurol, Stanford, CA 94305 USA; [Waters, Erica] Pediat Associates Stockton, Stockton, CA USA; [Wilson, Jenny L.] Oregon Hlth & Sci Univ, Sch Med, Div Pediat Neurol, Portland, OR 97201 USA; [Zhorne, Leah] Univ Iowa Hlth Care, Carver Coll Med, Stead Family Dept Pediat, Iowa City, IA USA; [Schiffmann, Raphael] Baylor Scott & White Res Inst, Inst Metab Dis, Dallas, TX USA; [van der Knaap, Marjo S.] Vrije Univ Amsterdam Med Ctr, Dept Child Neurol, Amsterdam, Netherlands; [van der Knaap, Marjo S.] Vrije Univ Amsterdam, Dept Funct Genom, Amsterdam Neurosci, Amsterdam, Netherlands; [Shults, Justine] Univ Penn, Dept Biostat, Philadelphia, PA 19104 USA; [Taft, Ryan J.] Illumina, San Diego, CA 92122 USA		Vanderver, A (corresponding author), Childrens Hosp Philadelphia, Div Neurol, 3415 Civ Ctr Blvd, Philadelphia, PA 19104 USA.; Taft, RJ (corresponding author), Illumina, San Diego, CA 92122 USA.	vandervera@email.chop.edu; rtaft@illumina.com	Bernard, Geneviève/AAU-7569-2020; Vanderver, Adeline/CAG-5072-2022; Van Haren, Keith P./AGG-1580-2022; Simons, Cas/A-7905-2011; Helman, Guy/C-7935-2017	Bernard, Geneviève/0000-0002-9634-2966; Simons, Cas/0000-0003-3147-8042; Wilson, Jenny/0000-0001-6677-869X; van der Knaap, Marjo/0000-0001-8912-0954; Van Haren, Keith/0000-0002-0233-9361; Helman, Guy/0000-0002-4784-7423; Fraser, Jamie/0000-0002-7794-8903	Pennsylvania Department of Health's Commonwealth Universal Research Enhancement Program Tobacco Formula award SAP [4100077047]; Leukodystrophy Care Network of the Hunter's Hope Foundation; Children's Hospital of Philadelphia; Jacob A. Kamens endowed chair; Australian National Health and Medical Research Council [1068278]; Canadian Institutes of Health Research	Pennsylvania Department of Health's Commonwealth Universal Research Enhancement Program Tobacco Formula award SAP; Leukodystrophy Care Network of the Hunter's Hope Foundation; Children's Hospital of Philadelphia; Jacob A. Kamens endowed chair; Australian National Health and Medical Research Council(National Health and Medical Research Council (NHMRC) of Australia); Canadian Institutes of Health Research(Canadian Institutes of Health Research (CIHR))	Clinical genome sequencing, analysis, and interpretation were provided by Illumina and the Illumina Clinical Services Laboratory. The study is in part supported by the Pennsylvania Department of Health's Commonwealth Universal Research Enhancement Program Tobacco Formula award SAP# 4100077047. Creation of a diagnostic algorithm was supported by the Leukodystrophy Care Network of the Hunter's Hope Foundation. The Children's Hospital of Philadelphia and the Jacob A. Kamens endowed chair supported all other study-related activities. No non-Illumina staff received any direct financial work or compensation for this effort. The participation of G.H. and C.S. is in part financed by the Australian National Health and Medical Research Council (1068278). G.B. has received the New Investigator Salary Award from the Canadian Institutes of Health Research (2017-2022).	Adang LA, 2017, MOL GENET METAB, V122, P18, DOI 10.1016/j.ymgme.2017.08.006; Akwa Y, 1998, J IMMUNOL, V161, P5016; Belkadi A, 2015, P NATL ACAD SCI USA, V112, P5473, DOI 10.1073/pnas.1418631112; Boespflug-Tanguy O, 2008, CURR NEUROL NEUROSCI, V8, P217, DOI 10.1007/s11910-008-0034-x; Bonkowsky JL, 2010, NEUROLOGY, V75, P718, DOI 10.1212/WNL.0b013e3181eee46b; Costello DJ, 2009, NEUROLOGIST, V15, P319, DOI 10.1097/NRL.0b013e3181b287c8; Elashoff J.D., 2007, NQUERY ADVISOR VERSI; Helman G, 2015, MOL GENET METAB, V114, P527, DOI 10.1016/j.ymgme.2015.01.014; Ivakhno S, 2018, BIOINFORMATICS, V34, P516, DOI 10.1093/bioinformatics/btx618; Karrison TG, 2016, STATA J, V16, P678, DOI 10.1177/1536867X1601600308; Kevelam SH, 2016, NEUROPEDIATRICS, V47, P349, DOI 10.1055/s-0036-1588020; Klein CJ, 2017, MAYO CLIN PROC, V92, P292, DOI 10.1016/j.mayocp.2016.09.008; Lionel AC, 2018, GENET MED, V20, P435, DOI 10.1038/gim.2017.119; Parikh S, 2015, MOL GENET METAB, V114, P501, DOI 10.1016/j.ymgme.2014.12.434; Petrikin JE, 2018, NPJ GENOM MED, V3, DOI 10.1038/s41525-018-0045-8; Richards J, 2015, AM J MED GENET A, V167, P2541, DOI 10.1002/ajmg.a.37215; Richards S, 2015, GENET MED, V17, P405, DOI 10.1038/gim.2015.30; Roller E, 2016, BIOINFORMATICS, V32, P2375, DOI 10.1093/bioinformatics/btw163; Saunders CT, 2012, BIOINFORMATICS, V28, P1811, DOI 10.1093/bioinformatics/bts271; Seber CJ, 1993, AM STAT, V47, P178; Srivastava S, 2014, ANN NEUROL, V76, P473, DOI 10.1002/ana.24251; Tan TY, 2017, JAMA PEDIATR, V171, P855, DOI 10.1001/jamapediatrics.2017.1755; van der Knaap MS, 2017, ACTA NEUROPATHOL, V134, P351, DOI 10.1007/s00401-017-1739-1; van der Knaap MS, 1999, RADIOLOGY, V213, P121, DOI 10.1148/radiology.213.1.r99se01121; van der Knaap MS, 2005, MAGNETIC RESONANCE M; Vanderver A, 2016, ANN NEUROL, V79, P1031, DOI 10.1002/ana.24650; Vanderver A, 2015, MOL GENET METAB, V114, P494, DOI 10.1016/j.ymgme.2015.01.006; Vanderver A, 2012, SEMIN PEDIATR NEUROL, V19, P219, DOI 10.1016/j.spen.2012.10.001	28	10	10	0	2	WILEY	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA	0364-5134	1531-8249		ANN NEUROL	Ann. Neurol.	AUG	2020	88	2					264	273		10.1002/ana.25757	http://dx.doi.org/10.1002/ana.25757		JUN 2020	10	Clinical Neurology; Neurosciences	Science Citation Index Expanded (SCI-EXPANDED)	Neurosciences & Neurology	MO5KI	32342562	Green Accepted			2022-10-03	WOS:000538900400001
J	Goodman, WK; Storch, EA; Cohn, JF; Sheth, SA				Goodman, Wayne K.; Storch, Eric A.; Cohn, Jeffrey; Sheth, Sameer A.			Deep Brain Stimulation for Intractable Obsessive-Compulsive Disorder: Progress and Opportunities	AMERICAN JOURNAL OF PSYCHIATRY			English	Editorial Material							VENTRAL CAPSULE/VENTRAL STRIATUM; PREDICTORS		[Goodman, Wayne K.; Storch, Eric A.] Baylor Coll Med, Menninger Dept Psychiat & Behav Sci, Houston, TX 77030 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Sheth, Sameer A.] Baylor Coll Med, Dept Neurosurg, Houston, TX 77030 USA		Goodman, WK (corresponding author), Baylor Coll Med, Menninger Dept Psychiat & Behav Sci, Houston, TX 77030 USA.	wgoodman@bcm.edu			NIH [UH3NS100549, UH3NS103549]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	Supported by NIH (grant UH3NS100549 to Drs. Goodman, Storch, Cohn, and Sheth and grant UH3NS103549 to Drs. Goodman and Sheth).	Alonso P, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0133591; Baas JMP, 2014, FRONT BEHAV NEUROSCI, V8, DOI 10.3389/fnbeh.2014.00305; Baldermann JC, 2019, BIOL PSYCHIAT, V85, P735, DOI 10.1016/j.biopsych.2018.12.019; Bragdon LB, 2017, J ANXIETY DISORD, V45, P64, DOI 10.1016/j.janxdis.2016.12.002; Cohn JF, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P40, DOI 10.1145/3242969.3243023; Denys D, 2020, AM J PSYCHIAT, V177, P265, DOI 10.1176/appi.ajp.2019.19060656; Denys D, 2010, ARCH GEN PSYCHIAT, V67, P1061, DOI 10.1001/archgenpsychiatry.2010.122; Dougherty DD, 2018, JAMA PSYCHIAT, V75, P1081, DOI 10.1001/jamapsychiatry.2018.0930; Ettelt S, 2008, J AFFECT DISORDERS, V107, P265, DOI 10.1016/j.jad.2007.08.017; Figee M, 2014, BIOL PSYCHIAT, V75, P647, DOI 10.1016/j.biopsych.2013.06.021; Figee M, 2013, NAT NEUROSCI, V16, P386, DOI 10.1038/nn.3344; Figee M, 2011, BIOL PSYCHIAT, V69, P867, DOI 10.1016/j.biopsych.2010.12.003; Goodman WK, 2010, BIOL PSYCHIAT, V67, P535, DOI 10.1016/j.biopsych.2009.11.028; GOODMAN WK, 1989, ARCH GEN PSYCHIAT, V46, P1006; Gradinaru V, 2009, SCIENCE, V324, P354, DOI 10.1126/science.1167093; Guzick A, 2020, EXPERT REV NEUROTHER, V20, P95, DOI 10.1080/14737175.2020.1694409; Haq IU, 2011, NEUROIMAGE, V54, pS247, DOI 10.1016/j.neuroimage.2010.03.009; Holtzheimer PE, 2017, LANCET PSYCHIAT, V4, P839, DOI 10.1016/S2215-0366(17)30371-1; Karas PJ, 2019, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00998; Lewin AB, 2011, PSYCHIAT RES, V185, P394, DOI 10.1016/j.psychres.2010.08.021; Liebrand LC, 2019, BRAIN STIMUL, V12, P353, DOI 10.1016/j.brs.2018.11.014; Malone DA, 2009, BIOL PSYCHIAT, V65, P267, DOI 10.1016/j.biopsych.2008.08.029; Mantione M, 2014, PSYCHOL MED, V44, P3515, DOI 10.1017/S0033291714000956; Nuttin B, 1999, LANCET, V354, P1526, DOI 10.1016/S0140-6736(99)02376-4; Provenza NR, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00152; Rodriguez-Romaguera J, 2012, P NATL ACAD SCI USA, V109, P8764, DOI 10.1073/pnas.1200782109; Schruers K, 2019, TRANSL PSYCHIAT, V9, DOI 10.1038/s41398-019-0522-6; Tsai HC, 2014, NEUROPSYCH DIS TREAT, V10, P63, DOI 10.2147/NDT.S54964; van Westen Maarten, 2015, Curr Behav Neurosci Rep, V2, P41; Widge AS, 2016, J NEUROPSYCH CLIN N, V28, P38, DOI 10.1176/appi.neuropsych.15040089	30	9	9	0	2	AMER PSYCHIATRIC PUBLISHING, INC	WASHINGTON	800 MAINE AVE SW, SUITE 900, WASHINGTON, DC 20024 USA	0002-953X	1535-7228		AM J PSYCHIAT	Am. J. Psychiat.	MAR	2020	177	3					200	203		10.1176/appi.ajp.2020.20010037	http://dx.doi.org/10.1176/appi.ajp.2020.20010037			4	Psychiatry	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Psychiatry	LU5WP	32114787	Green Accepted, Bronze			2022-10-03	WOS:000537825700005
C	Ding, YH; Ertugrul, IO; Darzi, A; Provenza, NR; Jeni, LA; Borton, D; Goodman, W; Cohn, JF			ACM	Ding, Yaohan; Ertugrul, Itir Onal; Darzi, Ali; Provenza, Nicole R.; Jeni, Laszlo A.; Borton, David A.; Goodman, Wayne K.; Cohn, Jeffrey			Automated Detection of Optimal DBS Device Settings	COMPANION PUBLICATON OF THE 2020 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION (ICMI '20 COMPANION)			English	Proceedings Paper	International Conference on Multimodal Interaction (ICMI)	OCT 25-29, 2020	ELECTR NETWORK	Assoc Comp Machinery, ACM SIGCHI		affective computing; clinical research; deep brain stimulation (DBS); ventral striatum; obsessive-compulsive disorder (OCD)		Continuous deep brain stimulation (DBS) of the ventral striatum (VS) is an effective treatment for severe, treatment-refractory obsessive-compulsive disorder (OCD). Optimal parameter settings are signaled by a mirth response of intense positive affect, which is subjectively identified by clinicians. Subjective judgments are idiosyncratic and difficult to standardize. To objectively measure mirth responses, we used Automatic Facial Affect Recognition (AFAR) in a series of longitudinal assessments of a patient treated with DBS. Pre- and post-adjustment DBS were compared using both statistical and machine learning approaches. Positive affect was significantly higher after DBS adjustment. Using XGBoost and SVM, the participant's pre- and post-adjustment responses were differentiated with accuracy values of 0.76 and 0.75, which suggest feasibility of objective measurement of mirth response.	[Ding, Yaohan; Darzi, Ali; Cohn, Jeffrey] Univ Pittsburgh, Pittsburgh, PA 15260 USA; [Ertugrul, Itir Onal] Tilburg Univ, Tilburg, Netherlands; [Provenza, Nicole R.; Borton, David A.] Brown Univ, Providence, RI 02912 USA; [Jeni, Laszlo A.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Goodman, Wayne K.] Baylor Coll Med, Houston, TX 77030 USA		Cohn, JF (corresponding author), Univ Pittsburgh, Pittsburgh, PA 15260 USA.	yad30@pitt.edu; i.onal@tilburguniversity.edu; ald260@pitt.edu; nicole_provenza@brown.edu; laszlojeni@cmu.edu; david_borton@brown.edu; wayne.goodman@bcm.edu; jeffcohn@pitt.edu	Darzi, Ali/AAZ-9158-2021					Alonso P, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0133591; Ambadar Z, 2009, J NONVERBAL BEHAV, V33, P17, DOI 10.1007/s10919-008-0059-5; Baker JK, 2010, INT J BEHAV DEV, V34, P88, DOI 10.1177/0165025409350365; Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785; Denys D, 2020, AM J PSYCHIAT, V177, P265, DOI 10.1176/appi.ajp.2019.19060656; Dibeklioglu H, 2018, IEEE J BIOMED HEALTH, V22, P525, DOI 10.1109/JBHI.2017.2676878; Ekman P, 2002, NETWORK RES INFORM; Ertugrul Itir Onal, 2019, Proc Int Conf Autom Face Gesture Recognit, V2019, DOI 10.1109/FG.2019.8756623; Garcia MR, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073456; Girard Jeffrey M., 2020, PSYARXIV, DOI [10.31234/osf.io/397af (2020, DOI 10.31234/OSF.IO/397AF(2020]; GOODMAN WK, 1989, ARCH GEN PSYCHIAT, V46, P1006; Hammal Z, 2014, IEEE T AFFECT COMPUT, V5, P155, DOI 10.1109/TAFFC.2014.2326408; Martin KB, 2018, MOL AUTISM, V9, DOI 10.1186/s13229-018-0198-4; PLOS ONE Staff, 2019, PLoS One, V14, pe0213756, DOI 10.1371/journal.pone.0213756; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742	15	2	2	0	0	ASSOC COMPUTING MACHINERY	NEW YORK	1601 Broadway, 10th Floor, NEW YORK, NY, UNITED STATES			978-1-4503-8002-7				2020							354	356		10.1145/3395035.3425354	http://dx.doi.org/10.1145/3395035.3425354			3	Computer Science, Cybernetics; Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering	BS6LE		Green Accepted			2022-10-03	WOS:000749408300066
J	Ertugrul, IO; Yang, L; Jeni, LA; Cohn, JF				Ertugrul, Itir Onal; Yang, Le; Jeni, Laszlo A.; Cohn, Jeffrey			D-PAttNet: Dynamic Patch-Attentive Deep Network for Action Unit Detection	FRONTIERS IN COMPUTER SCIENCE			English	Article						action unit detection; 3D face registration; 3D-CNN; sigmoidal attention; patch-based	HUMAN NEURAL SYSTEM; FACIAL EXPRESSIONS; FACE; EMOTIONS; RECOGNITION; MACHINE	Facial action units (AUs) relate to specific local facial regions. Recent efforts in automated AU detection have focused on learning the facial patch representations to detect specific AUs. These efforts have encountered three hurdles. First, they implicitly assume that facial patches are robust to head rotation; yet non-frontal rotation is common. Second, mappings between AUs and patches are defined a priori, which ignores co-occurrences among AUs. And third, the dynamics of AUs are either ignored or modeled sequentially rather than simultaneously as in human perception. Inspired by recent advances in human perception, we propose a dynamic patch-attentive deep network, called D-PAttNet, for AU detection that (i) controls for 3D head and face rotation, (ii) learns mappings of patches to AUs, and (iii) models spatiotemporal dynamics. D-PAttNet approach significantly improves upon existing state of the art.	[Ertugrul, Itir Onal; Jeni, Laszlo A.] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA; [Yang, Le] Northwestern Polytech Univ, Sch Comp Sci, Xian, Peoples R China; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Ertugrul, IO (corresponding author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.	iertugru@andrew.cmu.edu			NIH [NS100549, MH096951]; NSF [CNS 1629716]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	This research was supported in part by NIH awards NS100549 and MH096951 and NSF award CNS 1629716.	Ambadar Z, 2005, PSYCHOL SCI, V16, P403, DOI 10.1111/j.0956-7976.2005.01548.x; Arcurio LR, 2012, NEUROPSYCHOLOGIA, V50, P2454, DOI 10.1016/j.neuropsychologia.2012.06.016; Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019; Bihan Jiang, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P314, DOI 10.1109/FG.2011.5771416; Bould E, 2008, COGNITION EMOTION, V22, P1569, DOI 10.1080/02699930801921156; Chu WS, 2017, IEEE INT CONF AUTOMA, P25, DOI 10.1109/FG.2017.13; Chu WS, 2013, PROC CVPR IEEE, P3515, DOI 10.1109/CVPR.2013.451; Cohn JF, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P40, DOI 10.1145/3242969.3243023; Corneanu C, 2018, LECT NOTES COMPUT SC, V11216, P309, DOI 10.1007/978-3-030-01258-8_19; Du SC, 2014, P NATL ACAD SCI USA, V111, pE1454, DOI 10.1073/pnas.1322355111; Ekman P, 2002, RES NEXUS; Eleftheriadis S, 2015, IEEE I CONF COMP VIS, P3792, DOI 10.1109/ICCV.2015.432; Ertugrul Itir Onal, 2019, Proc Int Conf Autom Face Gesture Recognit, V2019, DOI 10.1109/FG.2019.8756543; Ertugrul IO, 2019, IEEE INT CONF AUTOMA, P744; Fairhall SL, 2007, CEREB CORTEX, V17, P2400, DOI 10.1093/cercor/bhl148; Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632; George N, 1999, NAT NEUROSCI, V2, P574, DOI 10.1038/9230; Gonzalez I, 2015, MULTIMED TOOLS APPL, V74, P10001, DOI 10.1007/s11042-014-2320-8; Grill-Spector K, 2004, ANNU REV NEUROSCI, V27, P649, DOI 10.1146/annurev.neuro.27.070203.144220; Hammal Z, 2017, INT CONF AFFECT, P216, DOI 10.1109/ACII.2017.8273603; Haxby JV, 2000, TRENDS COGN SCI, V4, P223, DOI 10.1016/S1364-6613(00)01482-0; Hoffman EA, 2000, NAT NEUROSCI, V3, P80, DOI 10.1038/71152; Horstmann G, 2009, EMOTION, V9, P29, DOI 10.1037/a0014147; Jeni Laszlo A., 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163142; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Jeni LA, 2014, LECT NOTES COMPUT SC, V8692, P135, DOI 10.1007/978-3-319-10593-2_10; Jeni LA, 2013, INT CONF AFFECT, P245, DOI 10.1109/ACII.2013.47; Katsyri J, 2008, INT J HUM-COMPUT ST, V66, P233, DOI 10.1016/j.ijhcs.2007.10.001; Koelstra S, 2010, IEEE T PATTERN ANAL, V32, P1940, DOI 10.1109/TPAMI.2010.50; Kollias D, 2018, IEEE IJCNN; Le Yang, 2019, 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII). Proceedings, P538, DOI 10.1109/ACII.2019.8925514; Li W, 2018, IEEE T PATTERN ANAL, V40, P2583, DOI 10.1109/TPAMI.2018.2791608; Li W, 2017, PROC CVPR IEEE, P6766, DOI 10.1109/CVPR.2017.716; Liu CH, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P630, DOI 10.1145/3242969.3264989; Liu J, 2010, J COGNITIVE NEUROSCI, V22, P203, DOI 10.1162/jocn.2009.21203; Liu P, 2014, LECT NOTES COMPUT SC, V8692, P151, DOI 10.1007/978-3-319-10593-2_11; Lu C, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P646, DOI 10.1145/3242969.3264992; Lucey S., 2007, FACE RECOGNITION; Luong T., 2015, P 2015 C EMP METH NA, P1412; Nichols DF, 2010, FRONT PSYCHOL, V1, DOI 10.3389/fpsyg.2010.00028; Onal Ertugrul I., 2019, P BRIT MACH VIS C BM; Pitcher D, 2011, EXP BRAIN RES, V209, P481, DOI 10.1007/s00221-011-2579-1; Rodriguez P, 2018, LECT NOTES COMPUT SC, V11212, P357, DOI 10.1007/978-3-030-01237-3_22; Sanchez, 2018, ARXIV PREPRINT ARXIV; Shao ZW, 2018, LECT NOTES COMPUT SC, V11217, P725, DOI 10.1007/978-3-030-01261-8_43; Shojaeilangari S, 2015, IEEE T IMAGE PROCESS, V24, P2140, DOI 10.1109/TIP.2015.2416634; Taheri S, 2014, IEEE T IMAGE PROCESS, V23, P3590, DOI 10.1109/TIP.2014.2331141; Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962; Toser Z, 2016, LECT NOTES COMPUT SC, V9915, P359, DOI 10.1007/978-3-319-49409-8_29; Valstar M, 2016, APPL COMP VIS WACV 2, P1, DOI DOI 10.1109/WACV.2016.7477625; Valstar MF, 2007, LECT NOTES COMPUT SC, V4796, P118; Vielzeuf V., 2017, P 19 ACM INT C MULT, P569, DOI DOI 10.1145/3136755.3143011; Yang P, 2009, PATTERN RECOGN LETT, V30, P132, DOI 10.1016/j.patrec.2008.03.014; Zeng JB, 2015, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2015.413; Zhang Z., 2018, BMVC, P1; Zhao KL, 2018, PROC CVPR IEEE, P2090, DOI 10.1109/CVPR.2018.00223; Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369; Zhao KL, 2016, IEEE T IMAGE PROCESS, V25, P3931, DOI 10.1109/TIP.2016.2570550; Zhong L, 2015, IEEE T CYBERNETICS, V45, P1499, DOI 10.1109/TCYB.2014.2354351; Zichao Yang, 2016, P 2016 C N AM CHAPT, P1480	60	7	7	0	0	FRONTIERS MEDIA SA	LAUSANNE	AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND		2624-9898		FRONT COMP SCI-SWITZ	Front. Comput. Sci.-Switz	NOV 29	2019	1								11	10.3389/fcomp.2019.00011	http://dx.doi.org/10.3389/fcomp.2019.00011			13	Computer Science, Interdisciplinary Applications	Emerging Sources Citation Index (ESCI)	Computer Science	WE9XF	31930192	Green Accepted, gold			2022-10-03	WOS:000705969900001
J	Provenza, NR; Matteson, ER; Allawala, AB; Barrios-Anderson, A; Sheth, SA; Viswanathan, A; McIngvale, E; Storch, EA; Frank, MJ; McLaughlin, NCR; Cohn, JF; Goodman, WK; Borton, DA				Provenza, Nicole R.; Matteson, Evan R.; Allawala, Anusha B.; Barrios-Anderson, Adriel; Sheth, Sameer A.; Viswanathan, Ashwin; McIngvale, Elizabeth; Storch, Eric A.; Frank, Michael J.; McLaughlin, Nicole C. R.; Cohn, Jeffrey; Goodman, Wayne K.; Borton, David A.			The Case for Adaptive Neuromodulation to Treat Severe Intractable Mental Disorders	FRONTIERS IN NEUROSCIENCE			English	Article						responsive neuromodulation; mental disorders; adaptive deep brain stimulation; obsessive compulsive disorder; biomarkers	DEEP BRAIN-STIMULATION; OBSESSIVE-COMPULSIVE DISORDER; VENTRAL CAPSULE/VENTRAL STRIATUM; SUBSTANCE USE DISORDERS; SUBTHALAMIC NUCLEUS; BASAL GANGLIA; PSYCHIATRIC-DISORDERS; ANTERIOR CAPSULOTOMY; BEHAVIORAL-THERAPY; GLOBAL BURDEN	Mental disorders are a leading cause of disability worldwide, and available treatments have limited efficacy for severe cases unresponsive to conventional therapies. Neurosurgical interventions, such as lesioning procedures, have shown success in treating refractory cases of mental illness, but may have irreversible side effects. Neuromodulation therapies, specifically Deep Brain Stimulation (DBS), may offer similar therapeutic benefits using a reversible (explantable) and adjustable platform. Early DBS trials have been promising, however, pivotal clinical trials have failed to date. These failures may be attributed to targeting, patient selection, or the "open-loop" nature of DBS, where stimulation parameters are chosen ad hoc during infrequent visits to the clinician's office that take place weeks to months apart. Further, the tonic continuous stimulation fails to address the dynamic nature of mental illness; symptoms often fluctuate over minutes to days. Additionally, stimulation-based interventions can cause undesirable effects if applied when not needed. A responsive, adaptive DBS (aDBS) system may improve efficacy by titrating stimulation parameters in response to neural signatures (i.e., biomarkers) related to symptoms and side effects. Here, we present rationale for the development of a responsive DBS system for treatment of refractory mental illness, detail a strategic approach for identification of electrophysiological and behavioral biomarkers of mental illness, and discuss opportunities for future technological developments that may harness aDBS to deliver improved therapy.	[Provenza, Nicole R.; Matteson, Evan R.; Allawala, Anusha B.; Borton, David A.] Brown Univ, Sch Engn, Providence, RI 02912 USA; [Provenza, Nicole R.] Charles Stark Draper Lab, Cambridge, MA USA; [Barrios-Anderson, Adriel; McLaughlin, Nicole C. R.] Brown Univ, Psychiat Neurosurg Program, Butler Hosp, Warren Alpert Med Sch, Providence, RI 02912 USA; [Sheth, Sameer A.; Viswanathan, Ashwin] Baylor Coll Med, Dept Neurosurg, Houston, TX 77030 USA; [McIngvale, Elizabeth; Storch, Eric A.; Goodman, Wayne K.] Baylor Coll Med, Menninger Dept Psychiat & Behav Sci, Houston, TX 77030 USA; [Frank, Michael J.] Brown Univ, Dept Cognit Linguist & Psychol Sci, Providence, RI 02912 USA; [Frank, Michael J.; Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Borton, David A.] Brown Univ, Carney Inst Brain Sci, Providence, RI 02912 USA; [Borton, David A.] Providence Med Ctr, Ctr Neurorestorat & Neurotechnol, Dept Vet Affairs, Providence, RI 02908 USA		Borton, DA (corresponding author), Brown Univ, Sch Engn, Providence, RI 02912 USA.; Borton, DA (corresponding author), Brown Univ, Carney Inst Brain Sci, Providence, RI 02912 USA.; Borton, DA (corresponding author), Providence Med Ctr, Ctr Neurorestorat & Neurotechnol, Dept Vet Affairs, Providence, RI 02908 USA.	david_borton@brown.edu		Provenza, Nicole R./0000-0002-6952-5417; Barrios-Anderson, Adriel/0000-0002-7150-3938	NIH [1UH3NS100549-01]; McNair Foundation [K23MH100607]; National Science Foundation Graduate Research Fellowship; Draper Fellowship Program; NATIONAL INSTITUTE OF MENTAL HEALTH [K23MH100607] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE [UH3NS100549] Funding Source: NIH RePORTER	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); McNair Foundation; National Science Foundation Graduate Research Fellowship(National Science Foundation (NSF)); Draper Fellowship Program; NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS))	This work was supported by NIH (1UH3NS100549-01) (WG, DB, and JC), the McNair Foundation (SS), K23MH100607 (NM), the National Science Foundation Graduate Research Fellowship (AA), and the Draper Fellowship Program (NP).	Afshar P, 2013, FRONT NEURAL CIRCUIT, V6, DOI 10.3389/fncir.2012.00117; Ahmari SE, 2015, DEPRESS ANXIETY, V32, P550, DOI 10.1002/da.22367; Ajiboye AB, 2017, LANCET, V389, P1821, DOI 10.1016/S0140-6736(17)30601-3; Alonso P, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0133591; Aron AR, 2007, J NEUROSCI, V27, P3743, DOI 10.1523/JNEUROSCI.0519-07.2007; Barrett K, 2017, BJPSYCH BULL, V41, P281, DOI 10.1192/pb.bp.116.055772; Baskin-Sommers AR, 2015, INT J PSYCHOPHYSIOL, V98, P227, DOI 10.1016/j.ijpsycho.2015.01.011; Bewernick BH, 2012, NEUROPSYCHOPHARMACOL, V37, P1975, DOI 10.1038/npp.2012.44; Boccard SGJ, 2014, NEUROSURGERY, V74, P628, DOI 10.1227/NEU.0000000000000321; Brown LT, 2016, J NEUROSURG, V124, P77, DOI 10.3171/2015.1.JNS14681; Cagnan H, 2017, BRAIN, V140, P132, DOI 10.1093/brain/aww286; Cavanagh JF, 2015, J PHYSIOL-PARIS, V109, P3, DOI 10.1016/j.jphysparis.2014.04.003; Cavanagh JF, 2011, NAT NEUROSCI, V14, P1462, DOI 10.1038/nn.2925; Cavanagh JF, 2010, NEUROIMAGE, V49, P3198, DOI 10.1016/j.neuroimage.2009.11.080; Cleary DR, 2015, NEUROSURG FOCUS, V38, DOI 10.3171/2015.3.FOCUS1546; Cohen MX, 2009, J NEUROSCI, V29, P7591, DOI 10.1523/JNEUROSCI.5335-08.2009; Collins AGE, 2018, P NATL ACAD SCI USA, V115, P2502, DOI 10.1073/pnas.1720963115; Dangi S, 2013, NEURAL COMPUT, V25, P1693, DOI 10.1162/NECO_a_00460; Endrass T, 2014, NEUROSCI BIOBEHAV R, V46, P124, DOI 10.1016/j.neubiorev.2014.03.024; Figee M, 2013, NAT NEUROSCI, V16, P386, DOI 10.1038/nn.3344; Figee M, 2011, BIOL PSYCHIAT, V69, P867, DOI 10.1016/j.biopsych.2010.12.003; Frank MJ, 2015, J NEUROSCI, V35, P485, DOI 10.1523/JNEUROSCI.2036-14.2015; Gehring WJ, 2000, PSYCHOL SCI, V11, P1, DOI 10.1111/1467-9280.00206; Girard JM, 2015, BEHAV RES METHODS, V47, P1136, DOI 10.3758/s13428-014-0536-1; Graat I, 2017, INT REV PSYCHIATR, V29, P178, DOI 10.1080/09540261.2017.1282439; Greenberg BD, 2010, MOL PSYCHIATR, V15, P64, DOI 10.1038/mp.2008.55; Greenberg BD, 2003, NEUROSURG CLIN N AM, V14, P199, DOI 10.1016/S1042-3680(03)00005-6; Greicius M, 2008, CURR OPIN NEUROL, V21, P424, DOI 10.1097/WCO.0b013e328306f2c5; Haber SN, 2013, NEUROPSYCHOPHARMACOL, V38, P252, DOI 10.1038/npp.2012.182; Harris AZ, 2015, ANNU REV NEUROSCI, V38, P171, DOI 10.1146/annurev-neuro-071714-034111; Herron JA, 2017, IEEE T NEUR SYS REH, V25, P2180, DOI 10.1109/TNSRE.2017.2705661; Hofmann SG, 2008, J CLIN PSYCHIAT, V69, P621, DOI 10.4088/jcp.v69n0415; Horschig JM, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0138685; Huys QJM, 2016, NAT NEUROSCI, V19, P404, DOI 10.1038/nn.4238; Insel T, 2010, AM J PSYCHIAT, V167, P748, DOI 10.1176/appi.ajp.2010.09091379; Insel TR, 2009, J CLIN INVEST, V119, P700, DOI 10.1172/JCI38832; Jarosiewicz B, 2015, SCI TRANSL MED, V7, DOI 10.1126/scitranslmed.aac7328; Johansen-Berg H, 2008, CEREB CORTEX, V18, P1374, DOI 10.1093/cercor/bhm167; Khanna P., 2015, 2015 IEEE BIOM CIRC, DOI [10.1109/BioCAS.2015.7348348, DOI 10.1109/BIOCAS.2015.7348348]; Kirkby LA, 2018, CELL, V175, P1688, DOI 10.1016/j.cell.2018.10.005; Kisely S, 2014, PSYCHOL MED, V44, P3533, DOI 10.1017/S0033291714000981; Kopell BH, 2008, NEUROSCI BIOBEHAV R, V32, P408, DOI 10.1016/j.neubiorev.2007.07.004; Krystal JH, 2014, CELL, V157, P201, DOI 10.1016/j.cell.2014.02.042; Lega BC, 2011, NEUROREPORT, V22, P795, DOI 10.1097/WNR.0b013e32834b2975; Lehman JF, 2011, J NEUROSCI, V31, P10392, DOI 10.1523/JNEUROSCI.0595-11.2011; Little S, 2013, ANN NEUROL, V74, P449, DOI 10.1002/ana.23951; Locher C, 2017, JAMA PSYCHIAT, V74, P1011, DOI 10.1001/jamapsychiatry.2017.2432; Malone DA, 2009, BIOL PSYCHIAT, V65, P267, DOI 10.1016/j.biopsych.2008.08.029; McLaughlin N.C.R., 2016, PSYCHIAT NEUROTHERAP, P141, DOI [10.1007/978-1-59745-495-7_7, DOI 10.1007/978-1-59745-495-7_7, DOI 10.1007/978-1-59745-495-7]; Milad MR, 2012, TRENDS COGN SCI, V16, P43, DOI 10.1016/j.tics.2011.11.003; Munneke GJ, 2015, BRAIN RES, V1618, P222, DOI 10.1016/j.brainres.2015.05.030; Nambu A., 2014, DEEP BRAIN STIMULATI, P13; Norberg MM, 2008, DEPRESS ANXIETY, V25, P248, DOI 10.1002/da.20298; Pagnoni G, 2002, NAT NEUROSCI, V5, P97, DOI 10.1038/nn802; Peng XL, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0079476; Pepper J, 2015, J NEUROSURG, V122, P1028, DOI 10.3171/2014.11.JNS132618; Peters SK, 2016, FRONT SYST NEUROSCI, V10, DOI 10.3389/fnsys.2016.00104; PITMAN RK, 1987, COMPR PSYCHIAT, V28, P334, DOI 10.1016/0010-440X(87)90070-8; Pittenger Christopher, 2005, Psychiatry (Edgmont), V2, P34; Price JL, 2012, TRENDS COGN SCI, V16, P61, DOI 10.1016/j.tics.2011.12.011; Rao VR, 2018, CURR BIOL, V28, P3893, DOI 10.1016/j.cub.2018.10.026; RASMUSSEN SA, 1992, J CLIN PSYCHIAT, V53, P4; Rasmussen SA, 2018, BIOL PSYCHIAT, V84, P355, DOI 10.1016/j.biopsych.2017.11.034; Rauch SL, 2006, J NEUROSURG, V104, P558, DOI 10.3171/jns.2006.104.4.558; Riva-Posse P, 2018, MOL PSYCHIATR, V23, P843, DOI 10.1038/mp.2017.59; Romanelli RJ, 2014, DEPRESS ANXIETY, V31, P641, DOI 10.1002/da.22232; Rosin B, 2011, NEURON, V72, P370, DOI 10.1016/j.neuron.2011.08.023; Ruscio AM, 2010, MOL PSYCHIATR, V15, P53, DOI 10.1038/mp.2008.94; Rush AJ, 2007, AM J PSYCHIAT, V164, P201, DOI 10.1176/appi.ajp.164.2.201; Sani OG, 2018, NAT BIOTECHNOL, V36, P954, DOI 10.1038/nbt.4200; Schmidt R, 2013, NAT NEUROSCI, V16, P1118, DOI 10.1038/nn.3456; Shah Dhwani B, 2008, Psychiatry (Edgmont), V5, P24; Sharma M, 2016, J NEUROSURG SCI, V60, P242; Stanslaski S, 2012, IEEE T NEUR SYS REH, V20, P410, DOI 10.1109/TNSRE.2012.2183617; Steel Z, 2014, INT J EPIDEMIOL, V43, P476, DOI 10.1093/ije/dyu038; Sun FT, 2014, EXPERT REV MED DEVIC, V11, P563, DOI 10.1586/17434440.2014.947274; Tanaka SC, 2004, NAT NEUROSCI, V7, P887, DOI 10.1038/nn1279; Vigo D, 2016, LANCET PSYCHIAT, V3, P171, DOI 10.1016/S2215-0366(15)00505-2; Voon V, 2017, BRAIN, V140, P442, DOI 10.1093/brain/aww309; Walker ER, 2015, JAMA PSYCHIAT, V72, P334, DOI 10.1001/jamapsychiatry.2014.2502; Whelan R, 2012, NAT NEUROSCI, V15, P920, DOI 10.1038/nn.3092; Whiteford HA, 2013, LANCET, V382, P1575, DOI 10.1016/S0140-6736(13)61611-6; Wichmann T, 2006, NEURON, V52, P197, DOI 10.1016/j.neuron.2006.09.022; Widge AS, 2017, EXP NEUROL, V287, P461, DOI 10.1016/j.expneurol.2016.07.021; Widge AS, 2016, J NEUROPSYCH CLIN N, V28, P38, DOI 10.1176/appi.neuropsych.15040089; Wiecki TV, 2015, CLIN PSYCHOL SCI, V3, P378, DOI 10.1177/2167702614565359; Wu H, 2018, P NATL ACAD SCI USA, V115, P192, DOI 10.1073/pnas.1712214114; Zavala BA, 2014, J NEUROSCI, V34, P7322, DOI 10.1523/JNEUROSCI.1169-14.2014	88	25	27	0	1	FRONTIERS MEDIA SA	LAUSANNE	AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND		1662-453X		FRONT NEUROSCI-SWITZ	Front. Neurosci.	FEB 26	2019	13								152	10.3389/fnins.2019.00152	http://dx.doi.org/10.3389/fnins.2019.00152			9	Neurosciences	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Neurosciences & Neurology	LR6KV	30890909	Green Published, gold			2022-10-03	WOS:000535802500001
J	Parker, HM; Cohn, JS; O'Connor, HT; Garg, ML; Caterson, ID; George, J; Johnson, NA				Parker, Helen M.; Cohn, Jeffrey; O'Connor, Helen T.; Garg, Manohar L.; Caterson, Ian D.; George, Jacob; Johnson, Nathan A.			Effect of Fish Oil Supplementation on Hepatic and Visceral Fat in Overweight Men: A Randomized Controlled Trial	NUTRIENTS			English	Article						omega-3 PUFA; polyunsaturated fatty acids; RCT; non-alcoholic fatty liver; body composition	LIVER-DISEASE; OMEGA-3 INDEX; OMEGA-3-FATTY-ACID SUPPLEMENTATION; NONALCOHOLIC STEATOHEPATITIS; TRIGLYCERIDE CONTENT; OBESE ADULTS; WEIGHT-LOSS; ACIDS; STEATOSIS; RISK	Being overweight increases the risk of the development of metabolic conditions such as non-alcoholic fatty liver disease (NAFLD), which is itself an independent predictor of cardiovascular disease. Omega-3 polyunsaturated fatty acid (PUFA) supplementation is recommended for prevention of chronic disease, and is thought to reduce raised liver fat, yet there have been few randomized controlled trials with accurate measurement of liver fat. We assessed the effect of 12 weeks of supplementation with omega-3 PUFA from fish oil versus placebo on quantified liver fat, liver tests, and body composition including visceral adipose tissue (VAT) in a double-blind randomized controlled trial. Fifty apparently healthy overweight men (BMI 25.0-29.9 kg/m(2); waist > 94 cm) were randomly allocated to consume fish oil (total daily dose: 1728 mg marine triglycerides, of which 588 mg EPA and 412 mg DHA, combined with 200 mg antioxidant, coenzyme Q10) or placebo (olive oil capsules) daily for 12 weeks. Liver fat was assessed using proton magnetic resonance spectroscopy. All outcomes were assessed at baseline and following 6 and 12 weeks of supplementation. Baseline liver fat was 4.6 +/- 0.5% (range: 0.6 to 18.2%); 16 (32%) participants met the criteria for NAFLD (>5.5% liver fat). Repeated measures ANOVA revealed no significant time or group x time effect for fish oil versus placebo for liver fat, liver enzymes, anthropometry, or body composition including VAT (p > 0.05 for all), with similar finding for sub-analysis of participants with NAFLD. Omega-3 PUFA did not appear to be an effective agent for reducing liver fat in overweight men. The factors determining the health benefits of omega-3 PUFA supplementation on an individual level need to be clarified.	[Parker, Helen M.; O'Connor, Helen T.; Johnson, Nathan A.] Univ Sydney, Fac Hlth Sci, Lidcombe, NSW 2141, Australia; [Parker, Helen M.; O'Connor, Helen T.; Johnson, Nathan A.] Univ Sydney, Charles Perkins Ctr, Camperdown, NSW 2006, Australia; [Cohn, Jeffrey] Heart Res Inst, Nutr & Metab Grp, Newtown, NSW 2042, Australia; [Garg, Manohar L.] Univ Newcastle, Sch Biomed Sci & Pharm, Fac Hlth & Med, Callaghan, NSW 2308, Australia; [Caterson, Ian D.; Johnson, Nathan A.] Univ Sydney, Boden Inst Obes Nutr Exercise & Eating Disorders, Camperdown, NSW 2006, Australia; [George, Jacob] Univ Sydney, Westmead Inst Med Res, Storr Liver Ctr, Westmead, NSW 2145, Australia		Parker, HM (corresponding author), Univ Sydney, Fac Hlth Sci, Lidcombe, NSW 2141, Australia.; Parker, HM (corresponding author), Univ Sydney, Charles Perkins Ctr, Camperdown, NSW 2006, Australia.	h.parker@sydney.edu.au; cohnj@hri.org.au; helen.oconnor@sydney.edu.au; manohar.garg@newcastle.edu.au; ian.caterson@sydney.edu.au; jacob.george@sydney.edu.au; nathan.johnson@sydney.edu.au	Garg, Manohar L/A-5795-2009; Johnson, Nathan/Q-3158-2016	Parker, Helen/0000-0002-3826-2765; Johnson, Nathan/0000-0002-3874-7092; Garg, Manohar/0000-0003-0514-0865; Caterson, Ian/0000-0002-6139-3632	Blackmores Ltd. (Warriewood, Australia)	Blackmores Ltd. (Warriewood, Australia)	This research was funded by Blackmores Ltd. (Warriewood, Australia).	Alwayn IPJ, 2005, PEDIATR RES, V57, P445, DOI 10.1203/01.PDR.0000153672.43030.75; Angulo P, 2007, ALIMENT PHARM THER, V25, P883, DOI 10.1111/j.1365-2036.2007.03246.x; Argo CK, 2015, J HEPATOL, V62, P190, DOI 10.1016/j.jhep.2014.08.036; Armijo-Olivo S, 2009, PHYS THER REV, V14, P36, DOI 10.1179/174328809X405928; Balk EM, 2006, ATHEROSCLEROSIS, V189, P19, DOI 10.1016/j.atherosclerosis.2006.02.012; BOUCHARD C, 1983, AM J CLIN NUTR, V37, P461, DOI 10.1093/ajcn/37.3.461; Brea A, 2013, INT J CARDIOL, V167, P1109, DOI 10.1016/j.ijcard.2012.09.085; Buchman A, 2002, JPEN-PARENTER ENTER, V26, pS43, DOI 10.1177/014860710202600512; Burns DL, 2013, JPEN-PARENTER ENTER, V37, P274, DOI 10.1177/0148607112450301; Capanni M, 2006, ALIMENT PHARM THER, V23, P1143, DOI 10.1111/j.1365-2036.2006.02885.x; Chen LH, 2018, CLIN NUTR, V37, P516, DOI 10.1016/j.clnu.2016.12.009; Cussons AJ, 2009, J CLIN ENDOCR METAB, V94, P3842, DOI 10.1210/jc.2009-0870; Dasarathy S, 2015, J CLIN GASTROENTEROL, V49, P137, DOI 10.1097/MCG.0000000000000099; de Castro GS, 2018, CLIN NUTR, V37, P37, DOI 10.1016/j.clnu.2017.01.006; Di Minno MND, 2012, WORLD J GASTROENTERO, V18, P5839, DOI 10.3748/wjg.v18.i41.5839; Eslick GD, 2009, INT J CARDIOL, V136, P4, DOI 10.1016/j.ijcard.2008.03.092; Flock MR, 2013, NUTR REV, V71, P692, DOI 10.1111/nure.12071; Food Standards Australia and New Zealand, 2011, AUSTR FOOD SUPPL NUT; Franz MJ, 2007, J AM DIET ASSOC, V107, P1755, DOI 10.1016/j.jada.2007.07.017; Gonzalez-Periz A, 2009, FASEB J, V23, P1946, DOI 10.1096/fj.08-125674; Harris WS, 2008, CURR ATHEROSCLER REP, V10, P503, DOI 10.1007/s11883-008-0078-z; Harris WS, 2004, PREV MED, V39, P212, DOI 10.1016/j.ypmed.2004.02.030; Hartweg J, 2007, DIABETOLOGIA, V50, P1593, DOI 10.1007/s00125-007-0695-z; Hatzitolios Apostolos, 2004, Indian J Gastroenterol, V23, P131; Heart Foundation, FISH FISH OILS N 3 P; Howe PRC, 2014, NUTRIENTS, V6, P1850, DOI 10.3390/nu6051850; Iser D, 2013, AUST FAM PHYSICIAN, V42, P444; Johnson NA, 2008, HEPATOLOGY, V47, P1513, DOI 10.1002/hep.22220; Jump DB, 2012, J LIPID RES, V53, P2525, DOI 10.1194/jlr.R027904; Khan S, 2002, J LIPID RES, V43, P979; Kris-Etherton PM, 2002, CIRCULATION, V106, P2747, DOI 10.1161/01.CIR.0000038493.65177.94; LEPAGE G, 1986, J LIPID RES, V27, P114; Li YH, 2015, WORLD J GASTROENTERO, V21, P7008, DOI 10.3748/wjg.v21.i22.7008; Madmani ME, 2014, COCHRANE DB SYST REV, DOI 10.1002/14651858.CD008684.pub2; Mahady SE, 2011, J HEPATOL, V55, P1383, DOI 10.1016/j.jhep.2011.03.016; Masterton GS, 2010, ALIMENT PHARM THER, V31, P679, DOI 10.1111/j.1365-2036.2010.04230.x; Munro IA, 2013, FOOD FUNCT, V4, P650, DOI 10.1039/c3fo60038f; Musso G, 2010, HEPATOLOGY, V52, P79, DOI 10.1002/hep.23623; Ng M, 2014, LANCET, V384, P766, DOI 10.1016/S0140-6736(14)60460-8; Nogueira MA, 2016, CLIN NUTR, V35, P578, DOI 10.1016/j.clnu.2015.05.001; Overvad K, 1999, EUR J CLIN NUTR, V53, P764, DOI 10.1038/sj.ejcn.1600880; Park Y, 2009, J MED FOOD, V12, P803, DOI 10.1089/jmf.2008.1250; Parker HM, 2015, BRIT J NUTR, V114, P780, DOI 10.1017/S0007114515002305; Parker HM, 2012, J HEPATOL, V56, P944, DOI 10.1016/j.jhep.2011.08.018; Positano V, 2004, J MAGN RESON IMAGING, V20, P684, DOI 10.1002/jmri.20167; Ryan MC, 2013, J HEPATOL, V59, P138, DOI 10.1016/j.jhep.2013.02.012; Sanyal AJ, 2014, GASTROENTEROLOGY, V147, P377, DOI 10.1053/j.gastro.2014.04.046; Sawangjit R, 2016, MEDICINE, V95, DOI 10.1097/MD.0000000000004529; Scorletti E, 2018, MOL ASPECTS MED, V64, P135, DOI 10.1016/j.mam.2018.03.001; Scorletti E, 2014, HEPATOLOGY, V60, P1211, DOI 10.1002/hep.27289; Sofi F, 2010, INT J FOOD SCI NUTR, V61, P792, DOI 10.3109/09637486.2010.487480; Spadaro L, 2008, DIGEST LIVER DIS, V40, P194, DOI 10.1016/j.dld.2007.10.003; Szczepaniak LS, 2005, AM J PHYSIOL-ENDOC M, V288, pE462, DOI 10.1152/ajpendo.00064.2004; Tanaka N, 2008, J CLIN GASTROENTEROL, V42, P413, DOI 10.1097/MCG.0b013e31815591aa; Targher G, 2007, ATHEROSCLEROSIS, V191, P235, DOI 10.1016/j.atherosclerosis.2006.08.021; Van Gaal LF, 2006, NATURE, V444, P875, DOI 10.1038/nature05487; Vega GL, 2008, J INVEST MED, V56, P780, DOI 10.2310/JIM.0b013e318177024d; Volynets V, 2013, EUR J NUTR, V52, P527, DOI 10.1007/s00394-012-0355-z; von Schacky C, 2014, NUTRIENTS, V6, P799, DOI 10.3390/nu6020799; World Health Organization/WHO, 1995, PHYS STAT US INT ANT; Zelber-Sagil S, 2007, J HEPATOL, V47, P711, DOI 10.1016/j.jhep.2007.06.020; Zhu FS, 2008, WORLD J GASTROENTERO, V14, P6395, DOI 10.3748/wjg.14.6395; Zuta PC, 2007, FOOD CHEM, V100, P800, DOI 10.1016/j.foodchem.2005.11.003	63	29	29	1	10	MDPI	BASEL	ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND	2072-6643			NUTRIENTS	Nutrients	FEB	2019	11	2							475	10.3390/nu11020475	http://dx.doi.org/10.3390/nu11020475			15	Nutrition & Dietetics	Science Citation Index Expanded (SCI-EXPANDED)	Nutrition & Dietetics	HO3NH	30813440	Green Published, Green Submitted, gold			2022-10-03	WOS:000460829700266
C	Bhatia, S; Goecke, R; Hammal, Z; Cohn, JF			IEEE	Bhatia, Shalini; Goecke, Roland; Hammal, Zakia; Cohn, Jeffrey			Automated Measurement of Head Movement Synchrony during Dyadic Depression Severity Interviews	2019 14TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG 2019)	IEEE International Conference on Automatic Face and Gesture Recognition and Workshops		English	Proceedings Paper	14th IEEE International Conference on Automatic Face and Gesture Recognition (FG)	MAY 14-18, 2019	Lille, FRANCE	IEEE, Univ Lille, Inst Mines Telecom, Univ Lille, Inst Mines Telecom, Ecole Mines Telecom, IMT Lille Douai, INRIA, 3DMD, Google, I Site Univ Lille Nord Europe, Centre Rech Informatique Signal Automatique Lille, IEEE Comp Soc, IEEE Biometr Council				With few exceptions, most research in automated assessment of depression has considered only the patient's behavior to the exclusion of the therapist's behavior. We investigated the interpersonal coordination ( synchrony) of head movement during patient-therapist clinical interviews. Our sample consisted of patients diagnosed with major depressive disorder. They were recorded in clinical interviews ( Hamilton Rating Scale for Depression, HRSD) at 7-week intervals over a period of 21 weeks. For each session, patient and therapist 3D head movement was tracked from 2D videos. Head angles in the horizontal ( pitch) and vertical ( yaw) axes were used to measure head movement. Interpersonal coordination of head movement between patients and therapists was measured using windowed cross-correlation. Patterns of coordination in head movement were investigated using the peak picking algorithm. Changes in head movement coordination over the course of treatment were measured using a hierarchical linear model ( HLM). The results indicated a strong effect for patient-therapist head movement synchrony. Within-dyad variability in head movement coordination was found to be higher than between-dyad variability, meaning that differences over time in a dyad were higher as compared to the differences between dyads. Head movement synchrony did not change over the course of treatment with change in depression severity. To the best of our knowledge, this study is the first attempt to analyze the mutual influence of patient-therapist head movement in relation to depression severity.	[Bhatia, Shalini; Goecke, Roland] Univ Canberra, Human Ctr Technol Res Ctr, Canberra, ACT, Australia; [Hammal, Zakia] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Bhatia, S (corresponding author), Univ Canberra, Human Ctr Technol Res Ctr, Canberra, ACT, Australia.				NINR of the NIH [R21NRO16510]; United States NSF [IIS-1721667]; NIH [MH096951]	NINR of the NIH; United States NSF; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This research was supported in part by the NINR of the NIH under Award Number R21NRO16510, the United States NSF award IIS-1721667, and NIH award MH096951. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. The authors would like to thank Anthony Davidson and Julio Romero at University of Canberra for statistical support.	Alghowinem S, 2013, INTERSPEECH, P2533; Ambady N., 1993, J PERSONALITY SOCIAL, V64; American Psychiatric Association, 2000, DIAGN STAT MAN MENT, V4; American Psychiatric Association D Association A P, 2013, DIAGN STAT MAN MENT, V21, P591, DOI 10.1176/appi.books.9780890425596; Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01; Boker SM, 2002, PSYCHOL METHODS, V7, P338, DOI 10.1037//1082-989X.7.3.338; Caligiuri MP, 2000, J AFFECT DISORDERS, V57, P83, DOI 10.1016/S0165-0327(99)00068-3; Cascia M. L., 2000, IEEE T PAMI, V22; Cohen J, 1988, PERCEPT MOTOR SKILL; Cohn J.F., 2009, ACII; Cohn J. F., 2013, 3 INT WORKSH AUD VIS; Cummins N, 2013, INTERSPEECH, P857; Dibeklioglu H., 2018, IEEE BHI, V22; DUNCAN S, 1972, J PERS SOC PSYCHOL, V23, P283, DOI 10.1037/h0033031; Fournier J. C., 2010, JAMA, V303; Fridlund A. J., 1991, J PERSONALI SOCIAL P, V60; Fujiwara K., 2010, J AM STAT ASSOC, V101, P130; Gelman A, 2007, DATA ANAL USING REGR, DOI 10.1017/CBO9780511790942; Girard J. M., 2013, IEEE FG; Girard J. M., 2015, CURRENT OPINION PSYC, V4; Girard JM, 2015, BEHAV RES METHODS, V47, P1136, DOI 10.3758/s13428-014-0536-1; Girard JM, 2014, IMAGE VISION COMPUT, V32, P641, DOI 10.1016/j.imavis.2013.12.007; GOTTMAN JM, 1981, CHILD DEV, V52, P393, DOI 10.1111/j.1467-8624.1981.tb03063.x; Hall, 2010, NONVERBAL COMMUNICAT; Hamilton M., 1967, BRIT J SOCIAL CLIN P, V6; Hammal Z., 2014, P WORKSH ROADM FUT M, P19, DOI [DOI 10.1145/2666253.2666258, 10.1145/2666253.2666259]; Hammal Z., 2014, IEEE TAC, V5; Jeni L. A., 2015, IEEE FG; Joshi J., 2013, ACII; Kacem A., 2018, IEEE FGAHI; LAIRD NM, 1982, BIOMETRICS, V38, P963, DOI 10.2307/2529876; Laurent G, 1996, J NEUROSCI, V16, P3837; Lindstrom M. J., 1988, J AM STAT ASS, V83; Messinger DS, 2009, INFANCY, V14, P285, DOI 10.1080/15250000902839963; Neto Arcoverde E. N., 2014, INTEGRATED COMPUTER, V21; Prochazkovaa E., 2017, NEUROSCIENCE BIOBEHA, V80; R Core Team, 2018, R LANG ENV STAT COMP; Rosenthal R., 1984, J CONSULTING CLIN PS, V52; Rudiger A., 2002, SPECTRUM SPECTRAL DE; Scherer S., 2014, IMAGE VISION COMPUTI; Scherer S., 2014, JCM; Sobin C, 1997, AM J PSYCHIAT, V154, P4; Social Science Computing Cooperative, 2016, TECHNICAL REPORT; The Department of Statistics and Data Sciences, 2015, TECHNICAL REPORT; Valstar M., 2016, INT WORKSH AVEC; Yang Y., 2013, IEEE TAC, V4	46	3	3	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2326-5396		978-1-7281-0089-0	IEEE INT CONF AUTOMA			2019							169	176						8	Engineering, Electrical & Electronic	Conference Proceedings Citation Index - Science (CPCI-S)	Engineering	BN6AU					2022-10-03	WOS:000484960400022
J	Chu, WS; De la Torre, F; Cohn, JF				Chu, Wen-Sheng; De la Torre, Fernando; Cohn, Jeffrey			Learning facial action units with spatiotemporal cues and multi-label sampling	IMAGE AND VISION COMPUTING			English	Article; Proceedings Paper	12th IEEE International Conference on Automatic Face and Gesture Recognition (FG)	MAY 30-JUN 03, 2017	Washington, DC	IEEE, Baidu, Mitsubishi Elect Res Labs Inc, 3dMD, DI4D, Syst & Technol Res, ObjectVideo Labs, MUKH Technologies, IEEE Comp Soc		Multi-label learning; Deep learning; Spatio-temporal learning; Multi-label sampling; Facial action unit detection; Video analysis	EXPRESSION; RECOGNITION; EMOTION	Facial action units (AUs) can be represented spatially, temporally, and in terms of their correlation. Previous research focuses on one or another of these aspects or addresses them disjointly. We propose a hybrid network architecture that jointly models spatial and temporal representations and their correlation. In particular, we use a Convolutional Neural Network (CNN) to learn spatial representations, and a Long Short-Term Memory (LSTM) to model temporal dependencies among them. The outputs of CNNs and LSTMs are aggregated into a fusion network to produce per-frame prediction of multiple AUs. The hybrid network was compared to previous state-of-the-art approaches in two large FACS-coded video databases, GFT and BP4D, with over 400,000 AU-coded frames of spontaneous facial behavior in varied social contexts. Relative to standard multi-label CNN and feature-based state-of-the-art approaches, the hybrid system reduced person-specific biases and obtained increased accuracy for AU detection. To address class imbalance within and between batches during network training, we introduce multi-labeling sampling strategies that further increase accuracy when AUs are relatively sparse. Finally, we provide visualization of the learned AU models, which, to the best of our best knowledge, reveal for the first time how machines see AUs. (C) 2018 Elsevier B.V. All rights reserved.	[Chu, Wen-Sheng; De la Torre, Fernando; Cohn, Jeffrey] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Chu, WS (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.	wschu@cmu.edu	Chu, Wen-Sheng/AAF-6871-2019	Chu, Wen-Sheng/0000-0001-8592-6088	US National Institutes of Health [GM105004, MH096951]; Division of Computer and Network Systems [1629716]; NVIDIA; NATIONAL INSTITUTE OF GENERAL MEDICAL SCIENCES [R01GM105004] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH096951] Funding Source: NIH RePORTER	US National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Division of Computer and Network Systems(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); NVIDIA; NATIONAL INSTITUTE OF GENERAL MEDICAL SCIENCES(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of General Medical Sciences (NIGMS)); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))	This work was supported in part by the US National Institutes of Health grants GM105004 and MH096951 and Division of Computer and Network Systems grant number 1629716. The authors also thank NVIDIA for supporting this research with a Tesla K40c GPU, and Jiabei Zeng and Kaili Zhao for assisting partial experiments.	Chang K.-Y., 2009, CVPR; Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953; Chen J., 2009, CVPR; Chu W.-S., 2017, LEARNING SPATIAL TEM; Chu W. - S., 2013, CVPR; Cohn J. F., 2014, OXFORD HDB AFFECTIVE, P131; Cohn JF, 2010, BEHAV RES METHODS, V42, P1079, DOI 10.3758/BRM.42.4.1079; De la Torre F., 2015, INTRAFACE AUTOMATIC; Ding X., 2013, IEEE C INT C COMP VI; Donahue J., 2015, CVPR; Du SC, 2015, DIALOGUES CLIN NEURO, V17, P443; Ekman P., 2005, WHAT FACE REVEALS; Ekman P, 2002, RES NEXUS; Ekman P., 2005, WHAT FACE REVEALS BA, V2nd; Eleftheriadis S., 2015, ICCV; Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010; Ghosh S., 2015, ACII; Girard J., 2017, AFGR; Graves A., 2013, ICASSP; Gudi A., 2015, AFGR; He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239; Jaiswal S., 2016, WACV; Jia Y., 2014, P 22 ACM INT C MULTI, P675, DOI [10.1145/2647868.2654889, DOI 10.1145/2647868.2654889]; Jiang B., 2011, AFGR; Karpathy A., 2014, CVPR; Krizhevsky A., 2012, P NIPS, P1097; Lapin M., 2016, CVPR; LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115; Lipson Hod, 2015, UNDERSTANDING NEURAL; Liu P., 2014, CVPR; Lucey Patrick, 2010, CVPR WORKSH; Martinez A, 2012, J MACH LEARN RES, V13, P1589; Parkhi O. M., 2015, DEEP FAC REC BRIT MA; Prati RC, 2015, KNOWL INF SYST, V45, P247, DOI 10.1007/s10115-014-0794-3; Rudovic O, 2015, IEEE T PATTERN ANAL, V37, P944, DOI 10.1109/TPAMI.2014.2356192; Sangineto E., 2014, WE ARE NOT ALL EQUAL; Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127; Simonyan K., 2014, NIPS; Sun YM, 2009, INT J PATTERN RECOGN, V23, P687, DOI 10.1142/S0218001409007326; Taigman Yaniv, 2015, CVPR; Tian YL, 2005, HANDBOOK OF FACE RECOGNITION, P247, DOI 10.1007/0-387-27257-7_12; Tong Y, 2007, IEEE T PATTERN ANAL, V29, P1683, DOI 10.1109/TPAMI.2007.1094; Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P966, DOI 10.1109/TSMCB.2012.2200675; Vedaldi A., ARXIV13126034; Wang Z., 2013, ICCV; WONG AKC, 1985, IEEE T PATTERN ANAL, V7, P599, DOI 10.1109/TPAMI.1985.4767707; Wu C., 2015, AFGR; Wu Z., 2015, ACM MM; Yang S, 2014, LECT NOTES COMPUT SC, V8888, P269, DOI 10.1007/978-3-319-14364-4_26; Zeng J., 2015, ICCV; Zhang X., 2013, AFGR; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhao K., 2016, CVPR; Zhao Kaili, 2015, CVPR; Zhong L., 2012, CVPR; Zhu YF, 2011, IEEE T AFFECT COMPUT, V2, P79, DOI 10.1109/T-AFFC.2011.10	56	10	11	1	12	ELSEVIER	AMSTERDAM	RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS	0262-8856	1872-8138		IMAGE VISION COMPUT	Image Vis. Comput.	JAN	2019	81						1	14		10.1016/j.imavis.2018.10.002	http://dx.doi.org/10.1016/j.imavis.2018.10.002			14	Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Optics	Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering; Optics	HO1ZJ	30524157	Bronze, Green Accepted			2022-10-03	WOS:000460710900001
B	Cohn, JF; Ertugrul, IO; Chu, WS; Girard, JM; Jeni, LA; Hammal, Z		AlamedaPineda, X; Ricci, E; Sebe, N		Cohn, Jeffrey; Ertugrul, Itir Onal; Chu, Wen-Sheng; Girard, Jeffrey M.; Jeni, Laszlo A.; Hammal, Zakia			Affective facial computing: Generalizability across domains	MULTIMODAL BEHAVIOR ANALYSIS IN THE WILD: ADVANCES AND CHALLENGES	Computer Vision and Pattern Recognition Series		English	Article; Book Chapter							EXPRESSION; RECOGNITION; COEFFICIENT; AGREEMENT; FEATURES; KAPPA		[Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Ertugrul, Itir Onal; Jeni, Laszlo A.; Hammal, Zakia] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Chu, Wen-Sheng] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Girard, Jeffrey M.] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA		Cohn, JF (corresponding author), Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA.		Girard, Jeffrey M. M/H-4088-2019; Chu, Wen-Sheng/AAF-6871-2019	Girard, Jeffrey M. M/0000-0002-7359-3746; Chu, Wen-Sheng/0000-0001-8592-6088				Abd El Meguid MK, 2014, IEEE T AFFECT COMPUT, V5, P141, DOI 10.1109/TAFFC.2014.2317711; Alyuz N., 2008, P 1 COST 2101 WORKSH; Ambadar Z, 2009, J NONVERBAL BEHAV, V33, P17, DOI 10.1007/s10919-008-0059-5; Baltrusaitis T., 2015, 2015 11 IEEE INT C W, V6, P1, DOI DOI 10.1109/FG.2015.7284869; Baltrusaitis T, 2017, INT CONF AFFECT, P111, DOI 10.1109/ACII.2017.8273587; Banziger T., 2007, P AFF COMP INT INT A; Bartlett M. S., 2006, Journal of Multimedia, V1, DOI 10.4304/jmm.1.6.22-35; Bartlett MS, 2014, CURR BIOL, V24, P738, DOI 10.1016/j.cub.2014.02.009; Bennett FM, 1954, PUBLIC OPIN QUART, V18, P303; BRENNAN RL, 1981, EDUC PSYCHOL MEAS, V41, P687, DOI 10.1177/001316448104100307; Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339; Buolamwini Joy, 2018, P MACH LEARN RES 1 1, V81, P1; Chen M., P 29 INT C MACH LEAR; Chen W.-Y., 2016, P EUR C COMP VIS; Chu WS, 2017, IEEE INT CONF AUTOMA, P25, DOI 10.1109/FG.2017.13; Chu WS, 2017, IEEE T PATTERN ANAL, V39, P529, DOI 10.1109/TPAMI.2016.2547397; COHEN J, 1968, PSYCHOL BULL, V70, P213, DOI 10.1037/h0026256; COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104; Cohn J.F., 2007, HDB EMOTION ELICITAT, P203, DOI [10.1007/978-3-540-72348-6_1, DOI 10.1007/978-3-540-72348-6_1]; Cohn J. F., 2004, P 6 IEEE INT C AUT F; Cohn JF., 2005, NEW HDB METHODS NONV, P9; Dhall A., 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS), DOI 10.1109/ICCVW.2011.6130508; Dibeklioglu H, 2018, IEEE J BIOMED HEALTH, V22, P525, DOI 10.1109/JBHI.2017.2676878; Douglas-Cowie E., 2008, LREC WORKSH CORP RES; Ekman P., 1992, FIN REP NSF PLANN WO; Ekman P, 2002, RES NEXUS; Ekman P., 2005, HDB COGNITION EMOTIO, P45, DOI [10.1007/978-3-319-28099-8_495-1, 10.1002/0470013494.ch3]; Eleftheriadis S, 2017, IEEE T IMAGE PROCESS, V26, P4697, DOI 10.1109/TIP.2017.2721114; Frank M., RU FACS 1 DATABASE; FRIDLUND AJ, 1991, J PERS SOC PSYCHOL, V60, P229, DOI 10.1037/0022-3514.60.2.229; Gehrig T., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2092, DOI 10.1109/ICCVW.2011.6130506; Georgopoulos M., 2018, MODELLING FACIAL AGI; Ghifary M., 2015, P INT C COMP VIS; Ghosh S., 2015, P AFF COMP INT INT X; Ghosh S, 2015, INT CONF AFFECT, P609, DOI 10.1109/ACII.2015.7344632; Girard J. M., 2017, P IEEE INT C AUT FAC; Girard J. M., 2015, P IEEE INT C AUT FAC; Girard JM, 2016, ASSESSMENT, V23, P404, DOI 10.1177/1073191116635807; Girard JM, 2015, CURR OPIN PSYCHOL, V4, P75, DOI 10.1016/j.copsyc.2014.12.010; Girard JM, 2015, BEHAV RES METHODS, V47, P1136, DOI 10.3758/s13428-014-0536-1; Girard JM, 2014, IMAGE VISION COMPUT, V32, P641, DOI 10.1016/j.imavis.2013.12.007; Goeleven E, 2008, COGNITION EMOTION, V22, P1094, DOI 10.1080/02699930701626582; Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Gu WF, 2012, PATTERN RECOGN, V45, P80, DOI 10.1016/j.patcog.2011.05.006; Hammal Z, 2018, SOCIAL INTERPERSONAL, P121, DOI [10.1007/978-3-319-78340-6_7, DOI 10.1007/978-3-319-78340-6_7]; Hammal Z., 2017, P INT C AFF COMP INT; Hancock P., 2008, PSYCHOL IMAGE COLLEC; Hasani B, 2017, IEEE COMPUT SOC CONF, P2278, DOI 10.1109/CVPRW.2017.282; Hasani B, 2017, IEEE INT CONF AUTOMA, P790, DOI 10.1109/FG.2017.99; Hupont I., 2017, PATTERN ANAL APPL, P1; Jeni Laszlo A., 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163142; Jeni L. A., 2013, P HUM ASS C AFF COMP; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Ji Q., ISL FACIAL EXPRESSIO; Jiang BH, 2014, IEEE T CYBERNETICS, V44, P161, DOI 10.1109/TCYB.2013.2249063; Kanade T., 2000, P 4 INT C AUT FAC GE; Kim T, 2017, PR MACH LEARN RES, V70; Koelstra S, 2010, IEEE T PATTERN ANAL, V32, P1940, DOI 10.1109/TPAMI.2010.50; Konstantinos B., 2017, P IEEE INT C COMP VI; Langner O., 2009, COGNITION EMOTION, V24, P1377; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li YQ, 2013, IEEE T AFFECT COMPUT, V4, P127, DOI 10.1109/T-AFFC.2013.5; Liu M, 2017, AMB EXPRESS, V7, DOI 10.1186/s13568-017-0355-8; Long MS, 2015, PR MACH LEARN RES, V37, P97; Lucey P., 2010, 3 IEEE WORKSH CVPR H, P1; Lucey P, 2012, IMAGE VISION COMPUT, V30, P197, DOI 10.1016/j.imavis.2011.12.003; Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949; Martinez B, 2019, IEEE T AFFECT COMPUT, V10, P325, DOI 10.1109/TAFFC.2017.2731763; Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4; Mayer C., 2014, Pattern Recognition and Image Analysis, V24, P124, DOI 10.1134/S1054661814010106; McDuff D., 2017, P AFF COMP INT INT S; McDuff D, 2017, J NONVERBAL BEHAV, V41, P1, DOI 10.1007/s10919-016-0244-x; McGraw KO, 1996, PSYCHOL METHODS, V1, P30, DOI 10.1037/1082-989X.1.1.30; McKeown G, 2012, IEEE T AFFECT COMPUT, V3, P5, DOI 10.1109/T-AFFC.2011.20; Meng ZB, 2017, IEEE INT CONF AUTOMA, P558, DOI 10.1109/FG.2017.140; Miao YQ, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 2, P326, DOI 10.1109/ICMLA.2012.178; Mohammadian A, 2016, EXPERT SYST APPL, V56, P282, DOI 10.1016/j.eswa.2016.03.023; Mollahosseini Ali, 2016, 2016 IEEE WINT C APP, P1, DOI [DOI 10.1109/WACV.2016.7477450, 10.1109/WACV.2016.7477450]; Oster H., 2001, BABY FACS FACIAL ACT; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pantic M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P317, DOI 10.1109/ICME.2005.1521424; Radford A., 2016, ARXIV PREPRINT ARXIV; Rudovic O, 2015, IEEE T PATTERN ANAL, V37, P944, DOI 10.1109/TPAMI.2014.2356192; Ruiz M, 2015, PROC INT CONF RECON; RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714; Savran A., 2008, P 1 COST 2101 WORKSH; SCHLOSBERG H, 1954, PSYCHOL REV, V61, P81, DOI 10.1037/h0054570; Schmidt KL, 2006, J NONVERBAL BEHAV, V30, P37, DOI 10.1007/s10919-005-0003-x; Senechal T, 2012, IEEE T SYST MAN CY B, V42, P993, DOI 10.1109/TSMCB.2012.2193567; Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005; Shu X., 2015, P ACM INT C MULT BRI; Sun B, 2016, PROC EUR CONF ANTENN; Sun Wei, 2012, 2 INT C COMP APPL AS, V1, P106; Taigman Y., 2017, ARXIV161102200; Tang C., 2017, P IEEE INT C AUT FAC; Tingfan Wu, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P889, DOI 10.1109/FG.2011.5771369; Tulyakov S, 2018, IEEE T PATTERN ANAL, V40, P2250, DOI 10.1109/TPAMI.2017.2750687; Tzeng E., 2017, P IEEE INT C COMP VI; Tzeng E., 2014, ARXIV PREPRINT ARXIV; Valstar M. F., 2017, P INT C AUT FAC GEST; Valstar M. F., 2006, P ACM INT C MULT INT; Valstar M. F., 2015, P IEEE INT C AUT FAC; Valstar MF, 2010, P 3 INT WORKSH EM SA; Valstar M, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P3, DOI 10.1145/2988257.2988258; Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P28, DOI 10.1109/TSMCB.2011.2163710; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Wachtman GS, 2001, PLAST RECONSTR SURG, V107, P1124, DOI 10.1097/00006534-200104150-00005; Walecki R, 2017, IMAGE VISION COMPUT, V58, P25, DOI 10.1016/j.imavis.2016.04.009; Wallhoff F., 2006, FACIAL EXPRESSION EM; Wang J, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P75, DOI 10.1145/2671188.2749311; Wang SF, 2017, PATTERN RECOGN, V61, P78, DOI 10.1016/j.patcog.2016.07.028; WATSON D, 1985, PSYCHOL BULL, V98, P219, DOI 10.1037/0033-2909.98.2.219; Wen GH, 2017, COGN COMPUT, V9, P597, DOI 10.1007/s12559-017-9472-6; Wu Y, 2016, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2016.370; Yan K., INT C NEUR INF PROC, P427; Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211; Zhang X., 2016, P IEEE INT C COMP VI; Zhang X, 2015, MACH VISION APPL, V26, P467, DOI 10.1007/s00138-015-0677-y; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369; Zheng Wei-Long, 2017, IEEE T AFFECTIVE COM; Zhu R., 2015, P INT C BIOM ICB; Zhu RH, 2016, INT CONF BIOMETR	124	9	10	0	0	ACADEMIC PRESS LTD-ELSEVIER SCIENCE LTD	LONDON	125 LONDON WALL, LONDON EC2Y 5AS, ENGLAND			978-0-12-814602-6; 978-0-12-814601-9	COMPUT VIS PATT REC			2019							407	441		10.1016/B978-0-12-814601-9.00026-2	http://dx.doi.org/10.1016/B978-0-12-814601-9.00026-2	10.1016/C2017-0-01387-3		35	Behavioral Sciences; Computer Science, Artificial Intelligence; Ecology	Book Citation Index – Science (BKCI-S)	Behavioral Sciences; Computer Science; Environmental Sciences & Ecology	BN1AR					2022-10-03	WOS:000473685200020
C	Daoudi, M; Hammal, Z; Kacem, A; Cohn, JF			IEEE	Daoudi, Mohamed; Hammal, Zakia; Kacem, Anis; Cohn, Jeffrey			Gram Matrices Formulation of Body Shape Motion: An Application for Depression Severity Assessment	2019 8TH INTERNATIONAL CONFERENCE ON AFFECTIVE COMPUTING AND INTELLIGENT INTERACTION WORKSHOPS AND DEMOS (ACIIW)			English	Proceedings Paper	8th International Conference on Affective Computing and Intelligent Interaction (ACII)	SEP 03-06, 2019	Cambridge, ENGLAND			Gram matrices; body movement; dynamics; depression severity	POSITIVE SEMIDEFINITE MATRICES	We propose an automatic method to measure depression severity from body movement dynamics in participants undergoing treatment for depression. Participants in a clinical trial for treatment of depression were interviewed on up to four occasions at 7-week intervals with the clinician-administered Hamilton Rating Scale for Depression. Body movement was tracked using OpenPose from full-body video recordings of the interviews. Gram matrices formulation was used for body shape and trajectory representations from each video interview. Kinematic features were extracted and encoded for video based representation using Gaussian Mixture Models (GMM) and Fisher vector encoding. A multi-class SVM was used to classify the encoded body movement dynamics into three levels of depression severity: severe, mild, and remission. Accuracy was high for severe depression (68.57%) followed by mild depression (56%), and then remission (37.93%). The obtained results suggest that automatic detection of depression severity from body movement is feasible.	[Daoudi, Mohamed; Kacem, Anis] Univ Lille, IMT Lille Douai, CNRS, UMR 9189,CRIStAL, Lille, France; [Hammal, Zakia] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Hammal, Z (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.	zhammal@andrew.cmu.edu	; Daoudi, Mohammed/H-5935-2013	kacem, anis/0000-0003-0640-9862; Daoudi, Mohammed/0000-0003-4219-7860	National Institute of Nursing Research of the National Institutes of Health [R21NR016510]; National Institutes of Health [MH096951, MH51435]	National Institute of Nursing Research of the National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Nursing Research (NINR)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	Research reported in this publication was supported in part by the National Institute of Nursing Research of the National Institutes of Health under Award Number R21NR016510, and the National Institutes of Health under awards MH096951 and MH51435. The content is solely the responsibility of the authors and does not necessarily represent the official views of the sponsors.	Alghowinem S, 2013, INT CONF AFFECT, P283, DOI 10.1109/ACII.2013.53; Bonnabel S, 2009, SIAM J MATRIX ANAL A, V31, P1055, DOI 10.1137/080731347; Boumal N, 2014, J MACH LEARN RES, V15, P1455; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Dibeklioglu H., 2017, IEEE J BIOMED HEALTH; Dibeklioglu H, 2018, IEEE J BIOMED HEALTH, V22, P525, DOI 10.1109/JBHI.2017.2676878; Faraki M., 2016, APPL COMP VIS WACV 2, P1, DOI DOI 10.1109/ITEC.2016.7520261; First M., 2002, STRUCTURED CLIN INTE, DOI DOI 10.1016/J.JADA.2007.07.017; Fournier JC, 2010, JAMA-J AM MED ASSOC, V303, P47, DOI 10.1001/jama.2009.1943; FOWLES DC, 1994, NEBR SYM MOTIV, V41, P181; Gousenbourger P.-Y., 2017, ESANN; HAMILTON M, 1960, J NEUROL NEUROSUR PS, V23, P56, DOI 10.1136/jnnp.23.1.56; Joshi J., 2013, P 10 IEEE INT C WORK, P1, DOI DOI 10.1109/FG.2013.6553796; Kacem A., 2018, IEEE T PATTERN ANAL, P1; Kacem A, 2018, IEEE INT CONF AUTOMA, P739, DOI 10.1109/FG.2018.00116; Kacem A, 2017, IEEE I CONF COMP VIS, P3199, DOI 10.1109/ICCV.2017.345; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Massart E., PREPRINT; Meyer G, 2011, J MACH LEARN RES, V12, P593; Nesse RM, 2000, ARCH GEN PSYCHIAT, V57, P14, DOI 10.1001/archpsyc.57.1.14; Otberdout N., 2018, P BMVC, P159; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Scherer Stefan, 2014, Proc ACM Int Conf Multimodal Interact, V2014, P112, DOI 10.1145/2663204.2663238; Vandereycken B, 2009, 2009 IEEE/SP 15TH WORKSHOP ON STATISTICAL SIGNAL PROCESSING, VOLS 1 AND 2, P389, DOI 10.1109/SSP.2009.5278558; Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992	25	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-7281-3891-6				2019							258	263						6	Computer Science, Artificial Intelligence; Computer Science, Cybernetics; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BT0ZF		Green Submitted			2022-10-03	WOS:000794297900052
C	Du, WC; Morency, LP; Cohn, JF; Black, AW			Int Speech Commun Assoc	Du, Wenchao; Morency, Louis-Philippe; Cohn, Jeffrey; Black, Alan W.			Bag-of-Acoustic-Words for Mental Health Assessment: A Deep Autoencoding Approach	INTERSPEECH 2019	Interspeech		English	Proceedings Paper	Interspeech Conference	SEP 15-19, 2019	Graz, AUSTRIA			acoustic word; deep learning; affective computing; social interaction; behavior markers		Despite the recent success of deep learning, it is generally difficult to apply end-to-end deep neural networks to small datasets, such as those from the health domain, due to the tendency of neural networks to over-fit. In addition, how neural models reach their decisions is not well understood. In this paper, we present a two-stage approach to acoustic-based classification of behavior markers related to mental health disorders: first, a dictionary and the mapping from speech signals to the dictionary are learned jointly by a deep autoencoder, then the bag-of-words representation of speech is used for classification, using classifiers with simple decision boundaries. This deep bag-of-features approach has the advantage of offering more interpretability, while the use of deep autoencoder gains improvements in prediction by learning higher level features with long range dependencies, comparing to previous work using only low-level descriptors. In addition, we demonstrate the use of labeled emotion recognition data from other domains to supervise acoustic word encoding in order to help predict psychological traits. Experiments are conducted on audio recordings of 65 clinically recorded interviews with the self-reported level of post-traumatic stress disorder (PTSD), depression, and rapport with the interviewers.	[Du, Wenchao; Morency, Louis-Philippe; Black, Alan W.] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Du, WC (corresponding author), Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.	wenchaod@cs.cmu.edu; morency@cs.cmu.edu; jeffcohn@pitt.edu; awb@cs.cmu.edu			National Science Foundation [IIS-1721667, IIS-1722822]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work partially supported by the National Science Foundation (IIS-1721667 and IIS-1722822). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of National Science Foundation, and no official endorsement should be inferred.	Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Brendel W., 2019, INT C LEARN REPR; Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6; Chung YA, 2016, INTERSPEECH, P765, DOI 10.21437/Interspeech.2016-82; Cohn J. F., 2018, HDB MULTIMODAL MULTI, V2, P375; Cummins N, 2015, SPEECH COMMUN, V71, P10, DOI 10.1016/j.specom.2015.03.004; Degottex Gilles, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P960, DOI 10.1109/ICASSP.2014.6853739; Eyben F, 2013, RECENT DEV OPENSMILE, P835; Gratch J, 2007, LECT NOTES ARTIF INT, V4722, P125; Gratch J, 2014, LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P3123; Ioffe S., 2015, INT C MACHINE LEARNI, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Kingma D.P., 2015, ARXIV; Lim H, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3325; McAuliffe M., 2017, INTERSPEECH; Pancoast S., 2014, P ICASSP, P1370; Passalis N, 2017, IEEE I CONF COMP VIS, P5766, DOI 10.1109/ICCV.2017.614; Paszke A, 2017, NIPS; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Plinge Axel, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3704, DOI 10.1109/ICASSP.2014.6854293; Rawat S, 2013, INTERSPEECH, P2928; Sainath TN, 2015, INT CONF ACOUST SPEE, P4580, DOI 10.1109/ICASSP.2015.7178838; Scherer S, 2013, IEEE INT CONF AUTOMA; Scherer S, 2013, INTERSPEECH, P847; Schmitt M, 2016, INTERSPEECH, P495, DOI 10.21437/Interspeech.2016-1124; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Yang Y, 2013, IEEE T AFFECT COMPUT, V4, P142, DOI 10.1109/T-AFFC.2012.38; Zhang Y., 2017, NEURIPS, P4169, DOI 10.1109/TCYB.2020.2973300	27	0	0	0	0	ISCA-INT SPEECH COMMUNICATION ASSOC	BAIXAS	C/O EMMANUELLE FOXONET, 4 RUE DES FAUVETTES, LIEU DIT LOUS TOURILS, BAIXAS, F-66390, FRANCE	2308-457X			INTERSPEECH			2019							1428	1432		10.21437/Interspeech.2019-3059	http://dx.doi.org/10.21437/Interspeech.2019-3059			5	Audiology & Speech-Language Pathology; Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Audiology & Speech-Language Pathology; Computer Science	BT4LP					2022-10-03	WOS:000831796401117
C	Ertugrul, IO; Cohn, JF; Jeni, LA; Zhang, Z; Yin, LJ; Ji, Q			IEEE	Ertugrul, Itir Onal; Cohn, Jeffrey; Jeni, Laszlo A.; Zhang, Zheng; Yin, Lijun; Ji, Qiang			Cross-domain AU Detection: Domains, Learning Approaches, and Measures	2019 14TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG 2019)	IEEE International Conference on Automatic Face and Gesture Recognition and Workshops		English	Proceedings Paper	14th IEEE International Conference on Automatic Face and Gesture Recognition (FG)	MAY 14-18, 2019	Lille, FRANCE	IEEE, Univ Lille, Inst Mines Telecom, Univ Lille, Inst Mines Telecom, Ecole Mines Telecom, IMT Lille Douai, INRIA, 3DMD, Google, I Site Univ Lille Nord Europe, Centre Rech Informatique Signal Automatique Lille, IEEE Comp Soc, IEEE Biometr Council			FACIAL EXPRESSION; ALCOHOL; FEATURES; 3D	Facial action unit ( AU) detectors have performed well when trained and tested within the same domain. Do AU detectors transfer to new domains in which they have not been trained? To answer this question, we review literature on cross-domain transfer and conduct experiments to address limitations of prior research. We evaluate both deep and shallow approaches to AU detection ( CNN and SVM, respectively) in two large, well-annotated, publicly available databases, Expanded BP4D+ and GFT. The databases differ in observational scenarios, participant characteristics, range of head pose, video resolution, and AU base rates. For both approaches and databases, performance decreased with change in domain, often to below the threshold needed for behavioral research. Decreases were not uniform, however. They were more pronounced for GFT than for Expanded BP4D+ and for shallow relative to deep learning. These findings suggest that more varied domains and deep learning approaches may be better suited for promoting generalizability. Until further improvement is realized, caution is warranted when applying AU classifiers from one domain to another.	[Ertugrul, Itir Onal; Jeni, Laszlo A.] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Zhang, Zheng; Yin, Lijun] SUNY Binghamton, Dept Comp Sci, Binghamton, NY 13902 USA; [Ji, Qiang] Rensselaer Polytech Inst, Troy, NY USA		Ertugrul, IO (corresponding author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.				NIH [NS 100549, MH096951]; NSF [CNS-1629716, CNS-1629898, CNS-1629856]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	This research was supported in part by NIH awards NS 100549 and MH096951 and NSF awards CNS-1629716, CNS-1629898, and CNS-1629856.	Corneanu CA, 2016, IEEE T PATTERN ANAL, V38, P1548, DOI 10.1109/TPAMI.2016.2515606; Almaev TR, 2013, INT CONF AFFECT, P356, DOI 10.1109/ACII.2013.65; Banziger T, 2007, LECT NOTES COMPUT SC, V4738, P476; Baltrusaitis T., 2015, 2015 11 IEEE INT C W, V6, P1, DOI DOI 10.1109/FG.2015.7284869; Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019; Bartlett M. S., 2006, Journal of Multimedia, V1, DOI 10.4304/jmm.1.6.22-35; Bennett FM, 1954, PUBLIC OPIN QUART, V18, P303; BRENNAN RL, 1981, EDUC PSYCHOL MEAS, V41, P687, DOI 10.1177/001316448104100307; Chen JX, 2013, PATTERN RECOGN LETT, V34, P1964, DOI 10.1016/j.patrec.2013.02.002; Chew SW, 2012, IEEE T SYST MAN CY B, V42, P1006, DOI 10.1109/TSMCB.2012.2194485; Chu WS, 2017, IEEE INT CONF AUTOMA, P25, DOI 10.1109/FG.2017.13; Chu WS, 2017, IEEE T PATTERN ANAL, V39, P529, DOI 10.1109/TPAMI.2016.2547397; Cohn J. F., 2018, ICMI; Cohn JF, 2019, COMPUT VIS PATT REC, P407, DOI 10.1016/B978-0-12-814601-9.00026-2; Cohn JF., 2005, NEW HDB METHODS NONV, P9; Douglas-Cowie E., 2008, LREC WORKSH CORP RES, P1; Ekman P, 2002, RES NEXUS; Eleftheriadis S, 2017, IEEE T IMAGE PROCESS, V26, P4697, DOI 10.1109/TIP.2017.2721114; Fairbairn CE, 2015, CLIN PSYCHOL SCI, V3, P686, DOI 10.1177/2167702614548892; Fairbairn CE, 2013, EMOTION, V13, P468, DOI 10.1037/a0030934; Gehrig T., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2092, DOI 10.1109/ICCVW.2011.6130506; Ghosh S, 2015, INT CONF AFFECT, P609, DOI 10.1109/ACII.2015.7344632; Girard Jeffrey M., 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163106; Girard JM, 2017, IEEE INT CONF AUTOMA, P581, DOI 10.1109/FG.2017.144; Hammal Z, 2017, INT CONF AFFECT, P216, DOI 10.1109/ACII.2017.8273603; Huang J, 2005, IEEE T KNOWL DATA EN, V17, P299, DOI 10.1109/TKDE.2005.50; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Jeni LA, 2013, INT CONF AFFECT, P245, DOI 10.1109/ACII.2013.47; Jiang BH, 2014, IEEE T CYBERNETICS, V44, P161, DOI 10.1109/TCYB.2013.2249063; Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611; Koelstra S, 2010, IEEE T PATTERN ANAL, V32, P1940, DOI 10.1109/TPAMI.2010.50; Li YQ, 2013, IEEE T AFFECT COMPUT, V4, P127, DOI 10.1109/T-AFFC.2013.5; Lucey P, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P57, DOI 10.1109/FG.2011.5771462; Lucey P., 2010, 2010 IEEE COMPUTER S, P94, DOI [10.1109/CVPRW.2010.5543262, DOI 10.1109/CVPRW.2010.5543262]; Martinez B., 2017, IEEE T AFFECTIVE COM; Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4; McKeown G, 2012, IEEE T AFFECT COMPUT, V3, P5, DOI 10.1109/T-AFFC.2011.20; Mohammadian A, 2016, EXPERT SYST APPL, V56, P282, DOI 10.1016/j.eswa.2016.03.023; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pantic M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P317, DOI 10.1109/ICME.2005.1521424; Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976; Ruiz A, 2015, IEEE I CONF COMP VIS, P3703, DOI 10.1109/ICCV.2015.422; Sayette MA, 2012, PSYCHOL SCI, V23, P869, DOI 10.1177/0956797611435134; Senechal T, 2012, IEEE T SYST MAN CY B, V42, P993, DOI 10.1109/TSMCB.2012.2193567; Shao Z., 2018, ARXIV180305588; Simon T, 2010, PROC CVPR IEEE, P2737, DOI 10.1109/CVPR.2010.5539998; Tong Y., 2007, IEEE TPAMI, V29; Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P28, DOI 10.1109/TSMCB.2011.2163710; Walecki R., 2016, IMAGE VISION COMPUTI, V2; Wang J, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P75, DOI 10.1145/2671188.2749311; Wang SF, 2017, PATTERN RECOGN, V61, P78, DOI 10.1016/j.patcog.2016.07.028; Wu Y, 2016, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2016.370; Zadeh A, 2017, IEEE INT CONF COMP V, P2519, DOI 10.1109/ICCVW.2017.296; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhang Z, 2016, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2016.374; Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369; Zhou F, 2010, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2010.5539966	57	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2326-5396		978-1-7281-0089-0	IEEE INT CONF AUTOMA			2019							246	253						8	Engineering, Electrical & Electronic	Conference Proceedings Citation Index - Science (CPCI-S)	Engineering	BN6AU					2022-10-03	WOS:000484960400032
C	Ertugrul, IO; Jeni, LA; Ding, WQ; Cohn, JF			IEEE	Ertugrul, Itir Onal; Jeni, Laszlo A.; Ding, Wanqiao; Cohn, Jeffrey			AFAR: A Deep Learning Based Tool for Automated Facial Affect Recognition	2019 14TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG 2019)	IEEE International Conference on Automatic Face and Gesture Recognition and Workshops		English	Proceedings Paper	14th IEEE International Conference on Automatic Face and Gesture Recognition (FG)	MAY 14-18, 2019	Lille, FRANCE	IEEE, Univ Lille, Inst Mines Telecom, Univ Lille, Inst Mines Telecom, Ecole Mines Telecom, IMT Lille Douai, INRIA, 3DMD, Google, I Site Univ Lille Nord Europe, Centre Rech Informatique Signal Automatique Lille, IEEE Comp Soc, IEEE Biometr Council					[Ertugrul, Itir Onal; Jeni, Laszlo A.] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Ding, Wanqiao; Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Ertugrul, IO (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.				NIH [NS100549, MH096951]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This research was supported in part by NIH awards NS100549 and MH096951.	Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019; Chu WS, 2017, IEEE INT CONF AUTOMA, P25, DOI 10.1109/FG.2017.13; Cohn J. F., 2018, ICMI; Ertugrul I., 2019, FG; Ertugrul IO, 2018, IEEE COMPUT SOC CONF, P2211, DOI 10.1109/CVPRW.2018.00287; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Li W., 2018, TPAMI	7	1	1	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2326-5396		978-1-7281-0089-0	IEEE INT CONF AUTOMA			2019							744	744						1	Engineering, Electrical & Electronic	Conference Proceedings Citation Index - Science (CPCI-S)	Engineering	BN6AU					2022-10-03	WOS:000484960400114
C	Girard, JM; Shandar, G; Liu, Z; Cohn, JF; Yin, LJ; Morency, LP			IEEE	Girard, Jeffrey M.; Shandar, Gayatri; Liu, Zhun; Cohn, Jeffrey; Yin, Lijun; Morency, Louis-Philippe			Reconsidering the Duchenne Smile: Indicator of Positive Emotion or Artifact of Smile Intensity?	2019 8TH INTERNATIONAL CONFERENCE ON AFFECTIVE COMPUTING AND INTELLIGENT INTERACTION (ACII)	International Conference on Affective Computing and Intelligent Interaction		English	Proceedings Paper	8th International Conference on Affective Computing and Intelligent Interaction (ACII)	SEP 03-06, 2019	Cambridge, ENGLAND			Duchenne smile; facial expression; nonverbal behavior; emotion; FACS; Bayesian data analysis	SPONTANEOUS FACIAL EXPRESSIONS; DELIBERATE; MOVEMENT; LAUGHTER; FALSE; FELT	The Duchenne smile hypothesis is that smiles that include eye constriction (AU6) are the product of genuine positive emotion, whereas smiles that do not are either falsified or related to negative emotion. This hypothesis has become very influential and is often used in scientific and applied settings to justify the inference that a smile is either true or false. However, empirical support for this hypothesis has been equivocal and some researchers have proposed that, rather than being a reliable indicator of positive emotion, AU6 may just be an artifact produced by intense smiles. Initial support for this proposal has been found when comparing smiles related to genuine and feigned positive emotion; however, it has not yet been examined when comparing smiles related to genuine positive and negative emotion. The current study addressed this gap in the literature by examining spontaneous smiles from 136 participants during the elicitation of amusement, embarrassment, fear, and pain (from the BP4D+ dataset). Bayesian multilevel regression models were used to quantify the associations between AU6 and self-reported amusement while controlling for smile intensity. Models were estimated to infer amusement from AU6 and to explain the intensity of AU6 using amusement. In both cases, controlling for smile intensity substantially reduced the hypothesized association, whereas the effect of smile intensity itself was quite large and reliable. These results provide further evidence that the Duchenne smile is likely an artifact of smile intensity rather than a reliable and unique indicator of genuine positive emotion.	[Girard, Jeffrey M.; Shandar, Gayatri; Liu, Zhun; Morency, Louis-Philippe] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Yin, Lijun] Binghamton Univ, Dept Comp Sci, Binghamton, NY USA		Girard, JM (corresponding author), Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.	jmgirard@cmu.edu; gshandar@cs.cmu.edu; zhunl@cs.cmu.edu; jeffcohn@pitt.edu; lijun@cs.binghamton.edu; morency@cs.cmu.edu	Girard, Jeffrey M. M/H-4088-2019	Girard, Jeffrey M. M/0000-0002-7359-3746	National Science Foundation [1629716, 1629898, 1722822, 1734868]; National Institutes of Health [MH096951]	National Science Foundation(National Science Foundation (NSF)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This material is based upon work partially supported by the National Science Foundation (1629716, 1629898, 1722822, 1734868) and National Institutes of Health (MH096951). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of National Science Foundation or National Institutes of Health, and no official endorsement should be inferred.	Ambadar Z, 2009, J NONVERBAL BEHAV, V33, P17, DOI 10.1007/s10919-008-0059-5; Barr DJ, 2013, J MEM LANG, V68, P255, DOI 10.1016/j.jml.2012.11.001; Bernstein MJ, 2008, PSYCHOL SCI, V19, P981, DOI 10.1111/j.1467-9280.2008.02187.x; Bliese PD, 2004, ORGAN RES METHODS, V7, P400, DOI 10.1177/1094428104268542; Burkner PC, 2017, J STAT SOFTW, V80, P1, DOI 10.18637/jss.v080.i01; Carroll JM, 1997, J PERS SOC PSYCHOL, V72, P164, DOI 10.1037/0022-3514.72.1.164; Curran PJ, 2007, PSYCHOL METHODS, V12, P283, DOI 10.1037/1082-989X.12.3.283; DeCoster J, 2009, PSYCHOL METHODS, V14, P349, DOI 10.1037/a0016956; Dupont S., 2015, 11 IEEE INT C WORKSH, V5, P1; EKMAN P, 1988, J PERS SOC PSYCHOL, V54, P414, DOI 10.1037/0022-3514.54.3.414; EKMAN P, 1982, J NONVERBAL BEHAV, V6, P238, DOI 10.1007/BF00987191; EKMAN P, 1992, PSYCHOL REV, V99, P550, DOI 10.1037/0033-295X.99.3.550; EKMAN P, 1990, J PERS SOC PSYCHOL, V58, P342, DOI 10.1037/0022-3514.58.2.342; Ekman P, 2002, RES NEXUS; FRANK MG, 1993, J PERS SOC PSYCHOL, V64, P83, DOI 10.1037/0022-3514.64.1.83; FRANK MG, 1993, HUMOR, V6, P9, DOI 10.1515/humr.1993.6.1.9; Gadassi R, 2016, J BEHAV THER EXP PSY, V50, P8, DOI 10.1016/j.jbtep.2015.04.007; Gelman A., 2014, BAYESIAN DATA ANAL, V1542; Gelman A, 2015, J EDUC BEHAV STAT, V40, P530, DOI 10.3102/1076998615606113; Gosselin P, 2010, EMOTION, V10, P266, DOI 10.1037/a0017748; Gunnery S. D., 2015, SOCIAL PSYCHOL NONVE; Gunnery SD, 2013, J NONVERBAL BEHAV, V37, P29, DOI 10.1007/s10919-012-0139-4; Gwet K.L, 2014, HDB INTERRATER RELIA; Harris CR, 2005, COGNITION EMOTION, V19, P655, DOI 10.1080/02699930441000472; HESS U, 1990, EUR J SOC PSYCHOL, V20, P369, DOI 10.1002/ejsp.2420200502; Hess U, 2010, BIOL PSYCHOL, V84, P514, DOI 10.1016/j.biopsycho.2009.11.001; Hofmann J, 2017, IEEE T AFFECT COMPUT, V8, P495, DOI 10.1109/TAFFC.2017.2737000; Jakobs E, 1999, COGNITION EMOTION, V13, P321, DOI 10.1080/026999399379212; Jaynes E. T., 2007, PROBABILITY THEORY L; KELTNER D, 1995, J PERS SOC PSYCHOL, V68, P441, DOI 10.1037/0022-3514.68.3.441; Keltner D, 1997, J PERS SOC PSYCHOL, V73, P687, DOI 10.1037/0022-3514.73.4.687; Krumhuber EG, 2009, EMOTION, V9, P807, DOI 10.1037/a0017844; Kruschke JK, 2018, PSYCHON B REV, V25, P178, DOI 10.3758/s13423-016-1221-4; Kruschke JK, 2018, ADV METH PRACT PSYCH, V1, P270, DOI 10.1177/2515245918771304; Lee V, 1998, SEMIOTICA, V120, P39, DOI 10.1515/semi.1998.120.1-2.39; Matsumoto D, 2009, J PERS SOC PSYCHOL, V96, P1, DOI 10.1037/a0014037; McElreath R., 2016, ELEMENTS EVOLUTIONAR; Messinger DS, 2008, J NONVERBAL BEHAV, V32, P133, DOI 10.1007/s10919-008-0048-8; Papa A, 2008, EMOTION, V8, P1, DOI 10.1037/1528-3542.8.1.1; Schmidt KL, 2009, J NONVERBAL BEHAV, V33, P35, DOI 10.1007/s10919-008-0058-6; Schmidt KL, 2006, J NONVERBAL BEHAV, V30, P37, DOI 10.1007/s10919-005-0003-x; Schmidt KL, 2001, YEARB PHYS ANTHROPOL, V44, P3, DOI 10.1002/ajpa.20001; SCHNEIDER K, 1991, J NONVERBAL BEHAV, V15, P185, DOI 10.1007/BF01672220; Soussignan R, 1996, ETHOLOGY, V102, P1020, DOI 10.1111/j.1439-0310.1996.tb01179.x; Zhang Z, 2016, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2016.374	45	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2156-8103		978-1-7281-3888-6	INT CONF AFFECT			2019														6	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Engineering, Electrical & Electronic	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering	BO6TZ					2022-10-03	WOS:000522220800101
J	Hammal, Z; Wallace, ER; Speltz, ML; Heike, CL; Birgfeld, CB; Cohn, JF				Hammal, Zakia; Wallace, Erin R.; Speltz, Matthew L.; Heike, Carrie L.; Birgfeld, Craig B.; Cohn, Jeffrey			Dynamics of Face and Head Movement in Infants with and without Craniofacial Microsomia: An Automatic Approach	PLASTIC AND RECONSTRUCTIVE SURGERY-GLOBAL OPEN			English	Article							EXPRESSIONS	Background: Craniofacial microsomia (CFM) is a congenital condition associated with malformations of the bone and soft tissue of the face and the facial nerves, all of which have the potential to impair facial expressiveness. We investigated whether CFM-related variation in expressiveness is evident as early as infancy. Methods: Participants were 113 ethnically diverse 13-month-old infants (n = 63 cases with CFM and n = 50 unaffected matched controls). They were observed in 2 emotion induction tasks designed to elicit positive and negative effects. Facial and head movement was automatically measured using a computer vision-based approach. Expressiveness was quantified as the displacement, velocity, and acceleration of 49 facial landmarks (eg, lip corners) and head pitch and yaw. Results: For both cases and controls, all measures of expressiveness strongly differed between tasks. Case-control differences were limited to infants with microtia plus mandibular hypoplasia and other associated CFM features, which were the most common phenotypes and were characterized by decreased expressiveness relative to control infants. Conclusions: Infants with microtia plus mandibular hypoplasia and those with other associated CFM phenotypes were less facially expressive than same-aged peers. Both phenotypes were associated with more severe involvement than microtia alone, suggesting that infants with more severe CFM begin to diverge in expressiveness from controls by age 13 months. Further research is needed to both replicate the current findings and elucidate their developmental implications.	[Hammal, Zakia; Cohn, Jeffrey] Carnegie Mellon Univ, Robot Inst, 5000 Forbes Ave, Pittsburgh, PA 15213 USA; [Wallace, Erin R.; Speltz, Matthew L.; Heike, Carrie L.; Birgfeld, Craig B.] Seattle Childrens Res Inst, Seattle, WA USA; [Speltz, Matthew L.; Heike, Carrie L.; Birgfeld, Craig B.] Univ Washington, Sch Med, Seattle, WA USA; [Heike, Carrie L.; Birgfeld, Craig B.] Seattle Childrens Hosp, Seattle, WA USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Hammal, Z (corresponding author), Carnegie Mellon Univ, Robot Inst, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.	zakia_hammal@yahoo.fr	Heike, Carrie L./ABA-4852-2021	Heike, Carrie L./0000-0003-2178-7730	Center for Clinical and Translational Research at Seattle Children's Research Institute [UL1 TR000423]; National Institute of Dental and Craniofacial Research [R01 DE 022438, R03 DE026513]; NATIONAL CENTER FOR ADVANCING TRANSLATIONAL SCIENCES [UL1TR002319, UL1TR000423] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF DENTAL & CRANIOFACIAL RESEARCH [R01DE022438, R03DE026513] Funding Source: NIH RePORTER	Center for Clinical and Translational Research at Seattle Children's Research Institute; National Institute of Dental and Craniofacial Research(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Dental & Craniofacial Research (NIDCR)); NATIONAL CENTER FOR ADVANCING TRANSLATIONAL SCIENCES(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Advancing Translational Sciences (NCATS)); NATIONAL INSTITUTE OF DENTAL & CRANIOFACIAL RESEARCH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Dental & Craniofacial Research (NIDCR))	Supported by the Center for Clinical and Translational Research at Seattle Children's Research Institute, grant UL1 TR000423, and the National Institute of Dental and Craniofacial Research under award numbers R01 DE 022438 and R03 DE026513.	Corneanu CA, 2016, IEEE T PATTERN ANAL, V38, P1548, DOI 10.1109/TPAMI.2016.2515606; Ambadar Z, 2009, J NONVERBAL BEHAV, V33, P17, DOI 10.1007/s10919-008-0059-5; Ardinger MP, 1993, GENEREVIEWS; Birgfeld CB, 2016, HEAD FACE MED, V12, DOI 10.1186/s13005-016-0109-x; Birgfeld Craig B, 2012, Semin Plast Surg, V26, P91, DOI 10.1055/s-0032-1320067; Bishop CM, 2006, PATTERN RECOGN, P359; Bogart KR, 2012, REHABIL PSYCHOL, V57, P43, DOI 10.1037/a0026904; Bowlby J, 1969, ATTACHMENT; Busso C, 2007, IEEE T AUDIO SPEECH, V15, P1075, DOI 10.1109/TASL.2006.885910; Butterfield P. M., 2003, EMOTIONAL CONNECTION; Camras LA, 1998, DEV PSYCHOL, V34, P616, DOI 10.1037/0012-1649.34.4.616; Cohen J., 1988, STAT POWER ANAL SOCI, V2; Goldsmith H. H., 1999, THESIS; Gorlin RJ., 2001, SYNDROMES HEAD NECK, V4th ed, P101; Hammal Z, 2017, INT C AFF COMP INT I; Hammal Z, 2015, FRONTIERS HUMAN MEDI, V2, P1; Hammal Z, 2018, CLEFT PALATE-CRAN J, V55, P711, DOI 10.1177/1055665617753481; Hammal Z, 2015, IEEE T AFFECT COMPUT, V6, P361, DOI 10.1109/TAFFC.2015.2422702; Harris J, 1996, J MED GENET, V33, P809, DOI 10.1136/jmg.33.10.809; Heike CL, 2016, BIRTH DEFECTS RES A, V106, P915, DOI 10.1002/bdra.23560; Jeni LA, 2016, CHALEARN LOOK PEOPL; Jeni LA, 2016, EUR C COMP VIS AMST; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Kacem A, 2018, IEEE INT C AUT FAC G; KELTNER D, 1995, J PERS SOC PSYCHOL, V68, P441, DOI 10.1037/0022-3514.68.3.441; Lee G, 2013, J SPEC PEDIATR NURS, V18, P265, DOI 10.1111/jspn.12034; Pagon RA, 2009, GENEREVIEWS; Pederson DR, 1996, CHILD DEV, V67, P915, DOI 10.1111/j.1467-8624.1996.tb01773.x; POSWILLO D, 1988, DEVELOPMENT, V103, P207; Rothman KJ, 2014, J GEN INTERN MED, V29, P1060, DOI 10.1007/s11606-013-2755-z; Speltz ML, 2018, J PEDIATR-US, V198, P226, DOI 10.1016/j.jpeds.2018.02.076; TRONICK EZ, 1989, AM PSYCHOL, V44, P112, DOI 10.1037/0003-066X.44.2.112	32	6	6	0	0	LIPPINCOTT WILLIAMS & WILKINS	PHILADELPHIA	TWO COMMERCE SQ, 2001 MARKET ST, PHILADELPHIA, PA 19103 USA	2169-7574			PRS-GLOB OPEN	PRS-GLOB. OPEN	JAN	2019	7	1							e2081	10.1097/GOX.0000000000002081	http://dx.doi.org/10.1097/GOX.0000000000002081			7	Surgery	Emerging Sources Citation Index (ESCI)	Surgery	HL1LZ	30859039	gold, Green Published			2022-10-03	WOS:000458460600021
J	Nicolaou, MA; Zafeiriou, S; Kotsia, I; Zhao, GY; Cohn, JF				Nicolaou, Mihalis A.; Zafeiriou, Stefanos; Kotsia, Irene; Zhao, Guoying; Cohn, Jeffrey			Editorial of Special Issue on Human Behaviour Analysis "In-the-Wild"	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING			English	Editorial Material									[Nicolaou, Mihalis A.] Cyprus Inst, Computat Based Sci & Technol Res Ctr, 20 Konstantinou Kavafi St 2121, Nicosia, Cyprus; [Zafeiriou, Stefanos] Imperial Coll Londonm, Dept Comp, 180 Queens Gate, London SW7 2AZ, England; [Kotsia, Irene] Middlesex Univ, Comp Sci Dept, Sch Sci & Technol, London NW4 4BT, England; [Zhao, Guoying] Univ Oulu, Ctr Machine Vis & Signal Anal, Pentti Kaiteran Katu 1, Oulu 90014, Finland; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, 210 S Bouquet St,4327, Pittsburgh, PA 15260 USA		Nicolaou, MA (corresponding author), Cyprus Inst, Computat Based Sci & Technol Res Ctr, 20 Konstantinou Kavafi St 2121, Nicosia, Cyprus.	m.nicolaou@cyi.ac.cy; s.zafeiriou@imperial.ac.uk; I.Kotsia@mdx.ac.uk; guoying.zhao@oulu.fi; jeffcohn@pitt.edu	Zhao, Guoying/ABE-7716-2020	Zhao, Guoying/0000-0003-3694-206X	Tekes Fidipro Program [1849/31/2015]	Tekes Fidipro Program(Finnish Funding Agency for Technology & Innovation (TEKES))	This special issue would not have been possible without the efforts and interests from all the authors who contributed their submissions. We would like to take this opportunity to thank them. We are grateful to the reviewers for their careful and valuable comments on the submitted manuscripts and for their detailed and helpful suggestions for improvement. S. Zafeiriou and G. Zhao also acknowledge support from Tekes Fidipro Program (Grant No. 1849/31/2015)		0	2	2	0	1	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	1949-3045			IEEE T AFFECT COMPUT	IEEE Trans. Affect. Comput.	JAN-MAR	2019	10	1			SI		4	6		10.1109/TAFFC.2019.2895141	http://dx.doi.org/10.1109/TAFFC.2019.2895141			3	Computer Science, Artificial Intelligence; Computer Science, Cybernetics	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	HP0CQ		Bronze, Green Accepted, Green Submitted			2022-10-03	WOS:000461333200003
C	Pillai, RK; Jeni, LA; Yang, HY; Zhang, Z; Yin, LJ; Cohn, JF			IEEE	Pillai, Rohith Krishnan; Jeni, Laszlo A.; Yang, Huiyuan; Zhang, Zheng; Yin, Lijun; Cohn, Jeffrey			The 2nd 3D Face Alignment in the Wild Challenge (3DFAW-Video): Dense Reconstruction From Video	2019 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW)	IEEE International Conference on Computer Vision Workshops		English	Proceedings Paper	IEEE/CVF International Conference on Computer Vision (ICCV)	OCT 27-NOV 02, 2019	Seoul, SOUTH KOREA	IEEE, IEEE Comp Soc, CVF				3D face alignment approaches have strong advantages over 2D with respect to representational power and robustness to illumination and pose. Over the past few years a number of research groups have made rapid advances in dense 3D alignment from 2D video and obtained impressive results. How these various methods compare is relatively unknown. Previous benchmarks addressed sparse 3D alignment and single image 3D reconstruction. No commonly accepted evaluation protocol exists for dense 3D face reconstruction from video with which to compare them. The 2nd 3D Face Alignment in the Wild from Videos (3DFAW-Video) Challenge extends the previous 3DFAW 2016 competition to the estimation of dense 3D facial structure from video. It presented a new large corpora of profile-to-profile face videos recorded under different imaging conditions and annotated with corresponding high-resolution 3D ground truth meshes. In this paper we outline the evaluation protocol, the data used, and the results. 3DFAW-Video is to be held in conjunction with the 2019 International Conference on Computer Vision, in Seoul, Korea.	[Pillai, Rohith Krishnan; Jeni, Laszlo A.] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Yang, Huiyuan; Zhang, Zheng; Yin, Lijun] SUNY Binghamton, Dept Comp Sci, Binghamton, NY 13902 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Pillai, RK (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.	rohithkp@cmu.edu; laszlojeni@cmu.edu; hyang51@binghamton.edu; zzhang@binghamton.edu; lijun@cs.binghamton.edu; jeffcohn@pitt.edu	Yang, Huiyuan/AAX-5521-2021	Yang, Huiyuan/0000-0003-0517-5187	US Department of Defense [W911SR-17-C0060]; NSF [CNS-1629716, CNS-1629898]	US Department of Defense(United States Department of Defense); NSF(National Science Foundation (NSF))	This work was supported in part by US Department of Defense grant W911SR-17-C0060, and NSF grants CNS-1629716 and CNS-1629898. We also thank Yu Deng and Jiaolong Yang from Microsoft Research Asia for providing the evaluation code and protocol for use in the challenge.	Bagdanov A. D., 2011, P 2011 JOINT ACM WOR, P7980; Cao C., 2013, IEEE T VISUALIZATION, V20, P413; Chrysos GG, 2017, IEEE COMPUT SOC CONF, P2015, DOI 10.1109/CVPRW.2017.252; Cignoni Paolo, 2008, EUR IT CHAPT C, P129; Deng Y, 2019, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0400-9; Feng Y., 2018, P EUR C COMP VIS ECC, P534; Feng ZH, 2018, IEEE INT CONF AUTOMA, P780, DOI 10.1109/FG.2018.00123; Gecer B, 2019, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2019.00125; Gerig T, 2018, IEEE INT CONF AUTOMA, P75, DOI 10.1109/FG.2018.00021; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Jeni LA, 2016, LECT NOTES COMPUT SC, V9914, P511, DOI 10.1007/978-3-319-48881-3_35; Kazhdan M., 2013, ACM T GRAPHIC, V32, P29; King DE, 2009, J MACH LEARN RES, V10, P1755; Koppen P, 2018, PATTERN RECOGN, V74, P617, DOI 10.1016/j.patcog.2017.09.006; Li T, 2017, CHIN J CANCER, V36, DOI 10.1186/s40880-016-0172-5; Liu Z., 2018, LARGE SCALE CELEBFAC, V15, P2018; Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767; Maldonado E. R., 2019, P IEEE INT C COMP VI; Mortazavian P., 2016, VISIGRAPP; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Sanyal S, 2019, PROC CVPR IEEE, P7755, DOI 10.1109/CVPR.2019.00795; Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6; Shao X.-H., 2019, P IEEE INT C COMP VI; Tewari A, 2017, IEEE INT CONF COMP V, P1274, DOI 10.1109/ICCVW.2017.153; Tran A. Tuan, 2017, P IEEE C COMP VIS PA, P5163; Wu FZ, 2019, PROC CVPR IEEE, P959, DOI 10.1109/CVPR.2019.00105; Yi H., 2019, P IEEE C COMP VIS PA, P7663; Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211; Zafeiriou S, 2017, IEEE INT CONF COMP V, P2503, DOI 10.1109/ICCVW.2017.16; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhao DP, 2013, INT J ADV ROBOT SYST, V10, DOI 10.5772/56759; Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23	33	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	2473-9936		978-1-7281-5023-9	IEEE INT CONF COMP V			2019							3082	3089		10.1109/ICCVW.2019.00371	http://dx.doi.org/10.1109/ICCVW.2019.00371			8	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Imaging Science & Photographic Technology	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Imaging Science & Photographic Technology	BP4UH					2022-10-03	WOS:000554591603021
C	Schulc, A; Cohn, JF; Shen, J; Pantic, M			IEEE	Schulc, Attila; Cohn, Jeffrey; Shen, Jie; Pantic, Maja			Automatic Measurement of Visual Attention to Video Content using Deep Learning	PROCEEDINGS OF MVA 2019 16TH INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA)			English	Proceedings Paper	16th International Conference on Machine Vision Applications (MVA)	MAY 27-31, 2019	Tokyo, JAPAN	MVA Org, IEICE ISS TG Pattern Recognit & Media Understanding, Informat Proc Soc Japan SIG Comp Vis & Image Media, IEEE Robot & Automat Soc, Informat Proc Soc Japan, Inst Elect Engineers Japan, Inst Elect Informat & Commun Engineers, Inst Image Informat & Televis Engineers, Inst Syst Control & Informat Engineers, Inst Image Elect Engineers Japan, Japanese Soc Artificial Intelligence, Japanese Soc Non Destruct Inspect, Japan Soc Precis Engn, Robot Soc Japan, Soc Acad Study Sensing Image Informat, Soc Automot Engineers Japan, Soc Instrument & Control Engineers, Virtual Real Soc Japan				Advances in automated face analysis have made possible webcam-based assessment of viewer emotion during presentation of commercials and other video content. A key assumption of this technology is that viewer emotion is in response to the media. Is that assumption warranted? Because viewer attention is seldom assessed, emotional responses could result from other sources, such as talking to a friend, enjoying a meal, or attending to a pet. We developed a CNN-LSTM approach that detects attention and non-attention to commercials using webcam and mobile devices in settings of viewer's choice. Because cultural variation in viewer response is likely, we included participants from both Western and Eastern countries. Participants were 28,911 adults (ages 18 to 69 years) in Europe, USA, Russia, and China. A total of 15,543 sessions (ca. 6.5 million video frames) was analyzed. Accuracy was quantified using a variety of metrics. Our approach outperformed baseline and achieved moderate to high accuracy that approached that of human annotators.	[Schulc, Attila; Cohn, Jeffrey; Pantic, Maja] Realeyes, Budapest, Hungary; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Shen, Jie; Pantic, Maja] Imperial Coll London, Dept Comp, London, England; [Shen, Jie; Pantic, Maja] Samsung AI Ctr, Cambridge, England		Schulc, A (corresponding author), Realeyes, Budapest, Hungary.	attila.schulc@realeyesit.com; jeffcohn@pitt.edu; jie.shen07@imperial.ac.uk; m.pantic@imperial.ac.uk						Asteriadis S., 2011, Proceedings of the 2011 3rd International Conference on Games and Virtual Worlds for Serious Applications (VS-GAMES 2011), P186, DOI 10.1109/VS-GAMES.2011.38; Ba S. O., 2006, Multimodal Technologies for Perception of Humans. First International Evaluation Workshop on Classification of Events, Activities and Relationships, CLEAR 2006. Revised Selected Papers (Lecture Notes in Computer Science Vol.4122), P345; Bennett FM, 1954, PUBLIC OPIN QUART, V18, P303; Bergasa LM, 2008, PROCEEDINGS OF THE 11TH INTERNATIONAL IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, P1149, DOI 10.1109/ITSC.2008.4732544; BRENNAN RL, 1981, EDUC PSYCHOL MEAS, V41, P687, DOI 10.1177/001316448104100307; Campbell K, 2018, AUTISM; Chang C, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P616, DOI 10.1145/3242969.3264986; Dhall A, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P653; Fridman L, 2016, IET COMPUT VIS, V10, P308, DOI 10.1049/iet-cvi.2015.0296; Gupta A., 2016, CORR; Hassner Tal, 2015, IEEE C COMP VIS PATT; Hernandez J, 2013, 2013 10TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING, COMPUTING SCIENCE AND AUTOMATIC CONTROL (CCE), P13, DOI 10.1109/ICEEE.2013.6676009; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Jeni LA, 2013, INT CONF AFFECT, P245, DOI 10.1109/ACII.2013.47; KING WM, 1976, J NEUROPHYSIOL, V39, P1135, DOI 10.1152/jn.1976.39.6.1135; Klier EM, 2003, J NEUROPHYSIOL, V89, P2839, DOI 10.1152/jn.00763.2002; Ko BC, 2018, SENSORS; Li H, 2015, HCOMP; Li Y, 2017, WWW; Lyu J, 2018, CORR; Masse  Benoit, 2017, IEEE T PATTERN ANAL; McDuff D, 2013, P ESOMAR C, V1, P2; Mustafa A, 2018, CORR; Ngxande M, 2017, 2017 PATTERN RECOGNITION ASSOCIATION OF SOUTH AFRICA AND ROBOTICS AND MECHATRONICS (PRASA-ROBMECH), P156, DOI 10.1109/RoboMech.2017.8261140; Palinko O, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P5048, DOI 10.1109/IROS.2016.7759741; Robal T, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P189, DOI 10.1145/3172944.3172987; Rosenthal R, 2005, NEW HDB METHODS NONV, P199; Sheikhi Samira, 2012, Human Behavior Understanding. Proceedings of the Third International Workshop, HBU 2012, P99, DOI 10.1007/978-3-642-34014-7_9; Szirtes G, 2017, IMAGE VISION COMPUT, V65, P49, DOI 10.1016/j.imavis.2017.03.002; Waibel A., 1999, INT C ADV VIS INF SY, P765; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75	31	0	0	0	2	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-4-901122-18-4				2019														6	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering	BN5QA					2022-10-03	WOS:000484550200062
C	Yang, L; Ertugrul, IO; Cohn, JF; Hammal, Z; Jiang, DM; Sahli, H			IEEE	Yang, Le; Ertugrul, Itir Onal; Cohn, Jeffrey; Hammal, Zakia; Jiang, Dongmei; Sahli, Hichem			FACS3D-Net: 3D Convolution based Spatiotemporal Representation for Action Unit Detection	2019 8TH INTERNATIONAL CONFERENCE ON AFFECTIVE COMPUTING AND INTELLIGENT INTERACTION (ACII)	International Conference on Affective Computing and Intelligent Interaction		English	Proceedings Paper	8th International Conference on Affective Computing and Intelligent Interaction (ACII)	SEP 03-06, 2019	Cambridge, ENGLAND			CNN; CNN-LSTM; FACS3D-Net; spatiotemporal information; multi-label AU detection		Most approaches to automatic facial action unit (AU) detection consider only spatial information and ignore AU dynamics. For humans, dynamics improves AU perception. Is same true for algorithms? To make use of AU dynamics, recent work in automated AU detection has proposed a sequential spatiotemporal approach: Model spatial information using a 2D CNN and then model temporal information using LSTM (Long-Short-Term Memory). Inspired by the experience of human FACS coders, we hypothesized that combining spatial and temporal information simultaneously would yield more powerful AU detection. To achieve this, we propose FACS3D-Net that simultaneously integrates 3D and 2D CNN. Evaluation was on the Expanded BP4D+ database of 200 participants. FACS3D-Net outperformed both 2D CNN and 2D CNN-LSTM approaches. Visualizations of learnt representations suggest that FACS3D-Net is consistent with the spatiotemporal dynamics attended to by human FACS coders. To the best of our knowledge, this is the first work to apply 3D CNN to the problem of AU detection.	[Yang, Le; Jiang, Dongmei] Northwestern Polytech Univ, Sch Comp Sci, Xian, Peoples R China; [Ertugrul, Itir Onal; Hammal, Zakia] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Jiang, Dongmei] Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China; [Sahli, Hichem] Vrije Univ Brussel, Dept Elect & Informat, Brussels, Belgium; [Sahli, Hichem] Interuniv Microelect Ctr, Heverlee, Belgium		Yang, L (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian, Peoples R China.	yangle.cst@gmail.com			NIH [NS100549, MH096951]; NSF [CNS-1629716]; Shaanxi Provincial International Science and Technology Collaboration Project [2017KW-ZD-14]; VUB Interdisciplinary Research Program through the EMO-App project	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); Shaanxi Provincial International Science and Technology Collaboration Project; VUB Interdisciplinary Research Program through the EMO-App project	This research was supported in part by NIH awards NS100549 and MH096951, NSF award CNS-1629716, the Shaanxi Provincial International Science and Technology Collaboration Project (grant 2017KW-ZD-14), and the VUB Interdisciplinary Research Program through the EMO-App project.	Ambadar Z, 2005, PSYCHOL SCI, V16, P403, DOI 10.1111/j.0956-7976.2005.01548.x; Byeon YH, 2014, INT J ADV COMPUT SC, V5, P107, DOI 10.14569/ijacsa.2014.051215; Carreira J., 2017, P IEEE C COMP VIS PA, P6299; Chu WS, 2019, IMAGE VISION COMPUT, V81, P1, DOI 10.1016/j.imavis.2018.10.002; Chu WS, 2013, PROC CVPR IEEE, P3515, DOI 10.1109/CVPR.2013.451; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Ertugrul Itir Onal, 2019, 2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019), DOI 10.1109/FG.2019.8756623; Ertugrul I. Onal, 2019, 2019 14 IEEE INT C A; Ertugrul IO, 2018, IEEE COMPUT SOC CONF, P2211, DOI 10.1109/CVPRW.2018.00287; Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632; Friesen E., 1978, PALO ALTO, V3; Girard JM, 2017, IEEE INT CONF AUTOMA, P581, DOI 10.1109/FG.2017.144; Gonzalez I, 2015, MULTIMED TOOLS APPL, V74, P10001, DOI 10.1007/s11042-014-2320-8; He J, 2017, IEEE INT CONF AUTOMA, P848, DOI 10.1109/FG.2017.108; Jaiswal Shashank, 2016, WACV; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Jeni LA, 2013, INT CONF AFFECT, P245, DOI 10.1109/ACII.2013.47; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jing LL, 2018, J VIS COMMUN IMAGE R, V52, P58, DOI 10.1016/j.jvcir.2018.01.016; Li W, 2017, COMPUT INTEL NEUROSC, V2017, DOI 10.1155/2017/9471841; Li W, 2017, IEEE INT CONF AUTOMA, P103, DOI 10.1109/FG.2017.136; Shao Z., 2018, P EUR C COMP VIS ECC, P705; Valstar MF, 2007, LECT NOTES COMPUT SC, V4796, P118; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang Z, 2016, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2016.374; Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369	26	2	2	0	2	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2156-8103		978-1-7281-3888-6	INT CONF AFFECT			2019														7	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Engineering, Electrical & Electronic	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering	BO6TZ					2022-10-03	WOS:000522220800093
J	Cohn, JF				Cohn, Jeffrey			Advanced serious illness, multimorbidity, and multibeneficence: The role of communication	JOURNAL OF EVALUATION IN CLINICAL PRACTICE			English	Article								Sturmberg et al write about multimorbidity as "several diagnosable diseases within the same individual." They posit that this syndrome is the result of multiple interconnected disturbances reflecting scale-free, fractal signs of pathology ranging from biochemical/hormonal alterations at one end of a spectrum to community and societal ills at the other. In this commentary, I will be focusing on 3 perspectives: 1) a preterminal phase of multimorbidity that is indicative of that loss of reparative or even homeokinetic properties, known by some as "advanced serious illness"; 2) the manifestations of advanced serious illness multimorbidity that, using the same networks that connect into the patient, are signs of this syndrome at the levels of the immediate family/friend social network, the broader community, and society at large; and 3) the potential for these same networks that transmit pathological forces to convey the positive effects of therapeutic interventions in a scale-free manner, with a focus on how conversation can lead to what I'm calling "multibeneficence."	[Cohn, Jeffrey] 1227 N 4th St, Philadelphia, PA 19122 USA		Cohn, JF (corresponding author), 1227 N 4th St, Philadelphia, PA 19122 USA.	jeff@commonpractice.com						Allen JY, 2013, J PALLIAT MED, V16, P745, DOI 10.1089/jpm.2012.0450; Cerulo M., 2015, TOPOLOGIK, V17, P79; Christakis NA, 2003, SOC SCI MED, V57, P465, DOI 10.1016/S0277-9536(02)00370-2; Coalition to Transform Advanced Care, WHAT WE MEAN WE SAY; Dy Sydney M, 2012, Evid Rep Technol Assess (Full Rep), P1; Hawthorn M., 2015, WORKING RELATIONAL D; Hawthorn Maureen, 2015, Br J Nurs, V24, P702, DOI 10.12968/bjon.2015.24.13.702; IOM, 2015, WORK REL DEPTH COUNS, P138; Mearns D., 2015, WORKING RELATIONAL D; Pauley PM, 2015, HEALTH COMMUN, V30, P646, DOI 10.1080/10410236.2014.888385; Sturmberg JP, 2017, J EVAL CLIN PRACT, V23, P199, DOI 10.1111/jep.12587; Vig EK, 2007, J GEN INTERN MED, V22, P1274, DOI 10.1007/s11606-007-0252-y	12	3	3	0	2	WILEY	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA	1356-1294	1365-2753		J EVAL CLIN PRACT	J. Eval. Clin. Pract.	DEC	2018	24	6			SI		1279	1281		10.1111/jep.12706	http://dx.doi.org/10.1111/jep.12706			3	Health Care Sciences & Services; Medical Informatics; Medicine, General & Internal	Science Citation Index Expanded (SCI-EXPANDED)	Health Care Sciences & Services; Medical Informatics; General & Internal Medicine	HA1SL	28205360				2022-10-03	WOS:000450002400001
J	Escalera, S; Baro, X; Guyon, I; Escalante, HJ; Tzimiropoulos, G; Valstar, M; Pantic, M; Cohn, JF; Kanade, T				Escalera Guerrero, Sergio; Baro, Xavier; Guyon, Isabelle; Escalante, Hugo Jair; Tzimiropoulos, Georgios; Valstar, Michel F.; Pantic, Maja; Cohn, Jeffrey; Kanade, Takeo			Guest Editorial: The Computational Face	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									[Escalera Guerrero, Sergio] Univ Barcelona, Dept Math & Informat, E-08007 Barcelona, Spain; [Escalera Guerrero, Sergio] Comp Vis Ctr, Barcelona 08007, Spain; [Baro, Xavier] Univ Oberta Catalunya, Barcelona 08018, Spain; [Baro, Xavier] Univ Oberta Catalunya, Comp Sci Multimedia & Telecommun Dept, Barcelona 08018, Spain; [Baro, Xavier] Comp Vis Ctr, Barcelona 08018, Spain; [Guyon, Isabelle] Univ Paris Saclay, INRIA, UPSud, Big Data, F-91190 St Aubin, France; [Guyon, Isabelle] ChaLearn, Berkeley, CA 94708 USA; [Escalante, Hugo Jair] Inst Nacl Astrofis Opt & Eect, Puebla 72840, Mexico; [Tzimiropoulos, Georgios; Valstar, Michel F.] Univ Nottingham, Sch Comp Sci, Nottingham NG7 2RD, England; [Pantic, Maja] Imperial Coll, Affect & Behav Comp, London SW7 2AZ, England; [Cohn, Jeffrey] Univ Pittsburgh, Psychol & Psychiat, Pittsburgh, PA 15260 USA; [Kanade, Takeo] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA		Escalera, S (corresponding author), Univ Barcelona, Dept Math & Informat, E-08007 Barcelona, Spain.; Escalera, S (corresponding author), Comp Vis Ctr, Barcelona 08007, Spain.	sergio@maia.ub.es; xbaro@uoc.edu; guyon@chalearn.org; hugojair@inaoep.mx; yorgos.tzimiropoulos@nottingham.ac.uk; michel.valstar@nottingham.ac.uk; m.pantic@imperial.ac.uk; jeffcohn@cs.cmu.edu; Takeo.Kanade@cs.cmu.edu	Baró, Xavier/A-4064-2011; Escalera Guerrero, Sergio/L-2998-2015	Baró, Xavier/0000-0001-5338-3007; Escalera Guerrero, Sergio/0000-0003-0617-8873; Tzimiropoulos, Georgios/0000-0002-1803-5338; Guyon, Isabelle/0000-0002-9266-1783; Valstar, Michel F./0000-0003-2414-161X	MINECO/FEDER, UE [TIN2015-66951-C2-2-R, TIN2016-74946-P]; CERCA Programme/Generalitat de Catalunya; INAOE, CONACyT-Mexico [241306]; ChaLearn Looking at People; Microsoft Research; Google; NVIDIA Coorporation; Amazon; Facebook; Disney Research	MINECO/FEDER, UE(Spanish Government); CERCA Programme/Generalitat de Catalunya; INAOE, CONACyT-Mexico; ChaLearn Looking at People; Microsoft Research(Microsoft); Google(Google Incorporated); NVIDIA Coorporation; Amazon; Facebook(Facebook Inc); Disney Research	This project has been partially supported by the Spanish projects TIN2015-66951-C2-2-R and TIN2016-74946-P (MINECO/FEDER, UE) and CERCA Programme/Generalitat de Catalunya and by INAOE, CONACyT-Mexico under grant 241306. We thank ChaLearn Looking at People sponsors for their support, including Microsoft Research, Google, NVIDIA Coorporation, Amazon, Facebook, and Disney Research.	Booth J, 2018, IEEE T PATTERN ANAL, V40, P2638, DOI 10.1109/TPAMI.2018.2832138; Chrysos G. G., 2018, IEEE T PATTERN ANAL; Escalera S, 2017, IEEE IJCNN, P1594, DOI 10.1109/IJCNN.2017.7966041; Escalera S, 2016, IEEE COMPUT SOC CONF, P706, DOI 10.1109/CVPRW.2016.93; Escalera S, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P243, DOI 10.1109/ICCVW.2015.40; Han H, 2018, IEEE T PATTERN ANAL, V40, P2597, DOI 10.1109/TPAMI.2017.2738004; Kononenko D, 2018, IEEE T PATTERN ANAL, V40, P2696, DOI 10.1109/TPAMI.2017.2737423; Li W., 2018, IEEE T PATTERN ANAL, DOI [10.11069/TPAMI.2018.2791608, DOI 10.11069/TPAMI.2018.2791608]; Liu H, 2018, IEEE T PATTERN ANAL, V40, P2546, DOI 10.1109/TPAMI.2017.2734779; Masse B., 2018, IEEE T PATTERN ANAL, DOI [10.1109/TPAMI.2017.2785819, DOI 10.1109/TPAMI.2017.2785819]; Robinson J. P., 2018, IEEE T PATTERN ANAL, DOI [10.1109/TPAMI.2018.2826849, DOI 10.1109/TPAMI.2018.2826849]; Sagonas C., 2018, IEEE T PATTERN ANAL, DOI [10.11069/TPAMI.2017.2784421, DOI 10.11069/TPAMI.2017.2784421]; Tan Z., 2018, IEEE T PATTERN ANAL, DOI [10.1109/TPAMI.2017.2738004, DOI 10.1109/TPAMI.2017.2738004]; Wang MJ, 2018, IEEE T PATTERN ANAL, V40, P2682, DOI 10.1109/TPAMI.2017.2783940; Wang W., 2018, IEEE T PATTERN ANAL, DOI [10.1106/TPAMI.2018.2810881, DOI 10.1106/TPAMI.2018.2810881]; Yu Y, 2018, IEEE T PATTERN ANAL, V40, P2653, DOI 10.1109/TPAMI.2018.2841403	16	2	2	1	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2018	40	11					2541	2545		10.1109/TPAMI.2018.2869610	http://dx.doi.org/10.1109/TPAMI.2018.2869610			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GW2AF		Green Submitted, Bronze			2022-10-03	WOS:000446683700001
J	Tulyakov, S; Jeni, LA; Cohn, JF; Sebe, N				Tulyakov, Sergey; Jeni, Laszlo A.; Cohn, Jeffrey; Sebe, Nicu			Viewpoint-Consistent 3D Face Alignment	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Face alignment; 3D face shape; morphable model	ACTIVE APPEARANCE MODELS; HEAD TRACKING; REGRESSION; DATABASE	Most approaches to face alignment treat the face as a 2D object, which fails to represent depth variation and is vulnerable to loss of shape consistency when the face rotates along a 3D axis. Because faces commonly rotate three dimensionally, 2D approaches are vulnerable to significant error. 3D morphable models, employed as a second step in 2D+3D approaches are robust to face rotation but are computationally too expensive for many applications, yet their ability to maintain viewpoint consistency is unknown. We present an alternative approach that estimates 3D face landmarks in a single face image. The method uses a regression forest-based algorithm that adds a third dimension to the common cascade pipeline. 3D face landmarks are estimated directly, which avoids fitting a 3D morphable model. The proposed method achieves viewpoint consistency in a computationally efficient manner that is robust to 3D face rotation. To train and test our approach, we introduce the Multi-PIE Viewpoint Consistent database. In empirical tests, the proposed method achieved simple yet effective head pose estimation and viewpoint consistency on multiple measures relative to alternative approaches.	[Tulyakov, Sergey] Snapchat Res, Venice, CA 90291 USA; [Jeni, Laszlo A.; Cohn, Jeffrey] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Jeni, Laszlo A.; Cohn, Jeffrey] Univ Pittsburgh, Affect Anal Grp, Pittsburgh, PA 15260 USA; [Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, I-38122 Trento, TN, Italy		Tulyakov, S (corresponding author), Snapchat Res, Venice, CA 90291 USA.	sergey.tulyakov@unitn.it; laszlojeni@cmu.edu; jeffcohn@cs.cmu.edu; niculae.sebe@unitn.it		Jeni, Laszlo A./0000-0002-2830-700X; Sebe, Niculae/0000-0002-6597-7248				An KH, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P307, DOI 10.1109/IROS.2008.4650742; Ariz M, 2016, COMPUT VIS IMAGE UND, V148, P201, DOI 10.1016/j.cviu.2015.04.009; Asteriadis S., 2009, P INT WORKSH AFF AW, P1, DOI 10.1145/1655260.1655261; Baltrusaitis T, 2012, PROC CVPR IEEE, P2610, DOI 10.1109/CVPR.2012.6247980; Belhumeur PN, 2011, PROC CVPR IEEE, P545, DOI 10.1109/CVPR.2011.5995602; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Bulat A, 2016, LECT NOTES COMPUT SC, V9914, P616, DOI 10.1007/978-3-319-48881-3_43; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204; Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249; Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012; Cao XD, 2012, PROC CVPR IEEE, P2887, DOI 10.1109/CVPR.2012.6248015; Cootes T., 1992, P BRIT MACH VIS C, P266, DOI DOI 10.1007/978-1-4471-3201-1_28; Cootes T. F, 1993, PROC BRIT MACH VIS C, P639; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Zavan FHD, 2016, LECT NOTES COMPUT SC, V9914, P581, DOI 10.1007/978-3-319-48881-3_40; Fanelli G, 2013, IEEE INT CONF AUTOMA; Gou C, 2016, LECT NOTES COMPUT SC, V9914, P604, DOI 10.1007/978-3-319-48881-3_42; Gross R, 2005, IMAGE VISION COMPUT, V23, P1080, DOI 10.1016/j.imavis.2005.07.009; Gross R., 2008, P IEEE INT C AUT FAC, V1, P1; Gupta SD, 2010, METHODS MOL BIOL, V589, P97, DOI [10.1007/978-1-60327-114-1_10, 10.1109/SSIAI.2010.5483908]; Hassner T, 2013, IEEE I CONF COMP VIS, P3607, DOI 10.1109/ICCV.2013.448; Hsieh PL, 2015, PROC CVPR IEEE, P1675, DOI 10.1109/CVPR.2015.7298776; Huang C, 2012, COMPUT VIS IMAGE UND, V116, P777, DOI 10.1016/j.cviu.2012.02.007; Huang G.B., 2007, TECHNICAL REPORT; Jeni Laszlo A., 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163142; Jeni LA, 2016, LECT NOTES COMPUT SC, V9914, P511, DOI 10.1007/978-3-319-48881-3_35; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Jesorsky O, 2001, LECT NOTES COMPUT SC, V2091, P90; Jourabloo A, 2015, IEEE I CONF COMP VIS, P3694, DOI 10.1109/ICCV.2015.421; KAZEMI V, 2014, PROC CVPR IEEE, P1867, DOI DOI 10.1109/CVPR.2014.241; King DE, 2009, J MACH LEARN RES, V10, P1755; Koestinger M., 2011, P IEEE INT C COMP VI, P2144, DOI DOI 10.1109/ICCVW.2011.6130513; Kumano S, 2009, INT J COMPUT VISION, V83, P178, DOI 10.1007/s11263-008-0185-x; La Cascia M, 2000, IEEE T PATTERN ANAL, V22, P322, DOI 10.1109/34.845375; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Li H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508407; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; MESSER K, 1999, AUDIO VIDEO BASED BI, P72; Milborrow S., 2010, MUCT LAND MARKED FAC; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218; Roth J, 2016, PROC CVPR IEEE, P4197, DOI 10.1109/CVPR.2016.455; Roth J, 2015, PROC CVPR IEEE, P2606, DOI 10.1109/CVPR.2015.7298876; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Sangineto E, 2013, IEEE T PATTERN ANAL, V35, P624, DOI 10.1109/TPAMI.2012.87; Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4; Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6; Sung J, 2008, INT J COMPUT VISION, V80, P260, DOI 10.1007/s11263-007-0125-1; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Thies J., 2016, P CVPR, DOI DOI 10.1109/CVPR.2016.262; Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453; TULYAKOV S, 2016, PROC CVPR IEEE, P2396, DOI DOI 10.1109/CVPR.2016.263; Tulyakov S, 2015, IEEE I CONF COMP VIS, P3748, DOI 10.1109/ICCV.2015.427; Tulyakov S, 2014, INT C PATT RECOG, P2263, DOI 10.1109/ICPR.2014.393; Tzimiropoulos G, 2015, PROC CVPR IEEE, P3659, DOI 10.1109/CVPR.2015.7298989; Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79; Valenti R, 2012, IEEE T IMAGE PROCESS, V21, P802, DOI 10.1109/TIP.2011.2162740; Vicente F, 2015, IEEE T INTELL TRANSP, V16, P2014, DOI 10.1109/TITS.2015.2396031; Wang N., 2014, ARXIV14101037; Weise T., 2011, P ACM SIGGRAPH 2011; Weise Thibaut, 2009, P 2009 ACM SIGGRAPH; Xiao J, 2003, INT J IMAG SYST TECH, V13, P85, DOI 10.1002/ima.10048; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Xiong XH, 2015, PROC CVPR IEEE, P2664, DOI 10.1109/CVPR.2015.7298882; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yang H, 2013, IEEE I CONF COMP VIS, P1936, DOI 10.1109/ICCV.2013.243; Yi D, 2013, PROC CVPR IEEE, P3539, DOI 10.1109/CVPR.2013.454; Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211; Yin LJ, 2008, IEEE INT CONF AUTOMA, P116; Yu X, 2013, IEEE I CONF COMP VIS, P1944, DOI 10.1109/ICCV.2013.244; Zhang J, 2016, PROC CVPR IEEE, P3428, DOI 10.1109/CVPR.2016.373; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhao RQ, 2016, LECT NOTES COMPUT SC, V9914, P590, DOI 10.1007/978-3-319-48881-3_41; Zhou SK, 2007, LECT NOTES COMPUT SC, V4584, P13; Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014; Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23	79	11	11	0	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2018	40	9					2250	2264		10.1109/TPAMI.2017.2750687	http://dx.doi.org/10.1109/TPAMI.2017.2750687			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GP4UX	28910758				2022-10-03	WOS:000440868400016
J	Hammal, Z; Cohn, JF; Wallace, ER; Heike, CL; Birgfeld, CB; Oster, H; Speltz, ML				Hammal, Zakia; Cohn, Jeffrey; Wallace, Erin R.; Heike, Carrie L.; Birgfeld, Craig B.; Oster, Harriet; Speltz, Matthew L.			Facial Expressiveness in Infants With and Without Craniofacial Microsomia: Preliminary Findings	CLEFT PALATE-CRANIOFACIAL JOURNAL			English	Article						craniofacial microsomia; facial expressiveness; AUs; infants	HEMIFACIAL MICROSOMIA; PSYCHOSOCIAL ADJUSTMENT; EMOTION REGULATION; CLINICAL-FEATURES; EUROPEAN-AMERICAN; CHINESE INFANTS; CLASSIFICATION; EXPRESSIONS; ANOMALIES; CHILDREN	Objective: To compare facial expressiveness (FE) of infants with and without craniofacial macrosomia (cases and controls, respectively) and to compare phenotypic variation among cases in relation to FE. Design: Positive and negative affect was elicited in response to standardized emotion inductions, video recorded, and manually coded from video using the Facial Action Coding System for Infants and Young Children. Setting: Five craniofacial centers: Children's Hospital of Los Angeles, Children's Hospital of Philadelphia, Seattle Children's Hospital, University of Illinois-Chicago, and University of North Carolina-Chapel Hill. Participants: Eighty ethnically diverse 12- to 14-month-old infants. Main Outcome Measures: FE was measured on a frame-by-frame basis as the sum of 9 observed facial action units (AUs) representative of positive and negative affect. Results: FE differed between conditions intended to elicit positive and negative affect (95% confidence interval = 0.09-0.66, P = .01). FE failed to differ between cases and controls (ES = -0.16 to -0.02, P = .47 to .92). Among cases, those with and without mandibular hypoplasia showed similar levels of FE (ES = -0.38 to 0.54, P = .10 to .66). Conclusions: FE varied between positive and negative affect, and cases and controls responded similarly. Null findings for case/control differences may be attributable to a lower than anticipated prevalence of nerve palsy among cases, the selection of AUs, or the use of manual coding. In future research, we will reexamine group differences using an automated, computer vision approach that can cover a broader range of facial movements and their dynamics.	[Hammal, Zakia; Cohn, Jeffrey] Carnegie Mellon Univ, Robot Inst, 5000 Forbes Ave, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Wallace, Erin R.; Heike, Carrie L.; Birgfeld, Craig B.; Speltz, Matthew L.] Seattle Childrens Res Inst, Seattle, WA USA; [Heike, Carrie L.; Birgfeld, Craig B.] Seattle Childrens Hosp, Seattle, WA USA; [Heike, Carrie L.; Birgfeld, Craig B.; Speltz, Matthew L.] Univ Washington, Sch Med, Seattle, WA USA; [Oster, Harriet] NYU, Sch Profess Studies, New York, NY USA		Hammal, Z (corresponding author), Carnegie Mellon Univ, Robot Inst, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.	zakia_hammal@yahoo.fr	Heike, Carrie L./ABA-4852-2021	Heike, Carrie L./0000-0003-2178-7730	Center for Clinical and Translational Research at Seattle Children's Research Institute [UL1 TR000423]; National Institute of Dental and Craniofacial Research [R01 DE022438]; NATIONAL CENTER FOR ADVANCING TRANSLATIONAL SCIENCES [UL1TR000423, UL1TR002319] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF DENTAL & CRANIOFACIAL RESEARCH [R03DE026513, R01DE022438] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH096951] Funding Source: NIH RePORTER	Center for Clinical and Translational Research at Seattle Children's Research Institute; National Institute of Dental and Craniofacial Research(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Dental & Craniofacial Research (NIDCR)); NATIONAL CENTER FOR ADVANCING TRANSLATIONAL SCIENCES(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Advancing Translational Sciences (NCATS)); NATIONAL INSTITUTE OF DENTAL & CRANIOFACIAL RESEARCH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Dental & Craniofacial Research (NIDCR)); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))	The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This study was supported by the Center for Clinical and Translational Research at Seattle Children's Research Institute (grant UL1 TR000423) and the National Institute of Dental and Craniofacial Research (grant R01 DE022438).	Barisic I, 2014, EUR J HUM GENET, V22, P1026, DOI 10.1038/ejhg.2013.287; Birgfeld CB, 2016, HEAD FACE MED, V12, DOI 10.1186/s13005-016-0109-x; BRENNAN RL, 1981, EDUC PSYCHOL MEAS, V41, P687, DOI 10.1177/001316448104100307; Campos JJ, 2004, CHILD DEV, V75, P377, DOI 10.1111/j.1467-8624.2004.00681.x; Camras LA, 2003, ANN NY ACAD SCI, V1000, P135, DOI 10.1196/annals.1280.007; Camras LA, 1998, DEV PSYCHOL, V34, P616, DOI 10.1037/0012-1649.34.4.616; Cline JM, 2014, OTOLARYNG HEAD NECK, V150, P188, DOI 10.1177/0194599813512775; Cohen J., 1988, STAT POWER ANAL BEHA, P400; Cohen N, 2017, AM J MED GENET A, V173, P1208, DOI 10.1002/ajmg.a.38151; Cohn J. F., 2015, OXFORD HDB AFFECTIVE, DOI DOI 10.1093/OXFORDHB/9780199942237.001.0001; Cole PM, 2004, CHILD DEV, V75, P317, DOI 10.1111/j.1467-8624.2004.00673.x; COUSLEY RRJ, 1993, BRIT J ORAL MAX SURG, V31, P78, DOI 10.1016/0266-4356(93)90165-S; Dibeklioglu H, 2017, IEEE J BIOMED HEALTH, DOI [10.1109/JBHI.2017.2676878, DOI 10.1109/JBHI.2017.2676878.]; Dibeklioglu H, 2015, P ACM INT C MULT INT; Dinehart LHB, 2005, INFANCY, V8, P279, DOI 10.1207/s15327078in0803_5; Dufton LM, 2011, J PEDIATR PSYCHOL, V36, P794, DOI 10.1093/jpepsy/jsq112; Ekman P, 2002, RES NEXUS; Goldsmith H. H., 1999, LAB TEMPERAMENT ASSE; Gougoutas AJ, 2007, PLAST RECONSTR SURG, V120, p112E, DOI 10.1097/01.prs.0000287383.35963.5e; Harris J, 1996, J MED GENET, V33, P809, DOI 10.1136/jmg.33.10.809; Heike CL, 2016, BIRTH DEFECTS RES A, V106, P915, DOI 10.1002/bdra.23560; Heike CL, 2011, HEAD FACE MED, V7, DOI 10.1186/1746-160X-7-25; HORGAN JE, 1995, CLEFT PALATE-CRAN J, V32, P405, DOI 10.1597/1545-1569(1995)032<0405:OPAOCA>2.3.CO;2; Maris CL, 1999, CLEFT PALATE-CRAN J, V36, P43, DOI 10.1597/1545-1569(1999)036<0043:PAITPW>2.3.CO;2; MATIAS R, 1993, DEV PSYCHOL, V29, P524, DOI 10.1037/0012-1649.29.3.524; Mattson WI, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0080161; Messinger DS, 2012, EMOTION, P12; Oster H, 2003, ANN NY ACAD SCI, V1000, P197, DOI 10.1196/annals.1280.024; Oster H., 2005, EMOTIONAL DEV, P261; PADWA BL, 1991, CLEFT PALATE-CRAN J, V28, P354, DOI 10.1597/1545-1569(1991)028<0354:PAICWH>2.3.CO;2; PERTSCHUK MJ, 1985, PLAST RECONSTR SURG, V75, P177, DOI 10.1097/00006534-198502000-00005; Petersson RS, 2014, JAMA FACIAL PLAST SU, V16, P432, DOI 10.1001/jamafacial.2014.651; PILLEMER FG, 1989, CLEFT PALATE J, V26, P201; POSWILLO D, 1988, DEVELOPMENT, V103, P207; ROSENSTEIN D, 1988, CHILD DEV, V59, P1555, DOI 10.2307/1130670; Rothman KJ, 2014, J GEN INTERN MED, V29, P1060, DOI 10.1007/s11606-013-2755-z; Segal LB, 1995, CHILD DEV, V66, P1829; Snyder HT, 2005, CLEFT PALATE-CRAN J, V42, P548, DOI 10.1597/04-078R.1; TRONICK EZ, 1989, AM PSYCHOL, V44, P112, DOI 10.1037/0003-066X.44.2.112	39	10	10	0	3	ALLIANCE COMMUNICATIONS GROUP DIVISION ALLEN PRESS	LAWRENCE	810 EAST 10TH STREET, LAWRENCE, KS 66044 USA	1055-6656	1545-1569		CLEFT PALATE-CRAN J	Cleft Palate-Craniofac. J.	MAY	2018	55	5					711	720		10.1177/1055665617753481	http://dx.doi.org/10.1177/1055665617753481			10	Dentistry, Oral Surgery & Medicine; Surgery	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Dentistry, Oral Surgery & Medicine; Surgery	GD2OS	29377723	Green Accepted			2022-10-03	WOS:000430340500011
J	Dibeklioglu, H; Hammal, Z; Cohn, JF				Dibeklioglu, Hamdi; Hammal, Zakia; Cohn, Jeffrey			Dynamic Multimodal Measurement of Depression Severity Using Deep Autoencoding	IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS			English	Article						Depression severity; Facial movement dynamics; Head movement dynamics; Multimodal fusion; Vocal prosody	FEATURE-SELECTION; SPEECH; INDICATORS; SYMPTOMS; SCALE; SPACE	Depression is one of the most common psychiatric disorders worldwide, with over 350 million people affected. Current methods to screen for and assess depression depend almost entirely on clinical interviews and self-report scales. While useful, such measures lack objective, systematic, and efficient ways of incorporating behavioral observations that are strong indicators of depression presence and severity. Using dynamics of facial and head movement and vocalization, we trained classifiers to detect three levels of depression severity. Participants were a community sample diagnosed with major depressive disorder. They were recorded in clinical interviews (Hamilton Rating Scale for Depression, HRSD) at seven-week intervals over a period of 21 weeks. At each interview, they were scored by the HRSD as moderately to severely depressed, mildly depressed, or remitted. Logistic regression classifiers using leave-one-participant-out validation were compared for facial movement, head movement, and vocal prosody individually and in combination. Accuracy of depression severity measurement from facial movement dynamics was higher than that for head movement dynamics, and each was substantially higher than that for vocal prosody. Accuracy using all three modalities combined only marginally exceeded that of face and head combined. These findings suggest that automatic detection of depression severity from behavioral indicators in patients is feasible and that multimodal measures afford the most powerful detection.	[Dibeklioglu, Hamdi] Delft Univ Technol, Pattern Recognit & Bioinformat Grp, NL-2628 CD Delft, Netherlands; [Hammal, Zakia; Cohn, Jeffrey] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Dibeklioglu, H (corresponding author), Delft Univ Technol, Pattern Recognit & Bioinformat Grp, NL-2628 CD Delft, Netherlands.	h.dibeklioglu@tudelft.nl; zakia_hammal@yahoo.fr; jeffcohn@cs.cmu.edu	Dibeklioglu, Hamdi/AAB-6907-2020		National Institute of Mental Health of the National Institutes of Health [MH096951]; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH096951] Funding Source: NIH RePORTER	National Institute of Mental Health of the National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))	This work was supported in part by the National Institute of Mental Health of the National Institutes of Health under Award MH096951. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.	Alghowinem Sharifa, 2015, IEEE Int Conf Autom Face Gesture Recognit Workshops, V1, DOI 10.1109/FG.2015.7163113; Alghowinem S, 2013, IEEE IMAGE PROC, P4220, DOI 10.1109/ICIP.2013.6738869; Alghowinem S, 2013, INT CONF AFFECT, P283, DOI 10.1109/ACII.2013.53; Allen NB, 2003, PSYCHOL BULL, V129, P887, DOI 10.1037/0033-2909.129.6.887; Alpert M, 2001, J AFFECT DISORDERS, V66, P59, DOI 10.1016/S0165-0327(00)00335-9; Alshamlan H, 2015, BIOMED RES INT, V2015, DOI 10.1155/2015/604910; American Psychiatric Association D Association A P, 2013, DIAGN STAT MAN MENT, V21, P591, DOI 10.1176/appi.books.9780890425596; [Anonymous], 2017, CMU SPHINX OPEN SOUR; BECK AT, 1961, ARCH GEN PSYCHIAT, V4, P561, DOI 10.1001/archpsyc.1961.01710120031004; Boersma P, 2017, PRAAT DOING PHONETIC; Boudahmane K., 2017, TRANSCRIBERAG; Caligiuri MP, 2000, J AFFECT DISORDERS, V57, P83, DOI 10.1016/S0165-0327(99)00068-3; Cannizzaro M, 2004, BRAIN COGNITION, V56, P30, DOI 10.1016/j.bandc.2004.05.003; COHEN J, 1968, PSYCHOL BULL, V70, P213, DOI 10.1037/h0026256; Cohn Jeffrey F., 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPR.2009.5204260; Cohn J. F., 2017, HDB MULTIMODAL MULTI; Cummins N., 2013, P ACM INT WORKSH AUD, P11, DOI DOI 10.1145/2512530.2512535; Cummins N, 2015, SPEECH COMMUN, V75, P27, DOI 10.1016/j.specom.2015.09.003; Cummins N, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P3008; DARBY JK, 1984, J COMMUN DISORD, V17, P75, DOI 10.1016/0021-9924(84)90013-3; Dibeklioglu H, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P307, DOI 10.1145/2818346.2820776; Dibeklioglu H, 2015, IEEE T MULTIMEDIA, V17, P279, DOI 10.1109/TMM.2015.2394777; Dibeklioglu H, 2012, LECT NOTES COMPUT SC, V7574, P525, DOI 10.1007/978-3-642-33712-3_38; Ding C, 2003, PROCEEDINGS OF THE 2003 IEEE BIOINFORMATICS CONFERENCE, P523, DOI 10.1109/CSB.2003.1227396; Ekman P., 2009, TELLING LIES CLUES D; Eyben F., 2009, AFF COMP INT INT WOR, P1, DOI DOI 10.1109/ACII.2009.5349350; Eyben Florian, 2010, OPENSMILE THE MUNICH, P1459, DOI [10.1145/1873951.1874246, DOI 10.1145/1873951.1874246]; First M.B., 1995, STRUCTURED CLIN INTE; Fournier JC, 2010, JAMA-J AM MED ASSOC, V303, P47, DOI 10.1001/jama.2009.1943; FOWLES DC, 1994, NEBR SYM MOTIV, V41, P181; France DJ, 2000, IEEE T BIO-MED ENG, V47, P829, DOI 10.1109/10.846676; Fridlund A. J., 1992, REV PERSONALITY SOCI, P90; Girard JM, 2014, IMAGE VISION COMPUT, V32, P641, DOI 10.1016/j.imavis.2013.12.007; HALL JA, 1995, APPL PREV PSYCHOL, V4, P21, DOI 10.1016/S0962-1849(05)80049-6; HAMILTON M, 1960, J NEUROL NEUROSUR PS, V23, P56, DOI 10.1136/jnnp.23.1.56; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton G. E., 2009, SCHOLARPEDIA, V4, P5947, DOI DOI 10.4249/SCHOLARPEDIA.5947; Hollon SD, 2002, PSYCHOL SCI, P39, DOI 10.1111/1529-1006.00008; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Jain V., 2014, P 4 INT WORKSH AUD V, P87; Jan A., 2014, P 4 INT WORKSHOP AUD, P73; Jeni Laszlo A., 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163142; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Joshi J., 2013, P 10 IEEE INT C WORK, P1, DOI DOI 10.1109/FG.2013.6553796; Joshi J, 2013, J MULTIMODAL USER IN, V7, P217, DOI 10.1007/s12193-013-0123-2; Joshi J, 2012, INT C PATT RECOG, P2634; KLINGER E, 1975, PSYCHOL REV, V82, P1, DOI 10.1037/h0076171; Kotov R, 2010, PSYCHOL BULL, V136, P768, DOI 10.1037/a0020327; Kuncheva L. I., 2004, COMBINING PATTERN CL, P151; LEFF J, 1981, PSYCHOL MED, V11, P849, DOI 10.1017/S0033291700041349; Lepine Jean-Pierre, 2011, Neuropsychiatr Dis Treat, V7, P3, DOI 10.2147/NDT.S19617; Maddage NC, 2009, IEEE ENG MED BIO, P3723, DOI 10.1109/IEMBS.2009.5334815; Manzagol P.-A., 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294; Mathers CD, 2006, PLOS MED, V3, DOI 10.1371/journal.pmed.0030442; Meng H., 2013, P ACM INT WORKSH AUD, P21, DOI DOI 10.1145/2512530.2512532; Nesse RM, 2000, ARCH GEN PSYCHIAT, V57, P14, DOI 10.1001/archpsyc.57.1.14; NILSONNE A, 1988, ACTA PSYCHIAT SCAND, V77, P253, DOI 10.1111/j.1600-0447.1988.tb05118.x; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Scherer Stefan, 2014, Proc ACM Int Conf Multimodal Interact, V2014, P112, DOI 10.1145/2663204.2663238; Scherer S, 2016, IEEE T AFFECT COMPUT, V7, P59, DOI 10.1109/TAFFC.2015.2440264; Scherer S, 2015, INT CONF ACOUST SPEE, P4789, DOI 10.1109/ICASSP.2015.7178880; Scherer S, 2013, INTERSPEECH, P847; Senoussaoui M., 2014, P 4 INT WORKSH AUD V, P57, DOI DOI 10.1145/2661806.2661819; Sidorov M., 2014, P 4 INT WORKSH AUD V, P81, DOI DOI 10.1145/2661806.2661816; Sobin C, 1997, AM J PSYCHIAT, V154, P4; Stratou G, 2013, INT CONF AFFECT, P147, DOI 10.1109/ACII.2013.31; Trevino AC, 2011, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2011-42; Valstar Michel, 2014, P 4 INT WORKSH AUD V, P3, DOI DOI 10.1145/2661806.2661807; VELLEMAN PF, 1980, J AM STAT ASSOC, V75, P609, DOI 10.2307/2287657; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Williams J, 2014, CURR CLIN UROL, P65, DOI 10.1007/978-1-4939-0853-0_7; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yang Y, 2013, IEEE T AFFECT COMPUT, V4, P142, DOI 10.1109/T-AFFC.2012.38	74	62	64	3	23	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	2168-2194	2168-2208		IEEE J BIOMED HEALTH	IEEE J. Biomed. Health Inform.	MAR	2018	22	2					525	536		10.1109/JBHI.2017.2676878	http://dx.doi.org/10.1109/JBHI.2017.2676878			12	Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Medical Informatics	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Mathematical & Computational Biology; Medical Informatics	FY4XT	28278485	Green Accepted			2022-10-03	WOS:000426831200024
J	Martin, KB; Hammal, Z; Ren, G; Cohn, JF; Cassell, J; Ogihara, M; Britton, JC; Gutierrez, A; Messinger, DS				Martin, Katherine B.; Hammal, Zakia; Ren, Gang; Cohn, Jeffrey; Cassell, Justine; Ogihara, Mitsunori; Britton, Jennifer C.; Gutierrez, Anibal; Messinger, Daniel S.			Objective measurement of head movement differences in children with and without autism spectrum disorder	MOLECULAR AUTISM			English	Article						Head movement; Motor movement; Autism spectrum disorder; Social processing	DEVELOPMENTAL COORDINATION DISORDER; HIGH-FUNCTIONING AUTISM; YOUNG-CHILDREN; REPETITIVE BEHAVIORS; MOTOR STEREOTYPIES; IDENTIFY CHILDREN; GAIT FUNCTION; PATTERNS; ATTENTION; INFANTS	Background: Deficits in motor movement in children with autism spectrum disorder (ASD) have typically been characterized qualitatively by human observers. Although clinicians have noted the importance of atypical head positioning (e.g. social peering and repetitive head banging) when diagnosing children with ASD, a quantitative understanding of head movement in ASD is lacking. Here, we conduct a quantitative comparison of head movement dynamics in children with and without ASD using automated, person-independent computer-vision based head tracking (Zface). Because children with ASD often exhibit preferential attention to nonsocial versus social stimuli, we investigated whether children with and without ASD differed in their head movement dynamics depending on stimulus sociality. Methods: The current study examined differences in head movement dynamics in children with (n = 21) and without ASD (n = 21). Children were video-recorded while watching a 16-min video of social and nonsocial stimuli. Three dimensions of rigid head movement-pitch (head nods), yaw (head turns), and roll (lateral head inclinations) - were tracked using Zface. The root mean square of pitch, yaw, and roll was calculated to index the magnitude of head angular displacement (quantity of head movement) and angular velocity (speed). Results: Compared with children without ASD, children with ASD exhibited greater yaw displacement, indicating greater head turning, and greater velocity of yaw and roll, indicating faster head turning and inclination. Followup analyses indicated that differences in head movement dynamics were specific to the social rather than the nonsocial stimulus condition. Conclusions: Head movement dynamics (displacement and velocity) were greater in children with ASD than in children without ASD, providing a quantitative foundation for previous clinical reports. Head movement differences were evident in lateral (yaw and roll) but not vertical (pitch) movement and were specific to a social rather than nonsocial condition. When presented with social stimuli, children with ASD had higher levels of head movement and moved their heads more quickly than children without ASD. Children with ASD may use head movement to modulate their perception of social scenes.	[Martin, Katherine B.; Britton, Jennifer C.; Gutierrez, Anibal; Messinger, Daniel S.] Univ Miami, Dept Psychol, 5665 Ponce Leon Blvd, Coral Gables, FL 33146 USA; [Hammal, Zakia] Carnegie Mellon Univ, Robot Inst, 5000 Forbes Ave, Pittsburgh, PA 15213 USA; [Ren, Gang] Univ Miami, Ctr Computat Sci, 1320 S Dixie Hwy, Miami, FL 33146 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, 210 S Bouquet St, Pittsburgh, PA 15260 USA; [Cassell, Justine] Carnegie Mellon Univ, Human Comp Interact, 5000 Forbes Ave, Pittsburgh, PA 15213 USA; [Ogihara, Mitsunori] Univ Miami, Dept Comp Sci, 1365 Mem Dr, Coral Gables, FL 33146 USA		Martin, KB (corresponding author), Univ Miami, Dept Psychol, 5665 Ponce Leon Blvd, Coral Gables, FL 33146 USA.	kmartin@psy.miami.edu	Ogihara, Mitsunori/AAB-8275-2020; Ren, Gang/R-6220-2019	Ogihara, Mitsunori/0000-0002-5690-7854; 	Autism Speaks National Institute of General Medical Sciences [1R01GM105004]; NATIONAL INSTITUTE OF DENTAL & CRANIOFACIAL RESEARCH [R03DE026513] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF GENERAL MEDICAL SCIENCES [R01GM105004] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH096951] Funding Source: NIH RePORTER	Autism Speaks National Institute of General Medical Sciences; NATIONAL INSTITUTE OF DENTAL & CRANIOFACIAL RESEARCH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Dental & Craniofacial Research (NIDCR)); NATIONAL INSTITUTE OF GENERAL MEDICAL SCIENCES(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of General Medical Sciences (NIGMS)); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))	Autism Speaks National Institute of General Medical Sciences (1R01GM105004).	American Psychiatric Association, 2013, DIAGNOSTIC STAT MANU, V5th ed., DOI [10.1176/appi.books.9780890425596, DOI 10.1176/APPI.BOOKS.9780890425596]; Anzulewicz A, 2016, SCI REP-UK, V6, DOI 10.1038/srep31107; BARNES GR, 1991, J PHYSIOL-LONDON, V439, P439, DOI 10.1113/jphysiol.1991.sp018675; Bryson SE, 2007, J AUTISM DEV DISORD, V37, P12, DOI 10.1007/s10803-006-0328-2; Calhoun M, 2011, CLIN BIOMECH, V26, P200, DOI 10.1016/j.clinbiomech.2010.09.013; Chang CH, 2010, RES DEV DISABIL, V31, P1536, DOI 10.1016/j.ridd.2010.06.003; Chawarska K, 2013, BIOL PSYCHIAT, V74, P195, DOI 10.1016/j.biopsych.2012.11.022; Chen FC, 2011, RES DEV DISABIL, V32, P1948, DOI 10.1016/j.ridd.2011.03.027; Cohen IL, 2014, MOL AUTISM, V5, DOI 10.1186/2040-2392-5-15; Cohn J. F, 2016, P IEEE C COMP VIS PA, P87; Cook J, 2016, PHILOS T R SOC B, V371, DOI 10.1098/rstb.2015.0372; Crippa A, 2015, J AUTISM DEV DISORD, V45, P2146, DOI 10.1007/s10803-015-2379-8; DUNCAN S, 1972, J PERS SOC PSYCHOL, V23, P283, DOI 10.1037/h0033031; Dunn W, 1997, INFANT YOUNG CHILD, V9, P23, DOI 10.1097/00001163-199704000-00005; Esposito G, 2011, BRAIN DEV-JPN, V33, P367, DOI 10.1016/j.braindev.2010.07.006; Fournier KA, 2010, J AUTISM DEV DISORD, V40, P1227, DOI 10.1007/s10803-010-0981-3; Fournier KA, 2010, GAIT POSTURE, V32, P6, DOI 10.1016/j.gaitpost.2010.02.007; FREEMAN BJ, 1978, J AM ACAD CHILD PSY, V17, P576, DOI 10.1016/S0002-7138(09)61012-8; Goldman S, 2009, DEV MED CHILD NEUROL, V51, P30, DOI 10.1111/j.1469-8749.2008.03178.x; Hammal Z, 2014, IEEE T AFFECT COMPUT, V5, P155, DOI 10.1109/TAFFC.2014.2326408; Hammal Z, 2013, INT CONF AFFECT, P276, DOI 10.1109/ACII.2013.52; Happe F, 2006, J AUTISM DEV DISORD, V36, P5, DOI 10.1007/s10803-005-0039-0; Hellendoorn A, 2014, RES DEV DISABIL, V35, P423, DOI 10.1016/j.ridd.2013.11.012; Jeni L. A., 2015, DENSE 3D FACE ALIGNM; Kim SH, 2010, AUTISM RES, V3, P162, DOI 10.1002/aur.142; Klin A, 2002, ARCH GEN PSYCHIAT, V59, P809, DOI 10.1001/archpsyc.59.9.809; Klin A, 2009, NATURE, V459, P257, DOI 10.1038/nature07868; Landa R, 2006, J CHILD PSYCHOL PSYC, V47, P629, DOI 10.1111/j.1469-7610.2006.01531.x; Libertus K, 2011, DEVELOPMENTAL SCI, V14, P1355, DOI 10.1111/j.1467-7687.2011.01084.x; Loh A, 2007, J AUTISM DEV DISORD, V37, P25, DOI 10.1007/s10803-006-0333-5; Lord C, 2000, J AUTISM DEV DISORD, V30, P205, DOI 10.1023/A:1005592401947; LORD C, 1994, J AUTISM DEV DISORD, V24, P659, DOI 10.1007/BF02172145; Mahone EM, 2004, J PEDIATR-US, V145, P391, DOI 10.1016/j.jpeds.2004.06.014; McLean L, 2003, J ELECTROMYOGR KINES, V13, P169, DOI 10.1016/S1050-6411(02)00051-2; Memari AH, 2013, RES AUTISM SPECT DIS, V7, P325, DOI 10.1016/j.rasd.2012.09.010; Mooney EL, 2006, EUR CHILD ADOLES PSY, V15, P12, DOI 10.1007/s00787-006-0499-6; Mottron L, 2007, DEV PSYCHOPATHOL, V19, P23, DOI 10.1017/S0954579407070022; Mullen E.M., 1995, MULLEN SCALES EARLY; Mundy P, 2007, CURR DIR PSYCHOL SCI, V16, P269, DOI 10.1111/j.1467-8721.2007.00518.x; Ozonoff S, 2008, J AUTISM DEV DISORD, V38, P644, DOI 10.1007/s10803-007-0430-0; Ozonoff S, 2010, J AM ACAD CHILD PSY, V49, P256, DOI 10.1016/j.jaac.2009.11.009; Piek JP, 2004, HUM MOVEMENT SCI, V23, P475, DOI 10.1016/j.humov.2004.08.019; Pierce K, 2011, ARCH GEN PSYCHIAT, V68, P101, DOI 10.1001/archgenpsychiatry.2010.113; Provost B, 2007, J AUTISM DEV DISORD, V37, P321, DOI 10.1007/s10803-006-0170-6; Rinehart NJ, 2006, DEV MED CHILD NEUROL, V48, P819, DOI 10.1017/S0012162206001769; Rinehart NJ, 2006, EUR CHILD ADOLES PSY, V15, P256, DOI 10.1007/s00787-006-0530-y; Rinehart NJ, 2001, J AUTISM DEV DISORD, V31, P79, DOI 10.1023/A:1005617831035; Rodgers J, 2012, J AUTISM DEV DISORD, V42, P2404, DOI 10.1007/s10803-012-1531-y; Shic F, 2011, BRAIN RES, V1380, P246, DOI 10.1016/j.brainres.2010.11.074; Singer HS, 2009, SEMIN PEDIATR NEUROL, V16, P77, DOI 10.1016/j.spen.2009.03.008; Staples KL, 2010, J AUTISM DEV DISORD, V40, P209, DOI 10.1007/s10803-009-0854-9; Torres EB, 2016, FRONT NEUROL, V7, DOI 10.3389/fneur.2016.00008; Torres EB, 2013, FRONT INTEGR NEUROSC, V7, DOI 10.3389/fnint.2013.00032; Trevarthen C, 2013, FRONT INTEGR NEUROSC, V7, DOI 10.3389/fnint.2013.00049; Wechsler D, 2002, WECHSLER PRESCHOOL P; Wolpert DM, 2003, PHILOS T R SOC B, V358, P593, DOI 10.1098/rstb.2002.1238; Zwaigenbaum L, 2005, INT J DEV NEUROSCI, V23, P143, DOI 10.1016/j.ijdevneu.2004.05.001	57	33	33	2	16	BIOMED CENTRAL LTD	LONDON	236 GRAYS INN RD, FLOOR 6, LONDON WC1X 8HL, ENGLAND	2040-2392			MOL AUTISM	Mol. Autism	FEB 27	2018	9								14	10.1186/s13229-018-0198-4	http://dx.doi.org/10.1186/s13229-018-0198-4			10	Genetics & Heredity; Neurosciences	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Genetics & Heredity; Neurosciences & Neurology	FX8YW	29492241	gold, Green Published			2022-10-03	WOS:000426382700001
C	Cohn, JF; Jeni, LA; Ertugrul, IO; Malone, D; Okun, MS; Borton, D; Goodman, WK			ACM	Cohn, Jeffrey; Jeni, Laszlo A.; Ertugrul, Itir Onal; Malone, Donald; Okun, Michael S.; Borton, David A.; Goodman, Wayne K.			Automated Affect Detection in Deep Brain Stimulation for Obsessive-Compulsive Disorder: A Pilot Study	ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION			English	Proceedings Paper	20th ACM International Conference on Multimodal Interaction (ICMI)	OCT 16-20, 2018	Boulder, CO	Assoc Comp Machinery, Assoc Comp Machinery SIGCHI, Openstream, Microsoft, Univ Colorado Boulder, Inst Cognit Sci, audEERING		Obsessive compulsive disorder; Deep brain stimulation; Facial expression; Body expression; Action units; Behavioral dynamics; Social signal processing	EXPRESSION; SMILE	Automated measurement of affective behavior in psychopathology has been limited primarily to screening and diagnosis. While useful, clinicians more often are concerned with whether patients are improving in response to treatment. Are symptoms abating, is affect becoming more positive, are unanticipated side effects emerging? When treatment includes neural implants, need for objective, repeatable biometrics tied to neurophysiology becomes especially pressing. We used automated face analysis to assess treatment response to deep brain stimulation (DBS) in two patients with intractable obsessive-compulsive disorder (OCD). One was assessed intraoperatively following implantation and activation of the DBS device. The other was assessed three months post-implantation. Both were assessed during DBS on and off conditions. Positive and negative valence were quantified using a CNN trained on normative data of 160 non-OCD participants. Thus, a secondary goal was domain transfer of the classifiers. In both contexts, DBS-on resulted in marked positive affect. In response to DBS-off, affect flattened in both contexts and alternated with increased negative affect in the outpatient setting. Mean AUC for domain transfer was 0.87. These findings suggest that parametric variation of DBS is strongly related to affective behavior and may introduce vulnerability for negative affect in the event that DBS is discontinued.	[Cohn, Jeffrey] Univ Pittsburgh, Pittsburgh, PA 15260 USA; [Jeni, Laszlo A.; Ertugrul, Itir Onal] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Malone, Donald] Cleveland Clin, Cleveland, OH 44106 USA; [Okun, Michael S.] Univ Florida, Gainesville, FL USA; [Borton, David A.] Brown Univ, Providence, RI 02912 USA; [Goodman, Wayne K.] Baylor Coll Med, Houston, TX 77030 USA		Cohn, JF (corresponding author), Univ Pittsburgh, Pittsburgh, PA 15260 USA.	jeffcohn@pitt.edu; laszlojeni@cmu.edu; iertugru@cmu.edu; maloned@ccf.org; okun@neurology.ufl.edu; david_borton@brown.edu; wayne.goodman@bcm.edu			NIH [NS100549, MH096951]; NSF [1418026]; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH096951] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE [UH3NS100549] Funding Source: NIH RePORTER	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); NATIONAL INSTITUTE OF NEUROLOGICAL DISORDERS AND STROKE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS))	This research was supported in part by NIH awards NS100549 and MH096951 and NSF award 1418026.	American Psychiatric Association, 2015, DIAGNOSTIC STAT MANU, V5th; Baker JK, 2010, INT J BEHAV DEV, V34, P88, DOI 10.1177/0165025409350365; Bennett FM, 1954, PUBLIC OPIN QUART, V18, P303; BRENNAN RL, 1981, EDUC PSYCHOL MEAS, V41, P687, DOI 10.1177/001316448104100307; CACIOPPO JT, 1988, J PERS SOC PSYCHOL, V54, P592, DOI 10.1037/0022-3514.54.4.592; CACIOPPO JT, 1986, J PERS SOC PSYCHOL, V50, P260, DOI 10.1037/0022-3514.50.2.260; Campbell K, 2019, AUTISM, V23, P619, DOI 10.1177/1362361318766247; CAMRAS LA, 1992, COGNITION EMOTION, V6, P269, DOI 10.1080/02699939208411072; Charles D, 1872, EXPRESSION EMOTIONS; Dibeklioglu H, 2018, IEEE J BIOMED HEALTH, V22, P525, DOI 10.1109/JBHI.2017.2676878; Dimberg U, 2002, COGNITION EMOTION, V16, P449, DOI 10.1080/02699930143000356; EKMAN P, 1990, J PERS SOC PSYCHOL, V58, P342, DOI 10.1037/0022-3514.58.2.342; Ekman P, 2002, RES NEXUS; Ekman P., 2003, ISHK, V10; Girard Jeffrey M., 2018, UNPUB; Greenberg BD, 2010, MOL PSYCHIATR, V15, P64, DOI 10.1038/mp.2008.55; Hammal Z, 2014, IEEE T AFFECT COMPUT, V5, P155, DOI 10.1109/TAFFC.2014.2326408; IZARD CE, 1993, PSYCHOL REV, V100, P68; Jeni LA, 2016, LECT NOTES COMPUT SC, V9914, P511, DOI 10.1007/978-3-319-48881-3_35; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Joshi J, 2013, INT CONF AFFECT, P492, DOI 10.1109/ACII.2013.87; Kingma D.P., 2015, ARXIV; Lucey P., 2010, 2010 IEEE COMPUTER S, P94, DOI [10.1109/CVPRW.2010.5543262, DOI 10.1109/CVPRW.2010.5543262]; Martin KB, 2018, MOL AUTISM, V9, DOI 10.1186/s13229-018-0198-4; Nathaniel Haines, 2017, THESIS; Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127; Scherer S, 2014, IMAGE VISION COMPUT, V32, P648, DOI 10.1016/j.imavis.2014.06.001; Springer US, 2006, NEUROCASE, V12, P191, DOI 10.1080/13554790600646995; Valstar MF, 2017, IEEE INT CONF AUTOMA, P839, DOI 10.1109/FG.2017.107; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhang Z, 2016, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2016.374; Zhao X., 2013, ANN INT COMMUNICATIO, V36, P419, DOI [10.1080/23808985.2013.11679142, DOI 10.1080/23808985.2013.11679142]	32	6	6	0	3	ASSOC COMPUTING MACHINERY	NEW YORK	1515 BROADWAY, NEW YORK, NY 10036-9998 USA			978-1-4503-5692-3				2018							40	44		10.1145/3242969.3243023	http://dx.doi.org/10.1145/3242969.3243023			5	Computer Science, Cybernetics; Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering	BL9OE	30511050	Green Accepted, Bronze			2022-10-03	WOS:000457913100009
C	Ertugrul, IO; Jeni, LA; Cohn, JF			IEEE	Ertugrul, Itir Onal; Jeni, Laszlo A.; Cohn, Jeffrey			FACSCaps: Pose-Independent Facial Action Coding with Capsules	PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS (CVPRW)	IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops		English	Proceedings Paper	IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)	JUN 18-22, 2018	Salt Lake City, UT	IEEE Comp Soc				Most automated facial expression analysis methods treat the face as a 2D object, flat like a sheet of paper. That works well provided images are frontal or nearly so. In real-world conditions, moderate to large head rotation is common and system performance to recognize expression degrades. Multi-view Convolutional Neural Networks (CNNs) have been proposed to increase robustness to pose, but they require greater model sizes and may generalize poorly across views that are not included in the training set. We propose FACSCaps architecture to handle multi-view and multi-label facial action unit (AU) detection within a single model that can generalize to novel views. Additionally, FACSCaps's ability to synthesize faces enables insights into what is leaned by the model. FACSCaps models video frames using matrix capsules, where hierarchical pose relationships between face parts are built into internal representations. The model is trained by jointly optimizing a multi-label loss and the reconstruction accuracy. FACSCaps was evaluated using the FERA 2017 facial expression dataset that includes spontaneous facial expressions in a wide range of head orientations. FACSCaps outperformed both state-of-the-art CNNs and their temporal extensions.	[Ertugrul, Itir Onal; Jeni, Laszlo A.; Cohn, Jeffrey] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Ertugrul, IO (corresponding author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.	iertugru@andrew.cmu.edu; laszlojeni@cmu.edu; jeffcohn@pitt.edu			National Institute of Mental Health of the National Institutes of Health [MH096951]; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH096951] Funding Source: NIH RePORTER	National Institute of Mental Health of the National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))	Preparation of this article was supported in part by the National Institute of Mental Health of the National Institutes of Health under Award Number MH096951.	AFSHAR P, 2018, ARXIV180210200; Andersen P., 2018, ARXIV180109597; [Anonymous], 2017, ARXIV171203480; Baltrusaitis T., 2016, 2016 IEEE WINT C APP, P1, DOI [10.1109/WACV.2016.7477553, DOI 10.1109/WACV.2016.7477553]; Batista JC, 2017, IEEE INT CONF AUTOMA, P866, DOI 10.1109/FG.2017.111; Chu WS, 2017, IEEE INT CONF AUTOMA, P25, DOI 10.1109/FG.2017.13; Chu WS, 2017, IEEE T PATTERN ANAL, V39, P529, DOI 10.1109/TPAMI.2016.2547397; Eleftheriadis S., 2016, P IEEE C COMP VIS PA, P18; Eleftheriadis S, 2015, IEEE I CONF COMP VIS, P3792, DOI 10.1109/ICCV.2015.432; Ghosh S, 2015, INT CONF AFFECT, P609, DOI 10.1109/ACII.2015.7344632; Gudi, 2015, AUT FAC GEST REC FG, P1, DOI DOI 10.1109/FG.2015.7284873; He J, 2017, IEEE INT CONF AUTOMA, P848, DOI 10.1109/FG.2017.108; Higgins I, 2017, ICLR; Hinton G., 2018, MATRIX CAPSULES EM R; Jaiswal A., 2018, ARXIV180206167; Jaiswal Shashank, 2016, WACV; Kingma D.P., 2015, ARXIV; Kulkarni TD, 2015, ADV NEUR IN, V28; Li XR, 2017, IEEE INT CONF AUTOMA, P860, DOI 10.1109/FG.2017.110; Li YQ, 2013, IEEE T AFFECT COMPUT, V4, P127, DOI 10.1109/T-AFFC.2013.5; Lucey S., 2007, FACE RECOGNITION; Sabour S., 2017, NEURIPS, P3856; Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127; Senechal T, 2012, IEEE T SYST MAN CY B, V42, P993, DOI 10.1109/TSMCB.2012.2193567; Tang CG, 2017, IEEE INT CONF AUTOMA, P878, DOI 10.1109/FG.2017.113; Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962; Tian YL, 2005, HANDBOOK OF FACE RECOGNITION, P247, DOI 10.1007/0-387-27257-7_12; Toser Z, 2016, LECT NOTES COMPUT SC, V9915, P359, DOI 10.1007/978-3-319-49409-8_29; Tran D. Linh, 2017, P IEEE C COMP VIS PA, P3190; Valstar MF, 2017, IEEE INT CONF AUTOMA, P839, DOI 10.1109/FG.2017.107; Wang Y., 2018, SENTIMENT ANAL CAPSU; Wang ZH, 2013, IEEE I CONF COMP VIS, P3304, DOI 10.1109/ICCV.2013.410; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang X, 2016, PATTERN RECOGN, V51, P187, DOI 10.1016/j.patcog.2015.08.026; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhang Z, 2016, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2016.374; Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369; Zhao KL, 2015, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2015.7298833	38	11	11	0	5	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2160-7508		978-1-5386-6100-0	IEEE COMPUT SOC CONF			2018							2211	2220		10.1109/CVPRW.2018.00287	http://dx.doi.org/10.1109/CVPRW.2018.00287			10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL9LT	30944768	Green Accepted			2022-10-03	WOS:000457636800280
C	Kacem, A; Hammal, Z; Daoudi, M; Cohn, JF			IEEE	Kacem, Anis; Hammal, Zakia; Daoudi, Mohamed; Cohn, Jeffrey			Detecting Depression Severity by Interpretable Representations of Motion Dynamics	PROCEEDINGS 2018 13TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE & GESTURE RECOGNITION (FG 2018)	IEEE International Conference on Automatic Face and Gesture Recognition and Workshops		English	Proceedings Paper	13th IEEE International Conference on Automatic Face & Gesture Recognition (FG)	MAY 15-19, 2018	Xi an, PEOPLES R CHINA	IEEE Comp Soc, IEEE Biometr Council			SCALE	Recent breakthroughs in deep learning using automated measurement of face and head motion have made possible the first objective measurement of depression severity. While powerful, deep learning approaches lack interpretability. We developed an interpretable method of automatically measuring depression severity that uses barycentric coordinates of facial landmarks and a Lie-algebra based rotation matrix of 3D head motion. Using these representations, kinematic features are extracted, preprocessed, and encoded using Gaussian Mixture Models (GMM) and Fisher vector encoding. A multi-class SVM is used to classify the encoded facial and head movement dynamics into three levels of depression severity. The proposed approach was evaluated in adults with history of chronic depression. The method approached the classification accuracy of state-of-the-art deep learning while enabling clinically and theoretically relevant findings. The velocity and acceleration of facial movement strongly mapped onto depression severity symptoms consistent with clinical data and theory.	[Kacem, Anis; Daoudi, Mohamed] Univ Lille, CNRS, IMT Lille Douai, CRIStAL,UMR 9189, F-59000 Lille, France; [Hammal, Zakia; Cohn, Jeffrey] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Kacem, A (corresponding author), Univ Lille, CNRS, IMT Lille Douai, CRIStAL,UMR 9189, F-59000 Lille, France.				U.S. National Institute Of Nursing Research of the National Institutes of Health [R21NR016510]; U.S. National Institute of Mental Health of the National Institutes of Health [MH096951]; U.S. National Science Foundation [IIS-1721667]; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH096951] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF NURSING RESEARCH [R21NR016510] Funding Source: NIH RePORTER	U.S. National Institute Of Nursing Research of the National Institutes of Health; U.S. National Institute of Mental Health of the National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); U.S. National Science Foundation(National Science Foundation (NSF)); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); NATIONAL INSTITUTE OF NURSING RESEARCH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Nursing Research (NINR))	We thank J-C. Alvarez Paiva for fruitful discussions on barycentric coordinate representation. Research reported in this publication was supported in part by the U.S. National Institute Of Nursing Research of the National Institutes of Health under Award Number R21NR016510, the U.S. National Institute of Mental Health of the National Institutes of Health under Award Number MH096951, and the U.S. National Science Foundation under award IIS-1721667. The content is solely the responsibility of the authors and does not necessarily represent the official views of the sponsors.	Alghowinem S, 2013, INT CONF AFFECT, P283, DOI 10.1109/ACII.2013.53; BECK AT, 1961, ARCH GEN PSYCHIAT, V4, P561, DOI 10.1001/archpsyc.1961.01710120031004; Begelfor E., 2006, P IEEE C COMP VIS PA, V2, P2087, DOI DOI 10.1109/CVPR.2006.50; Berger M., 1987, GEOMETRY, Vi; Boas ML, 2006, MATH METHODS PHYS SC; COHEN J, 1968, PSYCHOL BULL, V70, P213, DOI 10.1037/h0026256; Cohn J. F., 2009, 2009 3 INT C AFF COM, DOI 10.1109/ACII.2009.5349358; Dibeklioglu H., 2017, IEEE J BIOMEDICAL HL; Dibeklioglu H, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P307, DOI 10.1145/2818346.2820776; EKMAN P, 1980, J PERS SOC PSYCHOL, V39, P1125, DOI 10.1037/h0077722; FISCH HU, 1983, J ABNORM PSYCHOL, V92, P307, DOI 10.1037/0021-843X.92.3.307; Fournier JC, 2010, JAMA-J AM MED ASSOC, V303, P47, DOI 10.1001/jama.2009.1943; Girard JM, 2014, IMAGE VISION COMPUT, V32, P641, DOI 10.1016/j.imavis.2013.12.007; HAMILTON M, 1960, J NEUROL NEUROSUR PS, V23, P56, DOI 10.1136/jnnp.23.1.56; Jayasumana S, 2013, IEEE I CONF COMP VIS, P1249, DOI 10.1109/ICCV.2013.158; Jeni LA, 2017, IMAGE VISION COMPUT, V58, P13, DOI 10.1016/j.imavis.2016.05.009; Joshi J, 2013, J MULTIMODAL USER IN, V7, P1; Kacem A., 2017, INT C COMP VIS; Kacem A, 2018, IEEE INT C AUT FAC G; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Renneberg B, 2005, J BEHAV THER EXP PSY, V36, P183, DOI 10.1016/j.jbtep.2005.05.002; Rottenberg J, 2005, J ABNORM PSYCHOL, V114, P627, DOI 10.1037/0021-843X.114.4.627; Saragih J., 2006, PATT REC 2006 ICPR 2, V2, P1196; Schwartz G. E., 1976, PSYCHOSOMATIC MED; Taheri S, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P306, DOI 10.1109/FG.2011.5771415; Valstar Michel, 2014, P 4 INT WORKSH AUD V, P3, DOI DOI 10.1145/2661806.2661807; Vemulapalli R, 2016, PROC CVPR IEEE, P4471, DOI 10.1109/CVPR.2016.484; Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82; Williamson JR, 2014, P 4 INT WORKSHOP AUD, P65; Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992	31	18	19	4	5	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2326-5396		978-1-5386-2335-0	IEEE INT CONF AUTOMA			2018							739	745		10.1109/FG.2018.00116	http://dx.doi.org/10.1109/FG.2018.00116			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering	BL7GJ	30271308	Green Accepted, Green Submitted			2022-10-03	WOS:000454996700106
J	Szirtes, G; Orozco, J; Istvan, P; Daniel, S; Akos, U; Cohn, JF				Szirtes, Gabor; Orozco, Javier; Istvan, Petras; Daniel, Szolgay; Akos, Utasi; Cohn, Jeffrey			Behavioral cues help predict impact of advertising on future sales	IMAGE AND VISION COMPUTING			English	Article						Market research; Behavioral cue; Predictive modeling; Facial expression analysis		Advertising aims to influence consumer preferences, appraisals, action tendencies, and behavior in order to increase sales. These are all components of emotion. In the past, they have been measured through self report or panel discussions. While informative, these approaches are difficult to scale to large numbers of consumers, fail to capture moment-to-moment changes in appraisals that may be predictive of sales, and depend on verbal mediation. We used web-cam technology to sample non-verbal responses to television commercials from four product categories in six different countries. For each participant, head pose, head motion, and more frequent facial expressions like smiling, surprise and disgust were automatically measured at each video frame and aggregated across subjects. Dynamic features from the aggregated series were input to simple linear ensemble classifier with 10-fold cross-validation to predict product sales. Sales were predicted with ROC AUC = 0.75, 95% CI [0.727,0.773] and predictions for unseen categories were consistent for all, but one product groups (ROC AUC varies between 0.74 and 0.83, except for Confections with 0.61). Predictions for unseen countries showed similar pattern: ROC AUC varied between 0.71 and 0.89, with the exception of Russia with ROC AUC 0.53. In comparison with previous attempts, our approach yielded higher overall performance and greater generalization over not modeled factors like country or category. These findings support the feasibility, efficiency, and predictive validity of sales predictions from large-scale sampling of viewers' moment-to-moment responses to commercial media. (C) 2017 Elsevier B.V. All rights reserved.	[Szirtes, Gabor; Orozco, Javier; Istvan, Petras; Daniel, Szolgay; Akos, Utasi] Realeyes OU, Tolgyfa Utca 24, H-1027 Budapest, Hungary; [Cohn, Jeffrey] Univ Pittsburgh, 4322 Sennott Sq, Pittsburgh, PA 15260 USA		Szirtes, G (corresponding author), Realeyes OU, Tolgyfa Utca 24, H-1027 Budapest, Hungary.	gabor.szirtes@realeyesit.com			European Community [645094]	European Community(European Commission)	This work was financially supported by the European Community Horizon 2020 [H2020/2014-2020] under grant agreement no. 645094 (SEWA Automatic Sentiment Analysis in the Wild). The authors would like to thank the Ehrenberg-Bass Institute for Marketing Science for helping in defining the data collection experiment, and MARS, Incorporated for providing the valuable sales lift data that made this research possible. The authors are especially grateful for the anonymous reviewers for their concerns and comments on transparency and simplicity.	Airola A, 2010, JMLR WORKSH CONF PRO, V8, P3; Ambadar Z, 2009, J NONVERBAL BEHAV, V33, P17, DOI 10.1007/s10919-008-0059-5; Calvo MG, 2014, J NONVERBAL BEHAV, V38, P549, DOI 10.1007/s10919-014-0191-3; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018; Damasio A. R., 1995, DESCARTES ERROR EMOT; Dibeklioglu H, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P307, DOI 10.1145/2818346.2820776; Dolnicar S, 2011, INT J MARKET RES, V53, P231, DOI 10.2501/IJMR-53-2-231-252; Doshi A, 2012, J VISION, V12, DOI [10.1167/12.2.9, 10.1167/12.6.9]; Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010; Glauner P., 2015, THESIS; Good PI, 2012, COMMON ERRORS STAT A, V4th; Green D.M., 1966, SIGNAL DETECTION THE; Hammal Z, 2015, INT CONF AFFECT, P281, DOI 10.1109/ACII.2015.7344584; Jeni LA, 2013, INT CONF AFFECT, P245, DOI 10.1109/ACII.2013.47; KELTNER D, 1995, J PERS SOC PSYCHOL, V68, P441, DOI 10.1037/0022-3514.68.3.441; LAFRANCE M, 1999, SOCIAL CONTEXT NONVE, P45; Liaukonyte J, 2015, MARKET SCI, V34, P311, DOI 10.1287/mksc.2014.0899; McDuff Daniel, 2015, IEEE Transactions on Affective Computing, V6, P223, DOI 10.1109/TAFFC.2014.2384198; McDuff D., 2013, P ESOMAR C; McDuff D. I., 2014, THESIS; Opitz D., 1999, J ARTIF INTELL RES, V11, P169; Orozco J, 2015, IMAGE VISION COMPUT, V42, P47, DOI 10.1016/j.imavis.2015.07.002; Rokach L, 2010, ARTIF INTELL REV, V33, P1, DOI 10.1007/s10462-009-9124-7; Rottenberg J., 2007, EMOTION ELICITATION; Scherer K.R., 2004, FEELINGS EMOTIONS AM, DOI DOI 10.1017/CBO9780511806582.009; Slaney M., 2014, P 16 INT C MULT INT, P144; statista. com, 2015, US ADV IND STAT FACT; Vapnik V., 1999, NATURE STAT LEARNING; Vriens M, 2001, MARK RES, V13, P14; Xiong Xuehan, 2013, P IEEE C COMP VIS PA; ZAJONC RB, 1980, AM PSYCHOL, V35, P151, DOI 10.1037/0003-066X.35.2.151	31	1	1	0	8	ELSEVIER	AMSTERDAM	RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS	0262-8856	1872-8138		IMAGE VISION COMPUT	Image Vis. Comput.	SEP	2017	65				SI		49	57		10.1016/j.imavis.2017.03.002	http://dx.doi.org/10.1016/j.imavis.2017.03.002			9	Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Optics	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering; Optics	FJ3GM					2022-10-03	WOS:000412618500006
J	Chu, WS; De la Torre, F; Cohn, JF; Messinger, DS				Chu, Wen-Sheng; De la Torre, Fernando; Cohn, Jeffrey; Messinger, Daniel S.			A Branch-and-Bound Framework for Unsupervised Common Event Discovery	INTERNATIONAL JOURNAL OF COMPUTER VISION			English	Article						Common event discovery; Synchrony discovery; Video indexing; Event detection; Human interaction; Unsupervised learning; Global optimization; Branch and bound; Bag-of-words	FACIAL EXPRESSION; DYNAMICS; IMAGE	Event discovery aims to discover a temporal segment of interest, such as human behavior, actions or activities. Most approaches to event discovery within or between time series use supervised learning. This becomes problematic when relevant event labels are unknown, are difficult to detect, or not all possible combinations of events have been anticipated. To overcome these problems, this paper explores Common Event Discovery (CED), a new problem that aims to discover common events of variable-length segments in an unsupervised manner. A potential solution to CED is searching over all possible pairs of segments, which would incur a prohibitive quartic cost. In this paper, we propose an efficient branch-and-bound (B&B) framework that avoids exhaustive search while guaranteeing a globally optimal solution. To this end, we derive novel bounding functions for various commonality measures and provide extensions to multiple commonality discovery and accelerated search. The B&B framework takes as input any multidimensional signal that can be quantified into histograms. A generalization of the framework can be readily applied to discover events at the same or different times (synchrony and event commonality, respectively). We consider extensions to video search and supervised event detection. The effectiveness of the B&B framework is evaluated in motion capture of deliberate behavior and in video of spontaneous facial behavior in diverse interpersonal contexts: interviews, small groups of young adults, and parent-infant face-to-face interaction.	[Chu, Wen-Sheng; De la Torre, Fernando; Cohn, Jeffrey] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Messinger, Daniel S.] Univ Miami, Dept Psychol, POB 248185, Coral Gables, FL 33124 USA		Chu, WS (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.	wschu@cmu.edu; ftorre@cs.cmu.edu; jeffcohn@pitt.edu; dmessinger@miami.edu	Chu, Wen-Sheng/AAF-6871-2019	Chu, Wen-Sheng/0000-0001-8592-6088	US National Institutes of Health [GM105004, MH096951]; NATIONAL INSTITUTE OF GENERAL MEDICAL SCIENCES [R01GM105004] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH096951] Funding Source: NIH RePORTER	US National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NATIONAL INSTITUTE OF GENERAL MEDICAL SCIENCES(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of General Medical Sciences (NIGMS)); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))	This work was supported in part by US National Institutes of Health grants GM105004 and MH096951. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Institutes of Health. The authors would like to thank Feng Zhou and Jiabei Zeng for helping partial experiments.	Amberg B., 2011, ICCV; Chaaraoui AA, 2012, EXPERT SYST APPL, V39, P10873, DOI 10.1016/j.eswa.2012.03.005; [Anonymous], 2003, P ICCV; Balakrishnan V., 1991, International Journal of Robust and Nonlinear Control, V1, P295, DOI 10.1002/rnc.4590010404; Barbic J, 2004, PROC GRAPH INTERF, P185; Bartlett M. S., 2006, Journal of Multimedia, V1, DOI 10.4304/jmm.1.6.22-35; Begum N, 2014, PROC VLDB ENDOW, V8, P149, DOI 10.14778/2735471.2735476; Boiman O., 2005, ICCV; Brand M, 1997, CVPR; Brendel W, 2011, IEEE I CONF COMP VIS, P778, DOI 10.1109/ICCV.2011.6126316; Chu W.-S., 2012, ECCV; Chu W. S., 2015, ICCV; Chu W. S., 2010, P ACCV; Chu W. S., 2016, TPAMI; Cooper IL, 2009, CVPR; De la Torre F., 2015, AUTOMATIC FACE GESTU; Delaherche E, 2012, IEEE T AFFECT COMPUT, V3, P349, DOI 10.1109/T-AFFC.2012.12; Ding X., 2012, ICCV; Du SC, 2014, P NATL ACAD SCI USA, V111, pE1454, DOI 10.1073/pnas.1322355111; Duchenne O., 2009, ICCV; Everingham M., 2006, 2 PASCAL CHALLENGE; Feris R., 2014, ICMR; Gao L, 2015, CVPR; GENDRON B, 1994, OPER RES, V42, P1042, DOI 10.1287/opre.42.6.1042; Girard J. M., 2015, AFGR; Goldberger J., 2003, ICCV; Gusfield D, 1997, ALGORITHMS STRINGS T; Han D., 2009, ICCV; Hoai M., 2011, CVPR; Hongeng S., 2001, ICCV; Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710; Jhuang H., 2007, ICCV; Keogh E, 2005, KNOWL INF SYST, V7, P358, DOI 10.1007/s10115-004-0154-9; Kruger S. E., 2005, INTERSPEECH; Kuipers B., 2011, CVPR; Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144; Laptev I., 2008, P CVPR; Lehmann A, 2011, INT J COMPUT VISION, V94, P175, DOI 10.1007/s11263-010-0342-x; Littlewort G, 2006, IMAGE VISION COMPUT, V24, P615, DOI 10.1016/j.imavis.2005.09.011; Liu CD, 2010, IEEE T INF TECHNOL B, V14, P1236, DOI 10.1109/TITB.2010.2052061; Liu H.R., 2010, P CVPR; Lucey Patrick, 2010, CVPRW; MAIER D, 1978, J ACM, V25, P322, DOI 10.1145/322063.322075; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Messinger DM, 2010, NEURAL NETWORKS, V23, P1004, DOI 10.1016/j.neunet.2010.08.008; Messinger DS, 2009, INFANCY, V14, P285, DOI 10.1080/15250000902839963; Minnen D., 2007, KDD; Mukherjee L., 2011, P CVPR; NARENDRA P, 1977, IEEE T COMPUT, V26, P917, DOI 10.1109/tc.1977.1674939; Nayak S, 2012, J MACH LEARN RES, V13, P2589; Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684; Paterson M., 1994, Mathematical Foundations of Computer Science 1994. 19th International Symposium, MFCS'94. Proceedings, P127; Platt JC, 2000, ADV NEUR IN, P61; Reddy KK, 2013, MACH VISION APPL, V24, P971, DOI 10.1007/s00138-012-0450-4; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Sadanand S., 2012, P 2012 IEEE C COMP V; Sangineto E., 2014, P ACMMM; Sayette MA, 2012, PSYCHOL SCI, V23, P869, DOI 10.1177/0956797611435134; Schindler G., 2008, P CVPR; Schmidt RC, 2012, J NONVERBAL BEHAV, V36, P263, DOI 10.1007/s10919-012-0138-5; SCHOLKOPF B, 2001, NIPS; Schuller B., 2006, INTERSPEECH; Si Z., 2011, ICCV; Sun M., 2012, CVPR; Turaga P, 2009, COMPUT VIS IMAGE UND, V113, P353, DOI 10.1016/j.cviu.2008.08.009; Valstar Michel, 2006, CVPRW; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang HX, 2014, WIRES DATA MIN KNOWL, V4, P24, DOI 10.1002/widm.1110; Wang Y., 2009, ICASSP; Wang Y., 2006, P CVPR; YANG Y, 2013, PAMI, V35, P1635, DOI DOI 10.1109/TPAMI.2012.253; Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731; Yu X., 2013, AUTOMATIC FACE GESTU; Yuan JS, 2009, PROC CVPR IEEE, P2442, DOI [10.1109/CVPRW.2009.5206671, 10.1109/CVPR.2009.5206671]; Zheng Y., 2011, ACMMM; Zhou F., 2010, P CVPR; Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	79	5	5	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0920-5691	1573-1405		INT J COMPUT VISION	Int. J. Comput. Vis.	JUL	2017	123	3					372	391		10.1007/s11263-017-0989-7	http://dx.doi.org/10.1007/s11263-017-0989-7			20	Computer Science, Artificial Intelligence	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science	EX9EO	28943718	Green Accepted			2022-10-03	WOS:000403559600004
J	Chu, WS; De la Torre, F; Cohn, JF				Chu, Wen-Sheng; De la Torre, Fernando; Cohn, Jeffrey			Selective Transfer Machine for Personalized Facial Expression Analysis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Facial expression analysis; personalization; domain adaptation; transfer learning; support vector machine (SVM)	SUPPORT VECTOR MACHINE; ACTION UNIT DETECTION; RECOGNITION; OPTIMIZATION; ADAPTATION; FEATURES; EMOTION; MODELS; FACE	Automatic facial action unit (AU) and expression detection from videos is a long-standing problem. The problem is challenging in part because classifiers must generalize to previously unknown subjects that differ markedly in behavior and facial morphology (e. g., heavy versus delicate brows, smooth versus deeply etched wrinkles) from those on which the classifiers are trained. While some progress has been achieved through improvements in choices of features and classifiers, the challenge occasioned by individual differences among people remains. Person-specific classifiers would be a possible solution but for a paucity of training data. Sufficient training data for person-specific classifiers typically is unavailable. This paper addresses the problem of how to personalize a generic classifier without additional labels from the test subject. We propose a transductive learning method, which we refer to as a Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific mismatches. STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. We compared STM to both generic classifiers and cross-domain learning methods on four benchmarks: CK+ [44], GEMEP-FERA [67], RUFACS [4] and GFT [57]. STM outperformed generic classifiers in all.	[Chu, Wen-Sheng; De la Torre, Fernando; Cohn, Jeffrey] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Chu, WS (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.	wensheng.chu@gmail.com; ftorre@cs.cmu.edu; jeffcohn@cs.cmu.edu	Chu, Wen-Sheng/AAF-6871-2019	Chu, Wen-Sheng/0000-0001-8592-6088	National Institutes of Health (NIH) [R01MH096951]; National Science Foundation (NSF) [RI-1116583]; Army Research Laboratory Collaborative Technology Alliance Program [W911NF-10-2-0016]; Direct For Computer & Info Scie & Enginr [1418026] Funding Source: National Science Foundation; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH096951] Funding Source: NIH RePORTER	National Institutes of Health (NIH)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea); Army Research Laboratory Collaborative Technology Alliance Program; Direct For Computer & Info Scie & Enginr(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))	The authors would like thank many anonymous reviewers for constructive feedback. Research reported in this paper was supported in part by the National Institutes of Health (NIH) under Award Number R01MH096951, the National Science Foundation (NSF) under the grant RI-1116583, and Army Research Laboratory Collaborative Technology Alliance Program under cooperative agreement W911NF-10-2-0016. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH or the NSF.	AUMANN RJ, 1986, ISRAEL J MATH, V54, P159, DOI 10.1007/BF02764940; Aytar Y, 2011, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2011.6126504; Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100; Bartlett M. S., 2006, Journal of Multimedia, V1, DOI 10.4304/jmm.1.6.22-35; Bihan Jiang, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P314, DOI 10.1109/FG.2011.5771416; Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242; Bruzzone L, 2010, IEEE T PATTERN ANAL, V32, P770, DOI 10.1109/TPAMI.2009.57; Burleson W, 2005, ACM INT C MULT, P672, DOI DOI 10.1145/1101149.1101300; Chan C. F., 2010, P EUR SIGN PROC C EU, P1, DOI DOI 10.1109/APPEEC.2010.5448850; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chang KY, 2009, PROC CVPR IEEE, P533, DOI 10.1109/CVPRW.2009.5206612; Chang Y, 2006, IMAGE VISION COMPUT, V24, P605, DOI 10.1016/j.imavis.2005.08.006; Chapelle O, 2007, NEURAL COMPUT, V19, P1155, DOI 10.1162/neco.2007.19.5.1155; Chattopadhyay R, 2012, ACM T KNOWL DISCOV D, V6, DOI 10.1145/2382577.2382582; Chen JX, 2013, PATTERN RECOGN LETT, V34, P1964, DOI 10.1016/j.patrec.2013.02.002; Chew SW, 2012, IEEE T SYST MAN CY B, V42, P1006, DOI 10.1109/TSMCB.2012.2194485; Chew Sien W, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P915, DOI 10.1109/FG.2011.5771373; Collobert R, 2006, J MACH LEARN RES, V7, P1687; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Daume' H., 2007, P 45 ANN M ASS COMPU, P256; Ding XY, 2013, IEEE I CONF COMP VIS, P2400, DOI 10.1109/ICCV.2013.298; Duan Lixin, 2012, IEEE Trans Neural Netw Learn Syst, V23, P504, DOI 10.1109/TNNLS.2011.2178556; Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114; Dudik M., 2005, ADV NEURAL INFORM PR, V18, P323; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; FLOUDAS CA, 1990, COMPUT CHEM ENG, V14, P1397, DOI 10.1016/0098-1354(90)80020-C; Gehrig T., 2011, IEEE COMP SOC C COMP, P1; Girard JM, 2015, BEHAV RES METHODS, V47, P1136, DOI 10.3758/s13428-014-0536-1; Goldberg D. A., 2009, U.S. Patent, Patent No. [7,561,723, 7561723]; Gorski J, 2007, MATH METHOD OPER RES, V66, P373, DOI 10.1007/s00186-007-0161-1; Gretton A, 2009, NEURAL INF PROCESS S, P131; Gunes H, 2005, IEEE SYS MAN CYBERN, P3437; Guo YM, 2012, LECT NOTES COMPUT SC, V7573, P631, DOI 10.1007/978-3-642-33709-3_45; Jeni L. A., 2015, P IEEE INT C AUT FAC; Jixu Chen, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2655, DOI 10.1109/CVPRW.2009.5206580; Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200; Kanade T, 2000, P 4 IEEE INT C AUT F, P46, DOI [10.1109/AFGR.2000.840611, DOI 10.1109/AFGR.2000.840611]; Khosla A, 2012, LECT NOTES COMPUT SC, V7572, P158, DOI 10.1007/978-3-642-33718-5_12; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; la Torre FD, 2011, VISUAL ANAL HUMANS, P377; Littlewort G, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P298, DOI 10.1109/FG.2011.5771414; Liu M., 2013, 10 IEEE INT C WORKSH, P1, DOI [DOI 10.1109/FG.2013.6553734, DOI 10.1128/GEN0MEA.00300-13]; Lucas Bruce D., 1981, INT JOINT C ART INT; Lucey P., 2010, 2010 IEEE COMPUTER S, P94, DOI [10.1109/CVPRW.2010.5543262, DOI 10.1109/CVPRW.2010.5543262]; Lucey S., 2007, FACE RECOGNITION, P275; Martinez A, 2012, J MACH LEARN RES, V13, P1589; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Orrite C, 2009, LECT NOTES COMPUT SC, V5524, P176, DOI 10.1007/978-3-642-02172-5_24; Pantic M, 2007, FACE RECOGNITION, P377, DOI [10.5772/4847, DOI 10.5772/4847]; Quionero-Candela Joaquin, 2009, DATASET SHIFT MACHIN; Rudovic O, 2015, IEEE T PATTERN ANAL, V37, P944, DOI 10.1109/TPAMI.2014.2356192; Rudovic O, 2012, LECT NOTES COMPUT SC, V7584, P260, DOI 10.1007/978-3-642-33868-7_26; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Sangineto E, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P357, DOI 10.1145/2647868.2654916; Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127; Sayette MA, 2012, PSYCHOL SCI, V23, P869, DOI 10.1177/0956797611435134; Senechal T, 2012, IEEE T SYST MAN CY B, V42, P993, DOI 10.1109/TSMCB.2012.2193567; Shang LF, 2009, PROC CVPR IEEE, P2090, DOI 10.1109/CVPRW.2009.5206509; Sikka K, 2012, LECT NOTES COMPUT SC, V7584, P250, DOI 10.1007/978-3-642-33868-7_25; Simon T, 2010, PROC CVPR IEEE, P2737, DOI 10.1109/CVPR.2010.5539998; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Sugiyama M., 2007, NIPS, P1433, DOI DOI 10.1109/FG.2013.6553717; Sugiyama M, 2007, J MACH LEARN RES, V8, P985; Tong Y, 2007, IEEE T PATTERN ANAL, V29, P1683, DOI 10.1109/TPAMI.2007.1094; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P966, DOI 10.1109/TSMCB.2012.2200675; Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P28, DOI 10.1109/TSMCB.2011.2163710; van Gestel T, 2004, MACH LEARN, V54, P5, DOI 10.1023/B:MACH.0000008082.80494.e0; WANG ZH, 2013, PROC CVPR IEEE, P3422, DOI DOI 10.1109/CVPR.2013.439; Watson P., 2013, 2013 10 IEEE INT C W, P1; WENDELL RE, 1976, OPER RES, V24, P643, DOI 10.1287/opre.24.4.643; Whitehill J., 2013, SOC EMOTIONS NATURE, V88; Wu T., 2010, P 2010 IEEE COMP SOC, P42; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yamada M, 2012, LECT NOTES COMPUT SC, V7575, P674, DOI 10.1007/978-3-642-33765-9_48; Yang J, 2007, P 15 ACM INT C MULT, P188, DOI DOI 10.1145/1291233.1291276; Yang P, 2010, PROC CVPR IEEE, P2638, DOI 10.1109/CVPR.2010.5539978; Yang S, 2014, LECT NOTES COMPUT SC, V8888, P269, DOI 10.1007/978-3-319-14364-4_26; Zen Gloria, 2014, P 16 INT C MULTIMODA, P128; Zeng JB, 2015, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2015.413; Zhang H., 2006, 2006 IEEE COMP SOC C, V2, P2126, DOI DOI 10.1109/CVPR.2006.301; Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110; Zhao K., 2015, P IEEE C COMP VIS PA; Zhong L., 2012, P IEEE C COMP VIS PA, P1499; Zhou F, 2010, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2010.5539966; Zhu YF, 2011, IEEE T AFFECT COMPUT, V2, P79, DOI 10.1109/T-AFFC.2011.10	86	101	104	2	32	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2017	39	3					529	545		10.1109/TPAMI.2016.2547397	http://dx.doi.org/10.1109/TPAMI.2016.2547397			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	EM8IP	28113267	Green Accepted, hybrid			2022-10-03	WOS:000395555100009
J	Jeni, LA; Cohn, JF; Kanade, T				Jeni, Laszlo A.; Cohn, Jeffrey; Kanade, Takeo			Dense 3D face alignment from 2D video for real-time use	IMAGE AND VISION COMPUTING			English	Article						3D face alignment; Dense 3D model; Real-time method	ACTIVE APPEARANCE MODELS; HEAD; TRACKING; ROBUST; RECONSTRUCTION; REGISTRATION	To enable real-time, person-independent 3D registration from 2D video, we developed a 3D cascade regression approach in which facial landmarks remain invariant across pose over a range of approximately 60 degrees. From a single 2D image of a person's face, a dense 3D shape is registered in real time for each frame. The algorithm utilizes a fast cascade regression framework trained on high-resolution 3D face-scans of posed and spontaneous emotion expression. The algorithm first estimates the location of a dense set of landmarks and their visibility, then reconstructs face shapes by fitting a part-based 3D model. Because no assumptions are required about illumination or surface properties, the method can be applied to a wide range of imaging conditions that include 2D video and uncalibrated multi-view video. The method has been validated in a battery of experiments that evaluate its precision of 3D reconstruction, extension to multi-view reconstruction, temporal integration for videos and 3D head-pose estimation. Experimental findings strongly support the validity of real-time, 3D registration and reconstruction from 2D video. The software is available online at http://zface.org. (C) 2016 Elsevier B.V. All rights reserved.	[Jeni, Laszlo A.; Cohn, Jeffrey; Kanade, Takeo] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Jeni, LA (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.	laszlojem@cmu.edu		Jeni, Laszlo A./0000-0002-2830-700X	National Institute of Mental Health of the National Institutes of Health [MH096951]; Army Research Laboratory Collaborative Technology Alliance Program [W911NF-10-2-0016]; Carnegie Mellon University People Image Analysis Consortium; Direct For Computer & Info Scie & Enginr [1205195] Funding Source: National Science Foundation	National Institute of Mental Health of the National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); Army Research Laboratory Collaborative Technology Alliance Program; Carnegie Mellon University People Image Analysis Consortium; Direct For Computer & Info Scie & Enginr(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	Preparation of this publication was supported in part by the National Institute of Mental Health of the National Institutes of Health under Award Number MH096951, Army Research Laboratory Collaborative Technology Alliance Program under cooperative agreement W911NF-10-2-0016, and the Carnegie Mellon University People Image Analysis Consortium. The content is solely the responsibility of the authors and does not necessarily represent the official views of the sponsors.	Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311; An KH, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P307, DOI 10.1109/IROS.2008.4650742; Asteriadis S., 2009, P INT WORKSH AFF AW, P1, DOI 10.1145/1655260.1655261; Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240; Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396; Baltrusaitis T, 2012, PROC CVPR IEEE, P2610, DOI 10.1109/CVPR.2012.6247980; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Boker SM, 2011, J EXP PSYCHOL HUMAN, V37, P874, DOI 10.1037/a0021928; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222; Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204; Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012; Cao XD, 2012, PROC CVPR IEEE, P2887, DOI 10.1109/CVPR.2012.6248015; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Cristinacce D, 2008, PATTERN RECOGN, V41, P3054, DOI 10.1016/j.patcog.2008.01.024; Dantone M., 2012, REAL TIME FACIAL FEA; Dimitrijevic M, 2004, PROC CVPR IEEE, P1034; Dollar P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094; Dryden K. V. M. I. L., 1998, STAT SHAPE ANAL; Ekman P, 2002, RES NEXUS; Girard JM, 2015, BEHAV RES METHODS, V47, P1136, DOI 10.3758/s13428-014-0536-1; Gu L., 2006, COMP VIS PATT REC 20, V1, P1305, DOI DOI 10.1109/CVPR.2006.11; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hassner T, 2013, IEEE I CONF COMP VIS, P3607, DOI 10.1109/ICCV.2013.448; Jie Zhang, 2014, Computer Vision - ECCV 2014. 13th European Conference. Proceedings: LNCS 8690, P1, DOI 10.1007/978-3-319-10605-2_1; Kanaujia A, 2007, IEEE IMAGE PROC, P265; KAZEMI V, 2014, PROC CVPR IEEE, P1867, DOI DOI 10.1109/CVPR.2014.241; Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63; Kobbelt L, 2000, COMP GRAPH, P103; Kumano S, 2009, INT J COMPUT VISION, V83, P178, DOI 10.1007/s11263-008-0185-x; La Cascia M, 2000, IEEE T PATTERN ANAL, V22, P322, DOI 10.1109/34.845375; Levy B., 2006, GEOM SHAP MOD APPL 2; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Martinez B, 2013, IEEE T PATTERN ANAL, V35, P1149, DOI 10.1109/TPAMI.2012.205; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106; Ng AY, 2002, ADV NEUR IN, V14, P849; Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218; Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Sung J, 2008, INT J COMPUT VISION, V80, P260, DOI 10.1007/s11263-007-0125-1; Suwajanakorn S, 2014, LECT NOTES COMPUT SC, V8692, P796, DOI 10.1007/978-3-319-10593-2_52; Tena JR, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964971; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Tulyakov S., 2015, COMP VIS ICCV 2015 I, V1; Valenti R, 2012, IEEE T IMAGE PROCESS, V21, P802, DOI 10.1109/TIP.2011.2162740; Valstar M, 2010, PROC CVPR IEEE, P2729, DOI 10.1109/CVPR.2010.5539996; Vicente F, 2015, IEEE T INTELL TRANSP, V16, P2014, DOI 10.1109/TITS.2015.2396031; Xiao J, 2003, INT J IMAG SYST TECH, V13, P85, DOI 10.1002/ima.10048; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yin LJ, 2008, IEEE INT CONF AUTOMA, P116; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhang ZY, 2004, INT J COMPUT VISION, V58, P93, DOI 10.1023/B:VISI.0000015915.50080.85	53	48	49	2	13	ELSEVIER	AMSTERDAM	RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS	0262-8856	1872-8138		IMAGE VISION COMPUT	Image Vis. Comput.	FEB	2017	58						13	24		10.1016/j.imavis.2016.05.009	http://dx.doi.org/10.1016/j.imavis.2016.05.009			12	Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Optics	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering; Optics	EN2MN	29731533	Green Accepted			2022-10-03	WOS:000395844700003
S	Chow, SM; Ou, L; Cohn, JF; Messinger, DS		VonDavier, AA; Zhu, M; Kyllonen, PC		Chow, Sy-Miin; Ou, Lu; Cohn, Jeffrey; Messinger, Daniel S.			Representing Self-organization and Nonstationarities in Dyadic Interaction Processes Using Dynamic Systems Modeling Techniques	INNOVATIVE ASSESSMENT OF COLLABORATION	Methodology of Educational Measurement and Assessment		English	Article; Book Chapter						Dynamic systems; Time-varying coefficients; Self-organization; Dyadic interaction; Generalized additive model; Face-to-face/still-face; Nonstationarity	MOTHER-INFANT INTERACTION; TIME-SERIES; VARIABILITY; SYNCHRONY; SPLINES; FACE	Dynamic systems modeling techniques provide a convenient platform for representing multidimensional and multidirectional change processes over time. Central to dynamic systems models is the notion that a system may show emergent properties that allow the system to self-organize into qualitatively distinct states through temporal fluctuations in selected key parameters of interest. Using computer vision-based measurement of smiling in one infant-mother dyad's interactions during a face-to-face interaction, we illustrate the use of generalized additive modeling techniques to fit multivariate dynamic systems models with self-organizing, time-moderated dynamic parameters. We found evidence for systematic over-time changes in the infant. mother cross-regression effect, which provided a glimpse into how the dyad self-organized into distinct states over the course of the interaction, including periods where the mother's positivity was reinforced and strengthened by the infant's positivity, as well as periods where the mother's positivity was inversely related to the infant's past positivity levels.	[Chow, Sy-Miin; Ou, Lu] Penn State Univ, State Coll, PA 16801 USA; [Cohn, Jeffrey] Univ Pittsburgh, Pittsburgh, PA USA; [Messinger, Daniel S.] Univ Miami, Coral Gables, FL 33124 USA		Chow, SM (corresponding author), Penn State Univ, State Coll, PA 16801 USA.	symiin@psu.edu; lzo114@psu.edu; jeffcohn@pitt.edu; dmessinger@miami.edu						Ahmad I, 2005, ANN STAT, V33, P258, DOI 10.1214/009053604000000931; Ainsworth Mary D. S., 1978, PATTERNS ATTACHMENT; Akaike H., 1973, P 2 INT S INF THEOR, P267, DOI 10.1007/978-1-4612-1694-0; BARTON S, 1994, AM PSYCHOL, V49, P5, DOI 10.1037/0003-066X.49.1.5; Brazelton BT., 1974, EFFECT INFANT ITS CA, P137; Chatfield C., 2004, ANAL TIME SERIES INT; Chow SM, 2011, MULTIVAR BEHAV RES, V46, P303, DOI 10.1080/00273171.2011.563697; Chow SM, 2010, EMOTION, V10, P101, DOI 10.1037/a0017824; Chow SM, 2009, MULTIVAR BEHAV RES, V44, P465, DOI 10.1080/00273170903103324; Cohen J., 2003, APPL MULTIPLE REGRES; de Jong P, 1998, J AM STAT ASSOC, V93, P796, DOI 10.2307/2670129; de Weerth C, 2002, INFANT BEHAV DEV, V24, P347; Del Negro M., 2008, FEDERAL RESERVE BANK, P1; Durbin J., 2012, TIME SERIES ANAL STA; Ekas NV, 2013, DEV PSYCHOL, V49, P1027, DOI 10.1037/a0029330; Ekman P., 1978, FACIAL ACTION CODING; Feldman R, 1996, J APPL DEV PSYCHOL, V17, P347, DOI 10.1016/S0193-3973(96)90031-3; Feldman R, 1997, INF MENTAL HLTH J, V18, P4, DOI 10.1002/(SICI)1097-0355(199721)18:1<4::AID-IMHJ2>3.0.CO;2-R; FOGEL A, 1987, DEV PSYCHOL, V23, P747, DOI 10.1037/0012-1649.23.6.747; GREEN P J, 1994, NONPARAMETRIC REGRES; Haken H., 1977, SYNERGETICS INTRO NO; Halpin PF, 2013, PSYCHOMETRIKA, V78, P793, DOI 10.1007/s11336-013-9329-1; Hamaker EL, 2005, MULTIVAR BEHAV RES, V40, P207, DOI 10.1207/s15327906mbr4002_3; HASTIE T, 1993, J ROY STAT SOC B MET, V55, P757; Hastie T. J., 1990, GEN ADDITIVE MODELS, DOI DOI 10.1201/9780203753781; Henderson R. K., 2012, FRONT PSYCHOL, V3, P484; Heywood I, 2006, INTRO GEOGRAPHICAL I; ISABELLA RA, 1991, CHILD DEV, V62, P373, DOI 10.2307/1131010; Jaffe J, 2001, MONOGR SOC RES CHILD, V66, P1, DOI 10.1111/1540-5834.00137; James GM, 2002, J ROY STAT SOC B, V64, P411, DOI 10.1111/1467-9868.00342; Kelso J.S.., 1995, DYNAMIC PATTERNS SEL; Kochanska G, 1999, INFANT BEHAV DEV, V22, P249, DOI 10.1016/S0163-6383(99)00009-0; Landry SH, 1997, DEV PSYCHOL, V33, P1040, DOI 10.1037/0012-1649.33.6.1040; MacCallum RC, 1997, MULTIVAR BEHAV RES, V32, P215, DOI 10.1207/s15327906mbr3203_1; McCullagh P., 1989, GEN LINEAR MODELS, V37; McKeown GJ, 2014, PSYCHOL METHODS, V19, P155, DOI 10.1037/a0034282; Messinger DS, 2009, INFANCY, V14, P285, DOI 10.1080/15250000902839963; Mislevy R. J., 2014, PSYCHOMETRIC CONSIDE; Molenaar P. C. M., 1994, LATENT VARIABLES ANA, P155; Molenaar PCM., 2004, MEASUREMENT, V2, P201, DOI [10.1207/s15366359mea0204_1, DOI 10.1207/S15366359MEA0204_1]; Moustaki I, 2000, APPL PSYCH MEAS, V24, P211, DOI 10.1177/01466210022031679; PAGAN A, 1980, J ECONOMETRICS, V13, P341, DOI 10.1016/0304-4076(80)90084-6; Smith LB, 1993, DYNAMIC SYSTEMS APPR; Soller A., 2007, ADV LATENT VARIABLE; Stern Daniel., 1985, INTERPERSONAL WORLD; Stock J. H., 2008, METHODOLOGY PRACTICE; Tarvainen MP, 2006, PHYSIOL MEAS, V27, P225, DOI 10.1088/0967-3334/27/3/002; THELEN E, 1989, SYSTEMS DEV MINNESOT, V22, P77; TRONICK EZ, 1989, AM PSYCHOL, V44, P112, DOI 10.1037/0003-066X.44.2.112; Tronick EZ., 1986, ZERO 3, V6, P1, DOI DOI 10.1038/S41390-021-01451-4; Wang X, 2013, BIOMETRIKA, V100, P955, DOI 10.1093/biomet/ast031; Weiss A. A., 1985, J TIME SER ANAL, V6, P181, DOI DOI 10.1111/j.1467-9892.1985.tb00408.x; Wood S. N., 2017, GEN ADDITIVE MODELS, Vsecond, DOI [10.1201/9781315370279, DOI 10.1201/9781315370279]; Wood SN, 2003, J ROY STAT SOC B, V65, P95, DOI 10.1111/1467-9868.00374	54	3	4	0	0	SPRINGER INTERNATIONAL PUBLISHING AG	CHAM	GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND	2367-170X		978-3-319-33261-1; 978-3-319-33259-8	METHOD EDUC MEAS			2017							269	286		10.1007/978-3-319-33261-1_17	http://dx.doi.org/10.1007/978-3-319-33261-1_17	10.1007/978-3-319-33261-1		18	Education & Educational Research	Book Citation Index – Social Sciences & Humanities (BKCI-SSH)	Education & Educational Research	BJ1EW					2022-10-03	WOS:000417624500018
C	Chu, WS; De la Torre, F; Cohn, JF			IEEE	Chu, Wen-Sheng; De la Torre, Fernando; Cohn, Jeffrey			Learning Spatial and Temporal Cues for Multi-label Facial Action Unit Detection	2017 12TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG 2017)	IEEE International Conference on Automatic Face and Gesture Recognition and Workshops		English	Proceedings Paper	12th IEEE International Conference on Automatic Face and Gesture Recognition (FG)	MAY 30-JUN 03, 2017	Washington, DC	IEEE, Baidu, Mitsubishi Elect Res Labs Inc, 3dMD, DI4D, Syst & Technol Res, ObjectVideo Labs, MUKH Technologies, IEEE Comp Soc			EXPRESSIONS; EMOTION	Facial action units (AU) are the fundamental units to decode human facial expressions. At least three aspects affect performance of automated AU detection: spatial representation, temporal modeling, and AU correlation. Unlike most studies that tackle these aspects separately, we propose a hybrid network architecture to jointly model them. Specifically, spatial representations are extracted by a Convolutional Neural Network (CNN), which, as analyzed in this paper, is able to reduce person-specific biases caused by hand-crafted descriptors (e.g., HOG and Gabor). To model temporal dependencies, Long Short-Term Memory (LSTMs) are stacked on top of these representations, regardless of the lengths of input videos. The outputs of CNNs and LSTMs are further aggregated into a fusion network to produce per-frame prediction of 12 AUs. Our network naturally addresses the three issues together, and yields superior performance compared to existing methods that consider these issues independently. Extensive experiments were conducted on two large spontaneous datasets, GFT and BP4D, with more than 400,000 frames coded with 12 AUs. On both datasets, we report improvements over a standard multi-label CNN and feature-based state-of-the-art. Finally, we provide visualization of the learned AU models, which, to our best knowledge, reveal how machines see AUs for the first time.	[Chu, Wen-Sheng; De la Torre, Fernando; Cohn, Jeffrey] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Chu, WS (corresponding author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.		Chu, Wen-Sheng/AAF-6871-2019	Chu, Wen-Sheng/0000-0001-8592-6088	US National Institutes of Health [GM105004, MH096951]; NVIDIA	US National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NVIDIA	This work was supported in part by US National Institutes of Health grants GM105004 and MH096951. The authors also thank NVIDIA for supporting this research with a Tesla K40c GPU, and Jiabei Zeng and Kaili Zhao for assisting partial experiments.	Chang K.-Y., 2009, CVPR; Chen J, 2009, PROC CVPR IEEE, P156, DOI 10.1109/CVPRW.2009.5206832; Chu Wen-Sheng, 2013, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, V2013, P3515; Cohn JF, 2010, BEHAV RES METHODS, V42, P1079, DOI 10.3758/BRM.42.4.1079; De la Torre F., 2015, AUTOMATIC FACE GESTU; Ding X., 2013, IEEE C INT C COMP VI; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Du SC, 2015, DIALOGUES CLIN NEURO, V17, P443; Ekman R., 1997, WHAT FACE REVEALS BA; Eleftheriadis S., 2015, ICCV; Ghosh S., 2015, ACII; Graves A., 2013, ICASSP; Gudi A., 2015, AFGR; Jaiswal S., 2016, DEEP LEARNING DYNAMI; Jia Y., 2014, P 22 ACM INT C MULT; Jiang B., 2011, AFGR; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Krizhevsky A., P ADV NEURAL INFORM, DOI 10.1145/3065386; la Torre FD, 2011, VISUAL ANAL HUMANS, P377; LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115; Liu P., 2014, CVPR; Lucey P., 2010, 2010 IEEE COMPUTER S, P94, DOI [10.1109/CVPRW.2010.5543262, DOI 10.1109/CVPRW.2010.5543262]; Martinez A, 2012, J MACH LEARN RES, V13, P1589; Parkhi O.M., 2015, P BR MACH VIS, V1, P6, DOI 10.5244/C.29.41; Rudovic O, 2015, IEEE T PATTERN ANAL, V37, P944, DOI 10.1109/TPAMI.2014.2356192; Sangineto E., 2014, ACM MM; Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127; Simonyan K., 2014, ARXIV; Simonyan K., 2014, 2 INT C LEARN REPR I; Taigman Y, 2015, PROC CVPR IEEE, P2746, DOI 10.1109/CVPR.2015.7298891; Tong Y, 2007, IEEE T PATTERN ANAL, V29, P1683, DOI 10.1109/TPAMI.2007.1094; Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P966, DOI 10.1109/TSMCB.2012.2200675; Wang Z., 2013, ICCV; WONG AKC, 1985, IEEE T PATTERN ANAL, V7, P599, DOI 10.1109/TPAMI.1985.4767707; Wu BY, 2015, PATTERN RECOGN, V48, P2279, DOI 10.1016/j.patcog.2015.01.022; Wu Z., 2015, ACM MM; Yang S, 2014, LECT NOTES COMPUT SC, V8888, P269, DOI 10.1007/978-3-319-14364-4_26; Yosinski J., 2015, UNDERSTANDING NEURAL; Zeng J., 2015, ICCV; Zhang X., 2013, AFGR; Zhao KL, 2015, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2015.7298833; Zhong L., 2012, CVPR; Zhu YF, 2011, IEEE T AFFECT COMPUT, V2, P79, DOI 10.1109/T-AFFC.2011.10	43	60	61	2	4	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2326-5396		978-1-5090-4023-0	IEEE INT CONF AUTOMA			2017							25	32		10.1109/FG.2017.13	http://dx.doi.org/10.1109/FG.2017.13			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering; Imaging Science & Photographic Technology	BI7KC					2022-10-03	WOS:000414287400004
C	Girard, JM; Chu, WS; Jeni, LA; Cohn, JF; De la Torre, F; Sayette, MA			IEEE	Girard, Jeffrey M.; Chu, Wen-Sheng; Jeni, Laszlo A.; Cohn, Jeffrey; De la Torre, Fernando; Sayette, Michael A.			Sayette Group Formation Task (GFT) Spontaneous Facial Expression Database	2017 12TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG 2017)	IEEE International Conference on Automatic Face and Gesture Recognition and Workshops		English	Proceedings Paper	12th IEEE International Conference on Automatic Face and Gesture Recognition (FG)	MAY 30-JUN 03, 2017	Washington, DC	IEEE, Baidu, Mitsubishi Elect Res Labs Inc, 3dMD, DI4D, Syst & Technol Res, ObjectVideo Labs, MUKH Technologies, IEEE Comp Soc			KAPPA	Despite the important role that facial expressions play in interpersonal communication and our knowledge that interpersonal behavior is influenced by social context, no currently available facial expression database includes multiple interacting participants. The Sayette Group Formation Task (GFT) database addresses the need for well-annotated video of multiple participants during unscripted interactions. The database includes 172,800 video frames from 96 participants in 32 three-person groups. To aid in the development of automated facial expression analysis systems, GFT includes expert annotations of FACS occurrence and intensity, facial landmark tracking, and baseline results for linear SVM, deep learning, active patch learning, and personalized classification. Baseline performance is quantified and compared using identical partitioning and a variety of metrics (including means and confidence intervals). The highest performance scores were found for the deep learning and active patch learning methods. Learn more at http://osf.io/7wcyz.	[Girard, Jeffrey M.; Cohn, Jeffrey; Sayette, Michael A.] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Chu, Wen-Sheng; Jeni, Laszlo A.; Cohn, Jeffrey; De la Torre, Fernando] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA		Girard, JM (corresponding author), Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA.		Chu, Wen-Sheng/AAF-6871-2019; Girard, Jeffrey M. M/H-4088-2019	Chu, Wen-Sheng/0000-0001-8592-6088; Girard, Jeffrey M. M/0000-0002-7359-3746	NIH grant [MH096951]; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH096951] Funding Source: NIH RePORTER	NIH grant(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))	This research was supported in part by NIH grant MH096951. The Tesla K40 GPU used in this research was donated by the NVIDIA Corporation.	Adolphs R, 2009, ANNU REV PSYCHOL, V60, P693, DOI 10.1146/annurev.psych.60.110707.163514; Altman D, 1991, PRACTICAL STAT MED R, DOI [10.1201/9780429258589, DOI 10.1201/9780429258589]; [Anonymous], 2013, DIAGN STAT MAN MENT, V5th, DOI 10.1176/appi.books.9780890425596; Bennett FM, 1954, PUBLIC OPIN QUART, V18, P303; BRENNAN RL, 1981, EDUC PSYCHOL MEAS, V41, P687, DOI 10.1177/001316448104100307; Chu W.-S., 2016, IEEE T PATTERN ANAL; CICCHETTI DV, 1990, J CLIN EPIDEMIOL, V43, P551, DOI 10.1016/0895-4356(90)90159-M; Cohn JF., 2005, NEW HDB METHODS NONV, P9; Cohn JF, 2014, HDB AFFECTIVE COMPUT; Cumming G, 2001, EDUC PSYCHOL MEAS, V61, P532, DOI 10.1177/00131640121971374; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409; Drummond C, 2006, MACH LEARN, V65, P95, DOI 10.1007/s10994-006-8199-5; Ekman P, 2002, RES NEXUS; Fan RE, 2008, J MACH LEARN RES, V9, P1871; Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010; Fridlund AJ., 1994, HUMAN FACIAL EXPRESS, DOI 10.1016/b978-0-12-267630-7.50008-3; Ghosh S., 2015, ACII; Girard J., 2015, FG; Girard JM, 2015, BEHAV RES METHODS, V47, P1136, DOI 10.3758/s13428-014-0536-1; He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239; Jeni L.A., 2016, IMAGE VISION COMPUTI; Jia Y., 2014, P 22 ACM INT C MULTI, P675, DOI [10.1145/2647868.2654889, DOI 10.1145/2647868.2654889]; Kilem L, 2014, HDB INTERRATER RELIA; Krizhevsky A., 2012, P NIPS, P1097; Lucey P, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P57, DOI 10.1109/FG.2011.5771462; Mairal J, 2012, FOUND TRENDS COMPUT, V8, DOI 10.1561/0600000058; Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4; McDuff D, 2013, IEEE COMPUT SOC CONF, P881, DOI 10.1109/CVPRW.2013.130; McKeown G, 2012, IEEE T AFFECT COMPUT, V3, P5, DOI 10.1109/T-AFFC.2011.20; Sayette MA, 2012, PSYCHOL SCI, V23, P869, DOI 10.1177/0956797611435134; Valstar M., 2015, FG; Valstar M, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1243, DOI 10.1145/2647868.2647869; Zeng JB, 2016, IEEE T IMAGE PROCESS, V25, P4753, DOI 10.1109/TIP.2016.2594486; Zhang X., 2013, FG; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhang Z, 2016, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2016.374; Zhao KL, 2016, IEEE T IMAGE PROCESS, V25, P3931, DOI 10.1109/TIP.2016.2570550; Zhao X., 2012, COMMUNICATION YB, P418; Zhong L., 2012, CVPR	40	20	20	3	3	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2326-5396		978-1-5090-4023-0	IEEE INT CONF AUTOMA			2017							581	588		10.1109/FG.2017.144	http://dx.doi.org/10.1109/FG.2017.144			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering; Imaging Science & Photographic Technology	BI7KC	29606916	Green Accepted, Green Submitted			2022-10-03	WOS:000414287400079
C	Hammal, Z; Chu, WS; Cohn, JF; Heike, C; Speltz, ML			IEEE	Hammal, Zakia; Chu, Wen-Sheng; Cohn, Jeffrey; Heike, Carrie L.; Speltz, Matthew L.			Automatic Action Unit Detection in Infants Using Convolutional Neural Network	2017 SEVENTH INTERNATIONAL CONFERENCE ON AFFECTIVE COMPUTING AND INTELLIGENT INTERACTION (ACII)	International Conference on Affective Computing and Intelligent Interaction		English	Proceedings Paper	7th International Conference on Affective Computing and Intelligent Interaction (ACII)	OCT 23-26, 2017	San Antonio, TX				FACIAL EXPRESSION; RECOGNITION	Action unit detection in infants relative to adults presents unique challenges. Jaw contour is less distinct, facial texture is reduced, and rapid and unusual facial movements are common. To detect facial action units in spontaneous behavior of infants, we propose a multi-label Convolutional Neural Network (CNN). Eighty-six infants were recorded during tasks intended to elicit enjoyment and frustration. Using an extension of FACS for infants (Baby FACS), over 230,000 frames were manually coded for ground truth. To control for chance agreement, inter-observer agreement between Baby-FACS coders was quantified using free-margin kappa. Kappa coefficients ranged from 0.79 to 0.93, which represents high agreement. The multi-label CNN achieved comparable agreement with manual coding. Kappa ranged from 0.69 to 0.93. Importantly, the CNN-based AU detection revealed the same change in findings with respect to infant expressiveness between tasks. While further research is needed, these findings suggest that automatic AU detection in infants is a viable alternative to manual coding of infant facial expression.	[Hammal, Zakia; Chu, Wen-Sheng; Cohn, Jeffrey] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Heike, Carrie L.] Seattle Childrens Hosp, Seattle, WA USA; [Speltz, Matthew L.] Univ Washington, Sch Med, Seattle, WA USA		Hammal, Z (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.		Heike, Carrie L./ABA-4852-2021; Chu, Wen-Sheng/AAF-6871-2019	Heike, Carrie L./0000-0003-2178-7730; Chu, Wen-Sheng/0000-0001-8592-6088	National Institute of Health [DE026513, DE022438, MH096951]; NATIONAL INSTITUTE OF DENTAL & CRANIOFACIAL RESEARCH [R03DE026513, R01DE022438] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH096951] Funding Source: NIH RePORTER	National Institute of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NATIONAL INSTITUTE OF DENTAL & CRANIOFACIAL RESEARCH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Dental & Craniofacial Research (NIDCR)); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))	Research reported in this paper was supported in part by the National Institute of Health under Award Numbers DE026513, DE022438 and MH096951. We also thank NVidia for providing a Tesla K40c GPU to support this research.	Corneanu CA, 2016, IEEE T PATTERN ANAL, V38, P1548, DOI 10.1109/TPAMI.2016.2515606; Bartlett M. S., 2006, Journal of Multimedia, V1, DOI 10.4304/jmm.1.6.22-35; BRENNAN RL, 1981, EDUC PSYCHOL MEAS, V41, P687, DOI 10.1177/001316448104100307; Camras LA, 1998, DEV PSYCHOL, V34, P616, DOI 10.1037/0012-1649.34.4.616; Chang K.-Y., 2009, COMPUTER VISION PATT; Chew SW, 2012, IEEE T SYST MAN CY B, V42, P1006, DOI 10.1109/TSMCB.2012.2194485; Chu WS, 2017, IEEE T PATTERN ANAL, V39, P529, DOI 10.1109/TPAMI.2016.2547397; Chu Wen-Sheng, 2017, AUTOMATIC FACE GESTU; Cohn J. F., 1991, DEV PSYCHOPATHOL, V3, P367, DOI [DOI 10.1017/S0954579400007574, 10.1017/S0954579400007574]; Ding XY, 2016, IMAGE VISION COMPUT, V51, P36, DOI 10.1016/j.imavis.2016.03.008; Ekman P, 2002, RES NEXUS; Girard J. M., 2017, AUTOMATIC FACE GESTU; Goldsmith H. H., 1999, LAB TEMPERAMENT ASSE; Ijjina E. P., 2014, INT C MACH LEARN APP; Izard C. E., 1983, SYSTEM IDENTIFYING A; Izard CE, 1983, MAXIMALLY DISCRIMINA; Jaiswal S, 2016, IEEE WINT CONF APPL; Jeni Laszlo A., 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163142; Jia Y., 2014, P 22 ACM INT C MULT; Krizhevsky A., P ADV NEURAL INFORM, DOI 10.1145/3065386; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li XY, 2005, IEEE T SYST MAN CY A, V35, P93, DOI 10.1109/TSMCA.2004.838454; Lien JJJ, 2000, ROBOT AUTON SYST, V31, P131, DOI 10.1016/S0921-8890(99)00103-7; Lucey S., 2007, FACE RECOGNITION; MATIAS R, 1993, DEV PSYCHOL, V29, P524, DOI 10.1037/0012-1649.29.3.524; Mattson WI, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0080161; Messinger DS, 2012, EMOTION, V12, P430, DOI 10.1037/a0026498; OSTER H, 2000, MONOGRAPH CODI UNPUB; Pantic M, 2006, IEEE T SYST MAN CY B, V36, P433, DOI 10.1109/TSMCB.2005.859075; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127; Simon T., 2010, COMPUTER VISION PATT; Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962; Tong Y, 2007, IEEE T PATTERN ANAL, V29, P1683, DOI 10.1109/TPAMI.2007.1094; Tong Y, 2010, IEEE T PATTERN ANAL, V32, P258, DOI 10.1109/TPAMI.2008.293; Tsalakanidou F, 2010, PATTERN RECOGN, V43, P1763, DOI 10.1016/j.patcog.2009.12.009; Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P28, DOI 10.1109/TSMCB.2011.2163710; Walecki R., 2015, IEEE C WORKSHOPS AUT, P1; Wang Z., 2013, IEEE INT C COMP VIS; Yang S, 2014, LECT NOTES COMPUT SC, V8888, P269, DOI 10.1007/978-3-319-14364-4_26; Zaker N., 2014, INT C IM PROC; Zeng JB, 2016, IEEE T IMAGE PROCESS, V25, P4753, DOI 10.1109/TIP.2016.2594486; Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52; Zhang X., 2013, AUTOMATIC FACE GESTU; Zhao K., 2016, COMPUTER VISION PATT	45	8	8	1	2	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2156-8103		978-1-5386-0563-9	INT CONF AFFECT			2017							216	221						6	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Engineering, Electrical & Electronic	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering	BJ8FU	29862131	Green Accepted			2022-10-03	WOS:000428165700033
C	Oliu, M; Corneanu, C; Jeni, LA; Cohn, JF; Kanade, T; Escalera, S		Lai, SH; Lepetit, V; Nishino, K; Sato, Y		Oliu Simon, Marc; Corneanu, Ciprian; Jeni, Laszlo A.; Cohn, Jeffrey; Kanade, Takeo; Escalera Guerrero, Sergio			Continuous Supervised Descent Method for Facial Landmark Localisation	COMPUTER VISION - ACCV 2016, PT II	Lecture Notes in Computer Science		English	Proceedings Paper	13th Asian Conference on Computer Vision (ACCV)	NOV 20-24, 2016	Taipei, TAIWAN				FACE ALIGNMENT	Recent methods for facial landmark location perform well on close-to-frontal faces but have problems in generalising to large head rotations. In order to address this issue we propose a second order linear regression method that is both compact and robust against strong rotations. We provide a closed form solution, making the method fast to train. We test the method's performance on two challenging datasets. The first has been intensely used by the community. The second has been specially generated from a well known 3D face dataset. It is considerably more challenging, including a high diversity of rotations and more samples than any other existing public dataset. The proposed method is compared against state-of-the-art approaches, including RCPR, CGPRT, LBF, CFSS, and GSDM. Results upon both datasets show that the proposed method offers state-of-the-art performance on near frontal view data, improves state-of-the-art methods on more challenging head rotation problems and keeps a compact model size.	[Oliu Simon, Marc] Univ Oberta Catalunya, 156 Rambla del Poblenou, Barcelona, Spain; [Corneanu, Ciprian; Escalera Guerrero, Sergio] Univ Barcelona, 585 Gran Via de les Corts Catalanes, Barcelona, Spain; [Jeni, Laszlo A.; Cohn, Jeffrey; Kanade, Takeo] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Oliu Simon, Marc; Corneanu, Ciprian; Escalera Guerrero, Sergio] Comp Vis Ctr, O Bldg,UAB Campus, Bellaterra, Spain; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Oliu, M (corresponding author), Univ Oberta Catalunya, 156 Rambla del Poblenou, Barcelona, Spain.; Oliu, M (corresponding author), Comp Vis Ctr, O Bldg,UAB Campus, Bellaterra, Spain.	moliusimon@gmail.com	Escalera Guerrero, Sergio/L-2998-2015	Escalera Guerrero, Sergio/0000-0003-0617-8873	FI-DGR fellowship; Knowledge and Economy Department of the Generalitat de Catalunya; Spanish project [TIN2013-43478-P]; European Comission; U.S. National Institutes of Health [MH096951]	FI-DGR fellowship; Knowledge and Economy Department of the Generalitat de Catalunya; Spanish project(Spanish Government); European Comission(European CommissionEuropean Commission Joint Research Centre); U.S. National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	The work of Marc Oliu is supported by the FI-DGR 2016 fellowship, granted by the Universities and Research Secretary of the Knowledge and Economy Department of the Generalitat de Catalunya. This work has been partially supported by the Spanish project TIN2013-43478-P, the European Comission Horizon 2020 granted project SEE.4C under call H2020-ICT-2015 and the U.S. National Institutes of Health under the grant MH096951.	Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715; Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3; COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Corneanu C. A., 2016, T PATTERN ANAL MACH; Dollar P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Jeni Laszlo A., 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163142; Jie Zhang, 2014, Computer Vision - ECCV 2014. 13th European Conference. Proceedings: LNCS 8690, P1, DOI 10.1007/978-3-319-10605-2_1; KAZEMI V, 2014, PROC CVPR IEEE, P1867, DOI DOI 10.1109/CVPR.2014.241; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Lee D, 2015, PROC CVPR IEEE, P4204, DOI 10.1109/CVPR.2015.7299048; Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218; Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Xiong XH, 2015, PROC CVPR IEEE, P2664, DOI 10.1109/CVPR.2015.7298882; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yin LJ, 2008, IEEE INT CONF AUTOMA, P116; Zhou Bolei, 2014, C NEUR INF PROC SYST, DOI DOI 10.1162/153244303322533223; Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014	23	1	1	0	0	SPRINGER INTERNATIONAL PUBLISHING AG	CHAM	GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND	0302-9743	1611-3349	978-3-319-54184-6; 978-3-319-54183-9	LECT NOTES COMPUT SC			2017	10112						121	135		10.1007/978-3-319-54184-6_8	http://dx.doi.org/10.1007/978-3-319-54184-6_8			15	Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory & Methods	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BJ5RC					2022-10-03	WOS:000426207700008
C	Valstar, MF; Sanchez-Lozano, E; Cohn, JF; Jeni, LA; Girard, JM; Zhang, Z; Yin, LJ; Pantic, M			IEEE	Valstar, Michel F.; Sanchez-Lozano, Enrique; Cohn, Jeffrey; Jeni, Laszlo A.; Girard, Jeffrey M.; Zhang, Zheng; Yin, Lijun; Pantic, Maja			FERA 2017-Addressing Head Pose in the Third Facial Expression Recognition and Analysis Challenge	2017 12TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION (FG 2017)	IEEE International Conference on Automatic Face and Gesture Recognition and Workshops		English	Proceedings Paper	12th IEEE International Conference on Automatic Face and Gesture Recognition (FG)	MAY 30-JUN 03, 2017	Washington, DC	IEEE, Baidu, Mitsubishi Elect Res Labs Inc, 3dMD, DI4D, Syst & Technol Res, ObjectVideo Labs, MUKH Technologies, IEEE Comp Soc			3D; FEATURES	The field of Automatic Facial Expression Analysis has grown rapidly in recent years. However, despite progress in new approaches as well as benchmarking efforts, most evaluations still focus on either posed expressions, near-frontal recordings, or both. This makes it hard to tell how existing expression recognition approaches perform under conditions where faces appear in a wide range of poses (or camera views), displaying ecologically valid expressions. The main obstacle for assessing this is the availability of suitable data, and the challenge proposed here addresses this limitation. The FG 2017 Facial Expression Recognition and Analysis challenge (FERA 2017) extends FERA 2015 to the estimation of Action Units occurrence and intensity under different camera views. In this paper we present the third challenge in automatic recognition of facial expressions, to be held in conjunction with the 12th IEEE conference on Face and Gesture Recognition, May 2017, in Washington, United States. Two sub-challenges are defined: the detection of AU occurrence, and the estimation of AU intensity. In this work we outline the evaluation protocol, the data used, and the results of a baseline method for both sub-challenges.	[Valstar, Michel F.; Sanchez-Lozano, Enrique] Univ Nottingham, Sch Comp Sci, Nottingham, England; [Cohn, Jeffrey; Girard, Jeffrey M.] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA; [Zhang, Zheng; Yin, Lijun] Binghamton Univ, Dept Comp Sci, Binghamton, NY USA; [Pantic, Maja] Imperial Coll London, Dept Comp, London, England; [Pantic, Maja] Univ Twente, Elect Engn Math & Comp Sci, Enschede, Netherlands; [Cohn, Jeffrey; Jeni, Laszlo A.] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA		Valstar, MF (corresponding author), Univ Nottingham, Sch Comp Sci, Nottingham, England.		Girard, Jeffrey M. M/H-4088-2019	Girard, Jeffrey M. M/0000-0002-7359-3746; Sanchez-Lozano, Enrique/0000-0003-0196-922X	National Science Foundation [CNS-1629898, CNS-1629716, CNS-1205664, CNS-1205195, HS-1051103, HS-1051169]; National Institutes of Health [MH 096951]; European Union [645378]; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH096951] Funding Source: NIH RePORTER	National Science Foundation(National Science Foundation (NSF)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); European Union(European Commission); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))	Support was provided in part by National Science Foundation awards CNS-1629898, CNS-1629716, CNS-1205664, CNS-1205195, HS-1051103, and HS-1051169, National Institutes of Health award MH 096951, and the European Union's Horizon 2020 research and innovation programme under grant agreement No. 645378.	Corneanu CA, 2016, IEEE T PATTERN ANAL, V38, P1548, DOI 10.1109/TPAMI.2016.2515606; Almaev T, 2015, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2015.430; Bartlett M. S., 2006, Journal of Multimedia, V1, DOI 10.4304/jmm.1.6.22-35; Bazzo JJ, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P505, DOI 10.1109/AFGR.2004.1301583; Chew SW, 2012, IEEE T SYST MAN CY B, V42, P1006, DOI 10.1109/TSMCB.2012.2194485; Ekman P, 2002, RES NEXUS; Girard JM, 2015, PATTERN RECOGN LETT, V66, P13, DOI 10.1016/j.patrec.2014.10.004; Gonzalez-Jimenez D, 2007, IEEE T INF FOREN SEC, V2, P413, DOI 10.1109/TIFS.2007.903543; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Hamm J, 2011, J NEUROSCI METH, V200, P237, DOI 10.1016/j.jneumeth.2011.06.023; Jaiswal Shashank, 2016, WACV; Jeni L.A., 2016, IMAGE VISION COMPUTI; Jeni LA, 2013, IEEE INT CONF AUTOMA; Kaltwang S, 2012, LECT NOTES COMPUT SC, V7432, P368, DOI 10.1007/978-3-642-33191-6_36; Kim M, 2010, LECT NOTES COMPUT SC, V6313, P649; Lucey P, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P57, DOI 10.1109/FG.2011.5771462; Mahoor Mohammad H., 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P74, DOI 10.1109/CVPR.2009.5204259; Martinez Brais, 2017, TAC; Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4; McKeown G, 2015, EMOT REV, V7, P30, DOI 10.1177/1754073914544475; Pereira F., 2001, P 18 INT C MACHINE L, V1, P282; Powers DMW, 2011, J MACHINE LEARNING T, V2, P37; Sanchez-Lozano E., 2016, CORR; Sanchez-Lozano E, 2016, LECT NOTES COMPUT SC, V9912, P645, DOI 10.1007/978-3-319-46484-8_39; Savran A, 2012, IMAGE VISION COMPUT, V30, P774, DOI 10.1016/j.imavis.2011.11.008; Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6; Shen J., 2015, P IEEE INT C COMP VI; SHROUT PE, 1979, PSYCHOL BULL, V86, P420, DOI 10.1037/0033-2909.86.2.420; Tian YL, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P229, DOI 10.1109/AFGR.2002.1004159; Valstar M., 2014, P 2014 WORKSH ROADM, P57; Valstar M., 2010, PROC 3 INT WORKSHOP, P65; Valstar M.F., 2015, 11 IEEE INT C WORKSH, V06, P1; Valstar M. F., 2011, IEEE INT C AUT FAC G; Wu T., 2012, IEEE T SYSTEMS MAN B; Yang P, 2009, PATTERN RECOGN LETT, V30, P132, DOI 10.1016/j.patrec.2008.03.014; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhang Z, 2016, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2016.374	37	56	57	0	8	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	2326-5396		978-1-5090-4023-0	IEEE INT CONF AUTOMA			2017							839	847		10.1109/FG.2017.107	http://dx.doi.org/10.1109/FG.2017.107			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Engineering; Imaging Science & Photographic Technology	BI7KC	29606917	Green Submitted, Green Accepted			2022-10-03	WOS:000414287400115
J	Zeng, JB; Chu, WS; De la Torre, F; Cohn, JF; Xiong, Z				Zeng, Jiabei; Chu, Wen-Sheng; De la Torre, Fernando; Cohn, Jeffrey; Xiong, Zhang			Confidence Preserving Machine for Facial Action Unit Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING			English	Article						Transfer learning; semi-supervised learning; support vector machine (SVM); confident classifiers; self-paced learning; easy-to-hard; facial action unit (AU) detection	DOMAIN ADAPTATION; EXPRESSION RECOGNITION; REGULARIZATION; ROBUST	Facial action unit (AU) detection from video has been a long-standing problem in the automated facial expression analysis. While progress has been made, accurate detection of facial AUs remains challenging due to ubiquitous sources of errors, such as inter-personal variability, pose, and low-intensity AUs. In this paper, we refer to samples causing such errors as hard samples, and the remaining as easy samples. To address learning with the hard samples, we propose the confidence preserving machine (CPM), a novel two-stage learning framework that combines multiple classifiers following an "easy-to-hard" strategy. During the training stage, CPM learns two confident classifiers. Each classifier focuses on separating easy samples of one class from all else, and thus preserves confidence on predicting each class. During the test stage, the confident classifiers provide "virtual labels" for easy test samples. Given the virtual labels, we propose a quasi-semi-supervised (QSS) learning strategy to learn a person-specific classifier. The QSS strategy employs a spatio-temporal smoothness that encourages similar predictions for samples within a spatio-temporal neighborhood. In addition, to further improve detection performance, we introduce two CPM extensions: iterative CPM that iteratively augments training samples to train the confident classifiers, and kernel CPM that kernelizes the original CPM model to promote nonlinearity. Experiments on four spontaneous data sets GFT, BP4D, DISFA, and RU-FACS illustrate the benefits of the proposed CPM models over baseline methods and the state-of-the-art semi-supervised learning and transfer learning methods.	[Zeng, Jiabei; Xiong, Zhang] Beihang Univ, Engn Res Ctr Adv Comp Applicat Technol, Minist Educ, Beijing 100191, Peoples R China; [Zeng, Jiabei; Xiong, Zhang] Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China; [Chu, Wen-Sheng; De la Torre, Fernando; Cohn, Jeffrey] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Zeng, JB (corresponding author), Beihang Univ, Engn Res Ctr Adv Comp Applicat Technol, Minist Educ, Beijing 100191, Peoples R China.	zjb1990@gmail.com; wensheng.chu@gmail.com; ftorre@cs.cmu.edu; jeffcohn@cs.cmu.edu	Zeng, Jiabei/J-3865-2016; Chu, Wen-Sheng/AAF-6871-2019	Zeng, Jiabei/0000-0003-3256-4524; Chu, Wen-Sheng/0000-0001-8592-6088	National Natural Science Foundation of China [61272350]; State Key Laboratory of Software Development Environment [SKLSDE-2016ZX-24]; U.S. National Institutes of Health [MH096951]; National Institutes of Mental Health [R21 MH099487-01A1]; China Scholarship Council; Fundamental Research Funds for the Central Universities; NATIONAL INSTITUTE OF MENTAL HEALTH [R01MH096951, R21MH099487] Funding Source: NIH RePORTER	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); State Key Laboratory of Software Development Environment; U.S. National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Institutes of Mental Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); China Scholarship Council(China Scholarship Council); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); NATIONAL INSTITUTE OF MENTAL HEALTH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))	This work was supported in part by the National Natural Science Foundation of China under 61272350, in part by the State Key Laboratory of Software Development Environment under Grant SKLSDE-2016ZX-24, in part by the U.S. National Institutes of Health under Grant MH096951, and in part by the National Institutes of Mental Health under Grant R21 MH099487-01A1. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. The work of J. Zeng was supported in part by the China Scholarship Council, and in part by the Fundamental Research Funds for the Central Universities. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. David Clausi.	Almaev T, 2015, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2015.430; Andrews S., 2002, NEURIPS, P577; Bartlett M. S., 2006, Journal of Multimedia, V1, DOI 10.4304/jmm.1.6.22-35; Bartlett PL, 2008, J MACH LEARN RES, V9, P1823; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Bruzzone L, 2010, IEEE T PATTERN ANAL, V32, P770, DOI 10.1109/TPAMI.2009.57; Chapelle O., 2005, P 10 INT WORKSH ART, P57; Chapelle O., 2006, SEMISUPERVISED LEARN; Chapelle O, 2007, NEURAL COMPUT, V19, P1155, DOI 10.1162/neco.2007.19.5.1155; Chen JN, 2013, KNOWL-BASED SYST, V45, P1, DOI 10.1016/j.knosys.2013.01.031; Chen K, 2011, IEEE T PATTERN ANAL, V33, P129, DOI 10.1109/TPAMI.2010.92; Chu Wen-Sheng, 2013, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, V2013, P3515; Cohn J. F., 2014, OXFORD HDB AFFECTIVE; Cohn JF, 2010, BEHAV RES METHODS, V42, P1079, DOI 10.3758/BRM.42.4.1079; De la Torre F., 2015, P 11 IEEE INT C WORK, P1; Ding XY, 2013, IEEE I CONF COMP VIS, P2400, DOI 10.1109/ICCV.2013.298; Dornaika F, 2015, PATTERN RECOGN, V48, P3714, DOI 10.1016/j.patcog.2015.05.018; Dornaika F, 2008, INT J COMPUT VISION, V76, P257, DOI 10.1007/s11263-007-0059-7; Du SC, 2014, P NATL ACAD SCI USA, V111, pE1454, DOI 10.1073/pnas.1322355111; Duan L., 2009, P ANN INT C MACH LEA, P289, DOI [10.1145/1553374.1553411, DOI 10.1145/1553374.1553411]; Duan LX, 2012, IEEE T NEUR NET LEAR, V23, P504, DOI 10.1109/TNNLS.2011.2178556; Duan LX, 2012, PROC CVPR IEEE, P1338, DOI 10.1109/CVPR.2012.6247819; Ekman R., 1997, WHAT FACE REVEALS BA; Fan RE, 2008, J MACH LEARN RES, V9, P1871; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Girard Jeffrey M., 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163106; Girard JM, 2015, PATTERN RECOGN LETT, V66, P13, DOI 10.1016/j.patrec.2014.10.004; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Grandvalet Y., 2009, ADV NEURAL INFORM PR, P537; Grandvalet Yves, 2005, ADV NEURAL INFORM PR, V367, P281; Gretton A, 2009, NEURAL INF PROCESS S, P131; Guo GD, 2013, IEEE T AFFECT COMPUT, V4, P291, DOI 10.1109/T-AFFC.2013.13; Jayadeva, 2007, IEEE T PATTERN ANAL, V29, P905, DOI 10.1109/TPAMI.2007.1068; Jhuo IH, 2012, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2012.6247924; Kumano S, 2009, INT J COMPUT VISION, V83, P178, DOI 10.1007/s11263-008-0185-x; Kumar MPawan, 2010, NIPS, P1189; Li YQ, 2013, IEEE T AFFECT COMPUT, V4, P127, DOI 10.1109/T-AFFC.2013.5; Liu W, 2012, P IEEE, V100, P2624, DOI 10.1109/JPROC.2012.2197809; Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4; Melacci S, 2011, J MACH LEARN RES, V12, P1149; Muandet K., 2013, PROC ICML, P10; Pfister T, 2011, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2011.6126401; Rosenberg C, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P29; Rudovic O, 2015, IEEE T PATTERN ANAL, V37, P944, DOI 10.1109/TPAMI.2014.2356192; Sangineto E, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P357, DOI 10.1145/2647868.2654916; Sun Q., 2011, NIPS2011, P505; Taheri S, 2013, IEEE T AFFECT COMPUT, V4, P360, DOI 10.1109/T-AFFC.2013.28; Wang SJ, 2015, LECT NOTES COMPUT SC, V8925, P325, DOI 10.1007/978-3-319-16178-5_23; Whitehill J., 2014, SOCIAL EMOTIONS NATU; Yang J, 2007, P 15 ACM INT C MULT, P188, DOI DOI 10.1145/1291233.1291276; Yang S, 2014, LECT NOTES COMPUT SC, V8888, P269, DOI 10.1007/978-3-319-14364-4_26; Zeng JB, 2015, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2015.413; [张醒 Zhang Xing], 2013, [火工品, Initiators & Pyrotechnics], P1; Zhao KL, 2015, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2015.7298833; Zhu X., 2011, ENCY MACHINE LEARNIN, P892, DOI DOI 10.1007/978-0-387-30164-8_749; Zhu YF, 2011, IEEE T AFFECT COMPUT, V2, P79, DOI 10.1109/T-AFFC.2011.10	59	6	6	1	24	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	1057-7149	1941-0042		IEEE T IMAGE PROCESS	IEEE Trans. Image Process.	OCT	2016	25	10					4753	4767		10.1109/TIP.2016.2594486	http://dx.doi.org/10.1109/TIP.2016.2594486			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DV1JR	27479964	Green Accepted			2022-10-03	WOS:000382677700001
J	Corneanu, CA; Simon, MO; Cohn, JF; Guerrero, SE				Adrian Corneanu, Ciprian; Oliu Simon, Marc; Cohn, Jeffrey; Escalera Guerrero, Sergio			Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-Related Applications	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Facial expression; affect; emotion recognition; RGB; 3D; thermal; multimodal	AUTOMATIC FACE SEGMENTATION; EMOTION RECOGNITION; ANIMATION PARAMETERS; BODY; MODEL; AUDIO; VIDEO; EXTRACTION; SEQUENCES; DATABASE	Facial expressions are an important way through which humans interact socially. Building a system capable of automatically recognizing facial expressions from images and video has been an intense field of study in recent years. Interpreting such expressions remains challenging and much research is needed about the way they relate to human affect. This paper presents a general overview of automatic RGB, 3D, thermal and multimodal facial expression analysis. We define a new taxonomy for the field, encompassing all steps from face detection to facial expression recognition, and describe and classify the state of the art methods accordingly. We also present the important datasets and the bench-marking of most influential methods. We conclude with a general discussion about trends, important questions and future lines of research.	[Adrian Corneanu, Ciprian; Oliu Simon, Marc; Escalera Guerrero, Sergio] Univ Autonoma Barcelona, Comp Vis Ctr, E-08193 Barcelona, Spain; [Adrian Corneanu, Ciprian; Oliu Simon, Marc; Escalera Guerrero, Sergio] Univ Barcelona, Dept Appl Math, E-08007 Barcelona, Spain; [Cohn, Jeffrey] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA; [Cohn, Jeffrey] Univ Pittsburgh, Dept Psychol, Pittsburgh, PA 15260 USA		Corneanu, CA (corresponding author), Univ Autonoma Barcelona, Comp Vis Ctr, E-08193 Barcelona, Spain.; Corneanu, CA (corresponding author), Univ Barcelona, Dept Appl Math, E-08007 Barcelona, Spain.	cipriancorneanu@ub.edu; moliusimon@gmail.com; jeffcohn@cs.cmu.edu; sergio@maia.ub.es	Escalera Guerrero, Sergio/L-2998-2015	Escalera Guerrero, Sergio/0000-0003-0617-8873				Abadi Mojtaba Khomami, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163100; Aleksic PS, 2006, IEEE T INF FOREN SEC, V1, P3, DOI 10.1109/TIFS.2005.863510; Alyuz N, 2012, LECT NOTES COMPUT SC, V7585, P557, DOI 10.1007/978-3-642-33885-4_56; Alyuz N, 2010, IEEE T INF FOREN SEC, V5, P425, DOI 10.1109/TIFS.2010.2054081; Ambadar Z, 2009, J NONVERBAL BEHAV, V33, P17, DOI 10.1007/s10919-008-0059-5; [Anonymous], 2014, P 16 INT C MULT INT; [Anonymous], 2016, DATASET 02 IRIS THER; Ashraf AB, 2009, IMAGE VISION COMPUT, V27, P1788, DOI 10.1016/j.imavis.2009.05.007; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Bakkes S., 2012, PROCEEDINGS OF THE 8, V3, P4, DOI DOI 10.1145/2336727.2336731; Barrett LF, 2011, CURR DIR PSYCHOL SCI, V20, P400, DOI 10.1177/0963721411429125; Bartlett M. S., 2006, Journal of Multimedia, V1, DOI 10.4304/jmm.1.6.22-35; Batrinca L.M., 2011, P 13 INT C MULT INT, P255, DOI [10.1145/2070481.2070528, DOI 10.1145/2070481.2070528]; Berretti S, 2011, VISUAL COMPUT, V27, P1021, DOI 10.1007/s00371-011-0611-x; Besl P. J., 1992, ROBOTICS DL TENTATIV, V1611, P239, DOI DOI 10.1109/34.121791; Biel JI, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P119, DOI 10.1145/2522848.2522877; Biel JI, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P53; Biel JI, 2013, IEEE T MULTIMEDIA, V15, P41, DOI 10.1109/TMM.2012.2225032; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Blom P. M., 2014, P 10 AAAI C ART INT, P30; Bosch N., 2015, P 20 INT C INTELLIGE, P379; Burrows A., 2014, HDB BIOMETRICS, P1; Busso C., 2004, P INT C MULT INT, P205; CARIDAKIS G., 2006, P INT C MULT INT, P146, DOI DOI 10.1145/1180995.1181029; Castellano G, 2013, INT J HUM ROBOT, V10, DOI 10.1142/S0219843613500102; Chai D, 1999, IEEE T CIRC SYST VID, V9, P551, DOI 10.1109/76.767122; Chang Y, 2005, LECT NOTES COMPUT SC, V3723, P293; Chastel A., 2002, LEONARDO ART ARTIST; Chen LS, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P366, DOI 10.1109/AFGR.1998.670976; Cohen I, 2003, PROC CVPR IEEE, P595; Cohen I, 2003, COMPUT VIS IMAGE UND, V91, P160, DOI 10.1016/S1077-3142(03)00081-X; Cohn Jeffrey F., 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPR.2009.5204260; Cohn J. F., 2004, INT J WAVELETS MULTI, V2, P121, DOI [DOI 10.1142/S021969130400041X, 10.1142/S021969130400041X]; Colombo A, 2006, PATTERN RECOGN, V39, P444, DOI 10.1016/j.patcog.2005.09.009; COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; D'Mello SK, 2010, USER MODEL USER-ADAP, V20, P147, DOI 10.1007/s11257-010-9074-4; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dantone M, 2012, PROC CVPR IEEE, P2578, DOI 10.1109/CVPR.2012.6247976; Dapogny Arnaud, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163111; Darwin C., 1872, P374; De la Torre F., 2011, VISUAL ANAL HUMANS L, P377; De Silva L. C., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P332, DOI 10.1109/AFGR.2000.840655; De Silva LC, 1997, ICICS - PROCEEDINGS OF 1997 INTERNATIONAL CONFERENCE ON INFORMATION, COMMUNICATIONS AND SIGNAL PROCESSING, VOLS 1-3, P397, DOI 10.1109/ICICS.1997.647126; DeVault D, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1061; Dhall Abhinav, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P878, DOI 10.1109/FG.2011.5771366; Dhall A., 2011, TRCS11 AUSTR NAT U C, V2, P1; Dhall A., 2014, P 16 INT C MULTIMODA, P461, DOI [10.1145/2663204.2666275, DOI 10.1145/2663204.2666275]; Dhall A, 2015, IEEE INT CONF AUTOMA; Dhall A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS); Dhall A, 2012, INT C PATT RECOG, P3525; Duchenne de Boulogne G. -B, 1990, MECH HUMAN FACIAL EX; Duric Z, 2002, P IEEE, V90, P1272, DOI 10.1109/JPROC.2002.801449; Eibl-Eibesfeldt I., 1989, HUMAN ETHOLOGY; EKMAN P, 1979, ANNU REV PSYCHOL, V30, P527, DOI 10.1146/annurev.ps.30.020179.002523; EKMAN P, 1994, PSYCHOL BULL, V115, P268, DOI 10.1037/0033-2909.115.2.268; EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068; EKMAN P, 1990, J PERS SOC PSYCHOL, V58, P342, DOI 10.1037/0022-3514.58.2.342; Ekman P., 1993, HIL0984 U CAL; Ekman P., 2002, FACIAL ACTION CODING; Ekman P., 2005, WHAT FACE REVEALS BA, V2nd; Ekman P., 1971, NEBRASKA S MOTIVATIO, V19, P207; Ekman P, 1978, FACIAL ACTION CODING; el Kaliouby R, 2005, REAL-TIME VISION FOR HUMAN-COMPUTER INTERACTION, P181; Fanelli G, 2013, IEEE INT CONF AUTOMA; Fasel B, 2003, PATTERN RECOGN, V36, P259, DOI 10.1016/S0031-3203(02)00052-3; Fragopanagos N, 2005, NEURAL NETWORKS, V18, P389, DOI 10.1016/j.neunet.2005.03.006; Fridlund A. J., 1997, EMOTION, P90; Friesen W. V., 1983, EMFACS 7 EMOTI UNPUB, V2, P36; Frijda NH., 1997, PSYCHOL FACIAL EXPRE, P78, DOI [10.1017/cbo9780511659911.006, DOI 10.1017/CBO9780511659911.006]; Geetha A, 2009, EXPERT SYST APPL, V36, P303, DOI 10.1016/j.eswa.2007.09.002; Gehrig T, 2013, P 2013 EM REC WILD C, P9, DOI DOI 10.1145/2531923.2531924; Girard JM, 2015, BEHAV RES METHODS, V47, P1136, DOI 10.3758/s13428-014-0536-1; Girard JM, 2015, PATTERN RECOGN LETT, V66, P13, DOI 10.1016/j.patrec.2014.10.004; Girard JM, 2014, IMAGE VISION COMPUT, V32, P641, DOI 10.1016/j.imavis.2013.12.007; Gong BQ., 2009, P 17 ACM INT C MULT, P569, DOI 10.3969/j.issn.1000-6729.2011.01.011; Gray H., 1966, ANATOMY HUMAN BODY; Greenblatt S., 1994, UNIVERSAL LANGUAGE M, V21, P56; Greenwald M. K., 1989, J PSYCHOPHYSIOL, V3, P51; Gross I. R., 2008, P IEEE INT C AUT FAC, P1; Gu WF, 2012, PATTERN RECOGN, V45, P80, DOI 10.1016/j.patcog.2011.05.006; Gunes H, 2005, IEEE SYS MAN CYBERN, P3437; Haggard E.A., 1966, METHODS RES PSYCHOTH, P154; Hammal Z, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P47, DOI 10.1145/2388676.2388688; Hao Tang, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563052; Hayat M, 2013, IEEE WORK APP COMP, P83, DOI 10.1109/WACV.2013.6475003; He L., 2015, P 5 INT WORKSHOP AUD, P73, DOI [10.1145/2808196.2811641, DOI 10.1145/2808196.2811641]; He S, 2013, INT CONF AFFECT, P239, DOI 10.1109/ACII.2013.46; Hernandez B, 2007, COMPUT VIS IMAGE UND, V106, P258, DOI 10.1016/j.cviu.2006.08.012; Hernandez-Vela A, 2012, PROC CVPR IEEE, P726, DOI 10.1109/CVPR.2012.6247742; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Huang T. S., 1998, P ATR WORKSH VIRT CO, P1; Igual L, 2014, PATTERN RECOGN, V47, P659, DOI 10.1016/j.patcog.2013.08.006; Ijjina EP, 2014, 2014 13TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P392, DOI 10.1109/ICMLA.2014.70; Ioannou S, 2014, PSYCHOPHYSIOLOGY, V51, P951, DOI 10.1111/psyp.12243; Irani Ramin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P88, DOI 10.1109/CVPRW.2015.7301341; Ishiguro H, 2001, IND ROBOT, V28, P498, DOI 10.1108/01439910110410051; Izard C. E., 1983, SYSTEM IDENTIFYING A; Izard C.E., 1971, FACE EMOTION; Izard CE, 1983, MAXIMALLY DISCRIMINA; Jack Rachael E, 2009, Curr Biol, V19, P1543, DOI 10.1016/j.cub.2009.07.051; Jain V., 2014, P 4 INT WORKSH AUD V, P87; Jeni L. A., 2013, 2013 10 IEEE INT C W, P1; Ji QA, 2006, IEEE T SYST MAN CY A, V36, P862, DOI 10.1109/TSMCA.2005.855922; Jones M., 2003, TR200396 MITS EL RES; Joshi J, 2012, INT C PATT RECOG, P2634; Kaltwang S, 2012, LECT NOTES COMPUT SC, V7432, P368, DOI 10.1007/978-3-642-33191-6_36; Kanade T, 2000, P 4 IEEE INT C AUT F, P46, DOI [10.1109/AFGR.2000.840611, DOI 10.1109/AFGR.2000.840611]; Kapoor A, 2007, INT J HUM-COMPUT ST, V65, P724, DOI 10.1016/j.ijhcs.2007.02.003; Keltner D., 2000, HDB EMOTIONS, P236; Kessous L, 2010, J MULTIMODAL USER IN, V3, P33, DOI 10.1007/s12193-009-0025-5; Koda Y, 2009, RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication, P955, DOI 10.1109/ROMAN.2009.5326321; Koelstra S, 2010, IEEE T PATTERN ANAL, V32, P1940, DOI 10.1109/TPAMI.2010.50; Kohler CG, 2008, SCHIZOPHR RES, V105, P49, DOI 10.1016/j.schres.2008.05.010; Kotsia I, 2007, IEEE T IMAGE PROCESS, V16, P172, DOI 10.1109/TIP.2006.884954; Kuncheva L.I., 2014, COMBINING PATTERN CL, DOI 10.1002/0471660264; Lakshmi H. V., 2010, J COMPUT THEORY ENG, V2, P1793; Lemaire P., 2013, IEEE INT C WORKSH AU, P1, DOI DOI 10.1109/FG.2013.6553821; LEVENSON RW, 1990, PSYCHOPHYSIOLOGY, V27, P363, DOI 10.1111/j.1469-8986.1990.tb02330.x; Li HL, 2008, J VIS COMMUN IMAGE R, V19, P320, DOI 10.1016/j.jvcir.2008.04.001; Li YS, 2013, INT J ANTENN PROPAG, V2013, DOI 10.1155/2013/472645; Littlewort G, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P298, DOI 10.1109/FG.2011.5771414; Littlewort G., 2004, PROC IEEE C COMPUT V, P80; Littlewort GC, 2009, IMAGE VISION COMPUT, V27, P1797, DOI 10.1016/j.imavis.2008.12.010; Littlewort GC, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P15; Liu M., 2013, 10 IEEE INT C WORKSH, P1, DOI [DOI 10.1109/FG.2013.6553734, DOI 10.1128/GEN0MEA.00300-13]; Liu MY, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P525, DOI 10.1145/2522848.2531738; Liu MY, 2014, PROC CVPR IEEE, P1749, DOI 10.1109/CVPR.2014.226; Liu Mengyi, 2014, P 16 INT C MULT INT, P494, DOI DOI 10.1145/2663204.2666274; LIU P, 2014, PROC CVPR IEEE, P1805, DOI DOI 10.1109/CVPR.2014.233; Liu ZL, 2011, LECT NOTES COMPUT SC, V6975, P240, DOI 10.1007/978-3-642-24571-8_26; Lucas GM, 2015, INT CONF AFFECT, P539, DOI 10.1109/ACII.2015.7344622; Lucey P., 2010, 2010 IEEE COMPUTER S, P94, DOI [10.1109/CVPRW.2010.5543262, DOI 10.1109/CVPRW.2010.5543262]; Lucey P, 2011, IEEE T SYST MAN CY B, V41, P664, DOI 10.1109/TSMCB.2010.2082525; Lucey S., 2007, INVESTIGATING SPONTA; Lyons MJ, 1999, IEEE T PATTERN ANAL, V21, P1357, DOI 10.1109/34.817413; Maalej A, 2011, PATTERN RECOGN, V44, P1581, DOI 10.1016/j.patcog.2011.02.012; Maat L, 2007, LECT NOTES COMPUT SC, V4451, P251; Mao ZL, 2004, INT C PATT RECOG, P144, DOI 10.1109/ICPR.2004.1334489; Martinez A, 2012, J MACH LEARN RES, V13, P1589; Martinez H.P., 2011, P INT C MULT INT ACM, P3; Mase K., 1991, Systems and Computers in Japan, V22, P67; Matsumoto D., 2008, HDB EMOTIONS, P211, DOI DOI 10.1016/J.BRAT.2006.05.004; Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4; McDuff D, 2014, IMAGE VISION COMPUT, V32, P630, DOI 10.1016/j.imavis.2014.01.004; McDuff D, 2013, IEEE COMPUT SOC CONF, P881, DOI 10.1109/CVPRW.2013.130; McKeown G, 2012, IEEE T AFFECT COMPUT, V3, P5, DOI 10.1109/T-AFFC.2011.20; Memisevic, 2013, ICMI 13, P543; Mpiperis I, 2008, INT CONF ACOUST SPEE, P2133, DOI 10.1109/ICASSP.2008.4518064; Nair P, 2009, IEEE T MULTIMEDIA, V11, P611, DOI 10.1109/TMM.2009.2017629; Nguyen H, 2014, LECT NOTES COMPUT SC, V8333, P397, DOI 10.1007/978-3-642-53842-1_34; Nicolle J, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P501; Niedenthal PM, 2007, SCIENCE, V316, P1002, DOI 10.1126/science.1136930; Osadchy M, 2007, J MACH LEARN RES, V8, P1197; Pantic M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P317, DOI 10.1109/ICME.2005.1521424; Pantic M, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P1026; Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976; Pantic M, 2007, FACE RECOGNITION, P377, DOI [10.5772/4847, DOI 10.5772/4847]; Pantic M, 2006, IEEE T SYST MAN CY B, V36, P433, DOI 10.1109/TSMCB.2005.859075; Pardas M, 2002, SIGNAL PROCESS-IMAGE, V17, P675, DOI 10.1016/S0923-5965(02)00078-4; Pavlidis I, 2007, COMPUT VIS IMAGE UND, V108, P150, DOI 10.1016/j.cviu.2006.11.018; Peng Liu, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163094; Pfister T, 2011, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2011.6126401; Polikovsky S., 2009, FACIAL MICROEXPRESSI, P1, DOI [10.1049/ic.2009.0244, DOI 10.1049/IC.2009.0244]; Prkachin KM, 2008, PAIN, V139, P267, DOI 10.1016/j.pain.2008.04.010; Queirolo CC, 2010, IEEE T PATTERN ANAL, V32, P206, DOI 10.1109/TPAMI.2009.14; Kassim SRA, 2006, IEEE IMAGE PROC, P661; Ranzato M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2857, DOI 10.1109/CVPR.2011.5995710; Rastegari M, 2011, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2011.6126556; Rifai S, 2012, LECT NOTES COMPUT SC, V7577, P808, DOI 10.1007/978-3-642-33783-3_58; Roger R. W. Highfield, 2009, NEW SCI; Romdhani S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P59; RUSSELL JA, 1994, PSYCHOL BULL, V115, P102, DOI 10.1037/0033-2909.115.1.102; RUSSELL JA, 1977, J RES PERS, V11, P273, DOI 10.1016/0092-6566(77)90037-X; Russell JA, 2003, ANNU REV PSYCHOL, V54, P329, DOI 10.1146/annurev.psych.54.101601.145102; Ryan Andrew, 2009, 2009 IEEE 43rd International Carnahan Conference on Security Technology. ICCST 2009, P172, DOI 10.1109/CCST.2009.5335546; Salah A. A., 2010, AFFECTIVE COMPUTING, P157, DOI DOI 10.4018/978-1-61692-892-6.ch008; SAMAL A, 1992, PATTERN RECOGN, V25, P65, DOI 10.1016/0031-3203(92)90007-6; Sanchez-Cortes Dairazalia, 2013, P 12 INT C MOB UB MU, P22; Sandbach G, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P406, DOI 10.1109/FG.2011.5771434; Sandbach G, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P738, DOI 10.1109/ICCVW.2013.101; Sandbach G, 2012, IEEE IMAGE PROC, P1813, DOI 10.1109/ICIP.2012.6467234; Sandbach G, 2012, IMAGE VISION COMPUT, V30, P683, DOI 10.1016/j.imavis.2012.06.005; Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127; Savran A, 2008, PROC CVPR IEEE, P985; Savran A, 2015, IEEE T CYBERNETICS, V45, P1927, DOI 10.1109/TCYB.2014.2362101; Savran A, 2011, EUR SIGNAL PR CONF, P1969; Savran A, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P485; Savran A, 2012, IMAGE VISION COMPUT, V30, P774, DOI 10.1016/j.imavis.2011.11.008; Savran A, 2012, PATTERN RECOGN, V45, P767, DOI 10.1016/j.patcog.2011.07.022; Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6; Scherer S, 2013, IEEE INT CONF AUTOMA; Schmidt KL, 2001, YEARB PHYS ANTHROPOL, V44, P3, DOI 10.1002/ajpa.20001; Sebe N, 2007, IMAGE VISION COMPUT, V25, P1856, DOI 10.1016/j.imavis.2005.12.021; Sebe N, 2006, INT C PATT RECOG, P1136; Segundo MP, 2010, IEEE T SYST MAN CY B, V40, P1319, DOI 10.1109/TSMCB.2009.2038233; Senoussaoui M., 2014, P 4 INT WORKSH AUD V, P57, DOI DOI 10.1145/2661806.2661819; Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005; Shariff AF, 2011, CURR DIR PSYCHOL SCI, V20, P395, DOI 10.1177/0963721411424739; Shimada Keiji, 2013, International Journal of Computer Theory and Engineering, V5, P24, DOI 10.7763/IJCTE.2013.V5.640; Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381; Shreve Matthew, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P51, DOI 10.1109/FG.2011.5771451; Shreve M., 2009, PROC WORKSHOP APPL C, P1; Sidorov M., 2014, P 4 INT WORKSH AUD V, P81, DOI DOI 10.1145/2661806.2661816; Sikka K, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P517, DOI 10.1145/2522848.2531741; Sirohey S. A., 1998, CSTR3176 U MAR; Sobottka K, 1998, SIGNAL PROCESS-IMAGE, V12, P263, DOI 10.1016/S0923-5965(97)00042-8; Sobottka K, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P236, DOI 10.1109/AFGR.1996.557270; Soladie C, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P493; Song I, 2014, I SYMP CONSUM ELECTR, P566; Soyel H, 2007, LECT NOTES COMPUT SC, V4633, P831; Suwa M., 1978, Proceedings of the 4th International Joint Conference on Pattern Recognition, P408; Szeptycki P, 2009, 2009 IEEE 3RD INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS, P32, DOI 10.1109/BTAS.2009.5339052; Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310; Tan CT., 2012, P 8 AUSTRALASIAN C I, V2012, P5, DOI DOI 10.1145/2336727.2336732; Tang H, 2008, IEEE INT CONF AUTOMA, P110; Tena J. R., 2006, P IEEE INT C VID SIG, P81; Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962; Trujillo L., 2005, IEEE COMP SOC C COMP, P14, DOI DOI 10.1109/CVPR.2005.415; Tsalakanidou F, 2009, PROC CVPR IEEE, P763; Tsalakanidou F, 2010, PATTERN RECOGN, V43, P1763, DOI 10.1016/j.patcog.2009.12.009; Valstar Michel F, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P921, DOI 10.1109/FG.2011.5771374; Valstar M.F., 2015, 11 IEEE INT C WORKSH, V06, P1; Valstar M. F., 2006, P 8 INT C MULT INT, P162, DOI DOI 10.1145/1180995.1181031; Van den Stock J, 2007, EMOTION, V7, P487, DOI 10.1037/1528-3542.7.3.487; Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Vretos N, 2011, IEEE IMAGE PROC, P773, DOI 10.1109/ICIP.2011.6116669; Vuong Le, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P414, DOI 10.1109/FG.2011.5771435; Vural E., 2007, INT WORKSH HUM COMP, P6, DOI [10.1007/978-3-540-75773-3_2, DOI 10.1007/978-3-540-75773-3_2]; Walecki R., 2015, IEEE C WORKSHOPS AUT, P1; Waller BM, 2008, PHYSIOL BEHAV, V95, P93, DOI 10.1016/j.physbeh.2008.05.002; Waller BM, 2008, EMOTION, V8, P435, DOI 10.1037/1528-3542.8.3.435; Waller BM, 2012, INT J PRIMATOL, V33, P809, DOI 10.1007/s10764-012-9611-6; Wang J, 2005, FACIAL EXPRESSION RE; Wang J, 2006, P IEEE INT C COMP VI, V2, P1399, DOI DOI 10.1109/CVPR.2006.14; Wang N., 2014, ARXIV14101037; Wang SF, 2014, FRONT COMPUT SCI-CHI, V8, P609, DOI 10.1007/s11704-014-3295-3; Wang SF, 2010, IEEE T MULTIMEDIA, V12, P682, DOI 10.1109/TMM.2010.2060716; WATSON D, 1988, J PERS SOC PSYCHOL, V54, P1063, DOI 10.1037/0022-3514.54.6.1063; Wei J, 2013, IEEE ICCE, P1, DOI 10.1109/ICCE.2013.6486769; Williamson JR, 2014, P 4 INT WORKSHOP AUD, P65; Wollmer M, 2013, IMAGE VISION COMPUT, V31, P153, DOI 10.1016/j.imavis.2012.03.001; Wu B, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P79; Wu CC, 2015, IEEE GLOB COMM CONF, DOI [10.1109/FG.2015.7163116, 10.1109/GLOCOM.2015.7417352]; Wu LZ, 1999, IEEE T MULTIMEDIA, V1, P334, DOI 10.1109/6046.807953; Wu Q, 2011, LECT NOTES COMPUT SC, V6975, P152, DOI 10.1007/978-3-642-24571-8_16; Xi Zhao, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3724, DOI 10.1109/ICPR.2010.907; Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75; Yan W.J., 2013, P 2013 10 IEEE INT C, P1, DOI DOI 10.1109/FG.2013.6553799; Yin LJ, 2006, INT C PATT RECOG, P1248; Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211; Yin LJ, 2008, IEEE INT CONF AUTOMA, P116; Yoshitomi Y, 2000, IEEE RO-MAN 2000: 9TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P178, DOI 10.1109/ROMAN.2000.892491; Yoshitomi Y, 1997, RO-MAN '97 SENDAI: 6TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN COMMUNICATION, PROCEEDINGS, P380, DOI 10.1109/ROMAN.1997.647016; Yoshitomi Y, 2010, INT CONF APPL COMPUT, P182; Zafeiriou Stefanos, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2860, DOI 10.1109/CVPRW.2009.5206584; Zaker N., 2012, P IEEE INT C DEV LEA, P1; Zeng Z., 2006, J MULTIMEDIA, V1, P1, DOI DOI 10.4304/jmm.1.5.1-8; Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52; Zeng ZH, 2008, IEEE T MULTIMEDIA, V10, P570, DOI 10.1109/TMM.2008.921737; Zhang C., 2010, SURVEY RECENT ADV FA; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110; Zhao X, 2013, IMAGE VISION COMPUT, V31, P231, DOI 10.1016/j.imavis.2012.10.001; Zhi RC, 2011, IEEE T SYST MAN CY B, V41, P38, DOI 10.1109/TSMCB.2010.2044788	266	273	285	32	224	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	2016	38	8			SI		1548	1568		10.1109/TPAMI.2016.2515606	http://dx.doi.org/10.1109/TPAMI.2016.2515606			21	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DR5EO	26761193	Green Submitted, Green Accepted			2022-10-03	WOS:000379926200006